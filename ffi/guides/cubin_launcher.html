
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>CUBIN Launcher Guide &#8212; tvm-ffi</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/collapsible-lists/css/tree_view.css?v=a885cde7" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=38aa6179" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=3a1e5d7d"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script src="../_static/collapsible-lists/js/CollapsibleLists.compressed.js?v=73120307"></script>
    <script src="../_static/collapsible-lists/js/apply-collapsible-lists.js?v=660e4f45"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="../_static/downloads/require.min.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'guides/cubin_launcher';</script>
    <link rel="icon" href="https://tvm.apache.org/images/logo/tvm-logo-square.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Python Guide" href="python_lang_guide.html" />
    <link rel="prev" title="Compiler Integration" href="compiler_integration.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
    <meta name="docbuild:last-update" content="Feb 08, 2026"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">tvm-ffi</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/quickstart.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/stable_c_abi.html">Stable C ABI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Guides</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="export_func_cls.html">Export Functions and Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="kernel_library_guide.html">Kernel Library Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="compiler_integration.html">Compiler Integration</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">CUBIN Launcher Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="python_lang_guide.html">Python Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="cpp_lang_guide.html">C++ Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="rust_lang_guide.html">Rust Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../concepts/abi_overview.html">ABI Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/any.html">Any and AnyView</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/object_and_class.html">Object and Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/tensor.html">Tensor and DLPack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/func_module.html">Function and Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../concepts/exception_handling.html">Exception Handling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Packaging</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../packaging/python_packaging.html">Python Packaging</a></li>
<li class="toctree-l1"><a class="reference internal" href="../packaging/stubgen.html">Stub Generation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../packaging/cpp_tooling.html">C++ Tooling</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Reference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../reference/python/index.html">Python API</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Object.html">tvm_ffi.Object</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Tensor.html">tvm_ffi.Tensor</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.from_dlpack.html">tvm_ffi.from_dlpack</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Shape.html">tvm_ffi.Shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.dtype.html">tvm_ffi.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Device_class.html">tvm_ffi.Device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.DLDeviceType.html">tvm_ffi.DLDeviceType</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.device_function.html">tvm_ffi.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Function.html">tvm_ffi.Function</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Module.html">tvm_ffi.Module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.system_lib.html">tvm_ffi.system_lib</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.load_module.html">tvm_ffi.load_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Array.html">tvm_ffi.Array</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.Map.html">tvm_ffi.Map</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.register_error.html">tvm_ffi.register_error</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.register_object.html">tvm_ffi.register_object</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.register_global_func.html">tvm_ffi.register_global_func</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.get_global_func.html">tvm_ffi.get_global_func</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.get_global_func_metadata.html">tvm_ffi.get_global_func_metadata</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.init_ffi_api.html">tvm_ffi.init_ffi_api</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.remove_global_func.html">tvm_ffi.remove_global_func</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.StreamContext.html">tvm_ffi.StreamContext</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.use_torch_stream.html">tvm_ffi.use_torch_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.use_raw_stream.html">tvm_ffi.use_raw_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.get_raw_stream.html">tvm_ffi.get_raw_stream</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.load_inline.html">tvm_ffi.cpp.load_inline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.build_inline.html">tvm_ffi.cpp.build_inline</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.load.html">tvm_ffi.cpp.load</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.build.html">tvm_ffi.cpp.build</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.libinfo.load_lib_module.html">tvm_ffi.libinfo.load_lib_module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.nvrtc.nvrtc_compile.html">tvm_ffi.cpp.nvrtc.nvrtc_compile</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.serialization.from_json_graph_str.html">tvm_ffi.serialization.from_json_graph_str</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.serialization.to_json_graph_str.html">tvm_ffi.serialization.to_json_graph_str</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.access_path.AccessKind.html">tvm_ffi.access_path.AccessKind</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.access_path.AccessPath.html">tvm_ffi.access_path.AccessPath</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.access_path.AccessStep.html">tvm_ffi.access_path.AccessStep</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.convert.html">tvm_ffi.convert</a></li>
<li class="toctree-l2"><a class="reference internal" href="../reference/python/generated/tvm_ffi.ObjectConvertible.html">tvm_ffi.ObjectConvertible</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/cpp/index.html">C++ API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/rust/index.html">Rust API</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Developer Manual</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../dev/source_build.html">Build from Source</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/doc_build.html">Build This Doc Site</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/ci_cd.html">Reproduce CI/CD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/release_process.html">Release Process</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/apache/tvm-ffi" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/guides/cubin_launcher.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>CUBIN Launcher Guide</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-usage">Python Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-workflow">Basic Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-nvrtc-compilation">Example: NVRTC Compilation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-using-triton-kernels">Example: Using Triton Kernels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-usage">C++ Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-cubin-at-compile-time">Embedding CUBIN at Compile Time</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-cubin-at-runtime">Loading CUBIN at Runtime</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-cubin-with-cmake-utilities">Embedding CUBIN with CMake Utilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-cubin-with-python-utility">Embedding CUBIN with Python Utility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-cubin-embedding">Manual CUBIN Embedding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topics">Advanced Topics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu-support">Multi-GPU Support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-launch-configuration">Kernel Launch Configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-shared-memory">Dynamic Shared Memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-with-different-compilers">Integration with Different Compilers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-examples">Complete Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-classes">C++ Classes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-macros">C++ Macros</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-functions">Python Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-utilities">Python Utilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cmake-functions">CMake Functions</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="cubin-launcher-guide">
<h1>CUBIN Launcher Guide<a class="headerlink" href="#cubin-launcher-guide" title="Link to this heading">#</a></h1>
<p>This guide demonstrates how to load and launch CUDA kernels from CUBIN (CUDA Binary) modules using TVM-FFI. The CUBIN launcher enables you to execute pre-compiled or runtime-compiled CUDA kernels efficiently through the CUDA Runtime API or Driver API.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>TVM-FFI provides utilities for loading and launching CUDA kernels from CUBIN modules. The implementation supports both <strong>CUDA Runtime API</strong> (default for CUDA &gt;= 12.8) and <strong>CUDA Driver API</strong>.</p>
<p><strong>Runtime API (CUDA &gt;= 12.8):</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cudaLibraryLoadData()</span></code> - Load CUBIN from memory buffer</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cudaLibraryGetKernel()</span></code> - Get kernel handle by name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cudaLaunchKernel()</span></code> - Launch kernel with grid/block dimensions</p></li>
</ul>
<p><strong>Driver API:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cuLibraryLoadData()</span></code> - Load CUBIN from memory buffer</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuLibraryGetKernel()</span></code> - Get kernel handle by name</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cuLaunchKernel()</span></code> - Launch kernel with grid/block dimensions</p></li>
</ul>
<p><strong>Customization:</strong></p>
<p>By default, the implementation uses the Runtime API if compiled with CUDA &gt;= 12.8, falling back to the Driver API for older versions. You can force the usage of the Driver API (or Runtime API) by defining the macro <code class="docutils literal notranslate"><span class="pre">TVM_FFI_CUBIN_LAUNCHER_USE_DRIVER_API</span></code> (set to <code class="docutils literal notranslate"><span class="pre">1</span></code> for Driver API, <code class="docutils literal notranslate"><span class="pre">0</span></code> for Runtime API) before including the header.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>CMAKE_CUDA_RUNTIME_LIBRARY and Driver API</strong></p>
<p>When using CMake, the default behavior (if <code class="docutils literal notranslate"><span class="pre">CMAKE_CUDA_RUNTIME_LIBRARY</span></code> is not set) is to link against the CUDA Runtime Library (<code class="docutils literal notranslate"><span class="pre">cudart</span></code>). TVM-FFI’s CMake utility automatically defaults this variable to <code class="docutils literal notranslate"><span class="pre">Shared</span></code> if it is undefined. This introduces a dependency on the CUDA runtime version, requiring the system’s driver to be compatible with that runtime version.</p>
<p>If you intend to use the Driver API only (e.g. by setting <code class="docutils literal notranslate"><span class="pre">TVM_FFI_CUBIN_LAUNCHER_USE_DRIVER_API=1</span></code>) to avoid this runtime dependency:</p>
<ol class="arabic simple">
<li><p>You must explicitly set <code class="docutils literal notranslate"><span class="pre">CMAKE_CUDA_RUNTIME_LIBRARY</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code> in your CMake configuration to prevent linking <code class="docutils literal notranslate"><span class="pre">cudart</span></code>.</p></li>
<li><p>You must manually link your target against the CUDA Driver library (usually <code class="docutils literal notranslate"><span class="pre">cuda</span></code> on Linux/Windows or <cite>CUDA::cuda_driver</cite> provided by CMake’s <code class="docutils literal notranslate"><span class="pre">FindCUDAToolkit</span></code>).</p></li>
</ol>
<p>This ensures your application relies solely on the widely compatible CUDA Driver API (<code class="docutils literal notranslate"><span class="pre">libcuda.so.1</span></code>).</p>
</div>
<p>The implementation is in <code class="docutils literal notranslate"><span class="pre">tvm/ffi/extra/cuda/cubin_launcher.h</span></code> and provides:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinModule.html#_CPPv4N3tvm3ffi11CubinModuleE" title="tvm::ffi::CubinModule"><code class="xref cpp cpp-class docutils literal notranslate"><span class="pre">tvm::ffi::CubinModule</span></code></a>: RAII wrapper for loading CUBIN modules from memory</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinKernel.html#_CPPv4N3tvm3ffi11CubinKernelE" title="tvm::ffi::CubinKernel"><code class="xref cpp cpp-class docutils literal notranslate"><span class="pre">tvm::ffi::CubinKernel</span></code></a>: Handle for launching CUDA kernels with specified parameters</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1a99736a44462543179cb434ebd4512ade.html#c.TVM_FFI_EMBED_CUBIN" title="TVM_FFI_EMBED_CUBIN"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN</span></code></a>: Macro for embedding CUBIN data at compile time (legacy / object-linking approach)</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1a55832b50e83cf39108f1e306f031433d.html#c.TVM_FFI_EMBED_CUBIN_FROM_BYTES" title="TVM_FFI_EMBED_CUBIN_FROM_BYTES"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN_FROM_BYTES</span></code></a>: Macro for embedding CUBIN data from byte arrays (manual embedding approach)</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1ae8d64fa1cc7db9d38632e32054df72fc.html#c.TVM_FFI_EMBED_CUBIN_GET_KERNEL" title="TVM_FFI_EMBED_CUBIN_GET_KERNEL"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN_GET_KERNEL</span></code></a>: Macro for retrieving kernels from embedded CUBIN</p></li>
</ul>
<p>The CUBIN launcher supports:</p>
<ul class="simple">
<li><p>Loading CUBIN from memory (embedded data or runtime-generated)</p></li>
<li><p>Multi-GPU execution using CUDA primary contexts</p></li>
<li><p>Kernel parameter management and launch configuration</p></li>
<li><p>Integration with NVRTC, Triton, and other CUDA compilation tools</p></li>
</ul>
<p><strong>Build Integration:</strong></p>
<p>TVM-FFI provides convenient tools for embedding CUBIN data at build time:</p>
<ul class="simple">
<li><p><strong>CMake utilities</strong> (<code class="docutils literal notranslate"><span class="pre">cmake/Utils/EmbedCubin.cmake</span></code>): Functions for compiling CUDA to CUBIN/FATBIN and embedding it into C++ code or linking it.</p></li>
<li><p><strong>Python utility</strong> (<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">tvm_ffi.utils.embed_cubin</span></code>): Command-line tool for embedding CUBIN into object files.</p></li>
<li><p><strong>Python API</strong> (<a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.load_inline.html#tvm_ffi.cpp.load_inline" title="tvm_ffi.cpp.load_inline"><code class="xref py py-func docutils literal notranslate"><span class="pre">tvm_ffi.cpp.load_inline()</span></code></a>): Runtime embedding via <code class="docutils literal notranslate"><span class="pre">embed_cubin</span></code> parameter.</p></li>
</ul>
</section>
<section id="python-usage">
<h2>Python Usage<a class="headerlink" href="#python-usage" title="Link to this heading">#</a></h2>
<section id="basic-workflow">
<h3>Basic Workflow<a class="headerlink" href="#basic-workflow" title="Link to this heading">#</a></h3>
<p>The typical workflow for launching CUBIN kernels from Python involves:</p>
<ol class="arabic simple">
<li><p><strong>Generate CUBIN</strong>: Compile your CUDA kernel to CUBIN format</p></li>
<li><p><strong>Define C++ Wrapper</strong>: Write C++ code to load and launch the kernel</p></li>
<li><p><strong>Load Module</strong>: Use <a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.load_inline.html#tvm_ffi.cpp.load_inline" title="tvm_ffi.cpp.load_inline"><code class="xref py py-func docutils literal notranslate"><span class="pre">tvm_ffi.cpp.load_inline()</span></code></a> with <code class="docutils literal notranslate"><span class="pre">embed_cubin</span></code> parameter</p></li>
<li><p><strong>Call Kernel</strong>: Invoke the kernel function from Python</p></li>
</ol>
</section>
<section id="example-nvrtc-compilation">
<h3>Example: NVRTC Compilation<a class="headerlink" href="#example-nvrtc-compilation" title="Link to this heading">#</a></h3>
<p>Here’s a complete example using NVRTC to compile CUDA source at runtime.</p>
<p><strong>Step 1: Compile CUDA source to CUBIN using NVRTC</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define CUDA kernels</span>
<span class="n">cuda_source</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">extern &quot;C&quot; __global__ void add_one(float* x, float* y, int n) {</span>
<span class="s2">    int idx = blockIdx.x * blockDim.x + threadIdx.x;</span>
<span class="s2">    if (idx &lt; n) {</span>
<span class="s2">        y[idx] = x[idx] + 1.0f;</span>
<span class="s2">    }</span>
<span class="s2">}</span>

<span class="s2">extern &quot;C&quot; __global__ void mul_two(float* x, float* y, int n) {</span>
<span class="s2">    int idx = blockIdx.x * blockDim.x + threadIdx.x;</span>
<span class="s2">    if (idx &lt; n) {</span>
<span class="s2">        y[idx] = x[idx] * 2.0f;</span>
<span class="s2">    }</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="c1"># Compile CUDA source to CUBIN using NVRTC</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compiling CUDA kernels to CUBIN using NVRTC...&quot;</span><span class="p">)</span>
<span class="n">cubin_bytes</span> <span class="o">=</span> <span class="n">nvrtc</span><span class="o">.</span><span class="n">nvrtc_compile</span><span class="p">(</span><span class="n">cuda_source</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;kernels.cu&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compiled CUBIN: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cubin_bytes</span><span class="p">)</span><span class="si">}</span><span class="s2"> bytes</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Step 2: Define C++ wrapper with embedded CUBIN</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define C++ code inline to launch the CUDA kernels using embedded CUBIN</span>
<span class="n">sources</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#include &lt;tvm/ffi/container/tensor.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/error.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/extra/c_env_api.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/extra/cuda/cubin_launcher.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/function.h&gt;</span>

<span class="s2">// Embed CUBIN module with name &quot;nvrtc_cubin&quot;</span>
<span class="s2">TVM_FFI_EMBED_CUBIN(nvrtc_cubin);</span>

<span class="s2">namespace nvrtc_loader {</span>

<span class="s2">void AddOne(tvm::ffi::TensorView x, tvm::ffi::TensorView y) {</span>
<span class="s2">// Get kernel from embedded CUBIN (cached in static variable for efficiency)</span>
<span class="s2">static auto kernel = TVM_FFI_EMBED_CUBIN_GET_KERNEL(nvrtc_cubin, &quot;add_one&quot;);</span>

<span class="s2">TVM_FFI_CHECK(x.ndim() == 1, ValueError) &lt;&lt; &quot;Input must be 1D tensor&quot;;</span>
<span class="s2">TVM_FFI_CHECK(y.ndim() == 1, ValueError) &lt;&lt; &quot;Output must be 1D tensor&quot;;</span>
<span class="s2">TVM_FFI_CHECK(x.size(0) == y.size(0), ValueError) &lt;&lt; &quot;Input and output must have same size&quot;;</span>

<span class="s2">int64_t n = x.size(0);</span>
<span class="s2">void* x_ptr = x.data_ptr();</span>
<span class="s2">void* y_ptr = y.data_ptr();</span>

<span class="s2">// Prepare kernel arguments</span>
<span class="s2">void* args[] = {reinterpret_cast&lt;void*&gt;(&amp;x_ptr), reinterpret_cast&lt;void*&gt;(&amp;y_ptr),</span>
<span class="s2">                reinterpret_cast&lt;void*&gt;(&amp;n)};</span>

<span class="s2">// Launch configuration</span>
<span class="s2">tvm::ffi::dim3 grid((n + 255) / 256);</span>
<span class="s2">tvm::ffi::dim3 block(256);</span>

<span class="s2">// Get CUDA stream</span>
<span class="s2">DLDevice device = x.device();</span>
<span class="s2">cudaStream_t stream = static_cast&lt;cudaStream_t&gt;(TVMFFIEnvGetStream(device.device_type, device.device_id));</span>

<span class="s2">// Launch kernel</span>
<span class="s2">TVM_FFI_CHECK_CUBIN_LAUNCHER_CUDA_ERROR(kernel.Launch(args, grid, block, stream));</span>
<span class="s2">}</span>

<span class="s2">void MulTwo(tvm::ffi::TensorView x, tvm::ffi::TensorView y) {</span>
<span class="s2">// Get kernel from embedded CUBIN (cached in static variable for efficiency)</span>
<span class="s2">static auto kernel = TVM_FFI_EMBED_CUBIN_GET_KERNEL(nvrtc_cubin, &quot;mul_two&quot;);</span>

<span class="s2">TVM_FFI_CHECK(x.ndim() == 1, ValueError) &lt;&lt; &quot;Input must be 1D tensor&quot;;</span>
<span class="s2">TVM_FFI_CHECK(y.ndim() == 1, ValueError) &lt;&lt; &quot;Output must be 1D tensor&quot;;</span>
<span class="s2">TVM_FFI_CHECK(x.size(0) == y.size(0), ValueError) &lt;&lt; &quot;Input and output must have same size&quot;;</span>

<span class="s2">int64_t n = x.size(0);</span>
<span class="s2">void* x_ptr = x.data_ptr();</span>
<span class="s2">void* y_ptr = y.data_ptr();</span>

<span class="s2">// Prepare kernel arguments</span>
<span class="s2">void* args[] = {reinterpret_cast&lt;void*&gt;(&amp;x_ptr), reinterpret_cast&lt;void*&gt;(&amp;y_ptr),</span>
<span class="s2">                reinterpret_cast&lt;void*&gt;(&amp;n)};</span>

<span class="s2">// Launch configuration</span>
<span class="s2">tvm::ffi::dim3 grid((n + 255) / 256);</span>
<span class="s2">tvm::ffi::dim3 block(256);</span>

<span class="s2">// Get CUDA stream</span>
<span class="s2">DLDevice device = x.device();</span>
<span class="s2">cudaStream_t stream = static_cast&lt;cudaStream_t&gt;(TVMFFIEnvGetStream(device.device_type, device.device_id));</span>

<span class="s2">// Launch kernel</span>
<span class="s2">TVM_FFI_CHECK_CUBIN_LAUNCHER_CUDA_ERROR(kernel.Launch(args, grid, block, stream));</span>
<span class="s2">}</span>

<span class="s2">}  // namespace nvrtc_loader</span>

<span class="s2">TVM_FFI_DLL_EXPORT_TYPED_FUNC(add_one, nvrtc_loader::AddOne);</span>
<span class="s2">TVM_FFI_DLL_EXPORT_TYPED_FUNC(mul_two, nvrtc_loader::MulTwo);</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compiling C++ sources with tvm_ffi.cpp.load_inline...&quot;</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">cpp</span><span class="o">.</span><span class="n">load_inline</span><span class="p">(</span>
    <span class="s2">&quot;nvrtc_loader&quot;</span><span class="p">,</span>
    <span class="n">cuda_sources</span><span class="o">=</span><span class="n">sources</span><span class="p">,</span>
    <span class="n">embed_cubin</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;nvrtc_cubin&quot;</span><span class="p">:</span> <span class="n">cubin_bytes</span><span class="p">},</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Successfully compiled and loaded C++ sources with embedded CUBIN</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Key Points:</strong></p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">embed_cubin</span></code> parameter is a dictionary mapping CUBIN names to their binary data</p></li>
<li><p>CUBIN names in <code class="docutils literal notranslate"><span class="pre">embed_cubin</span></code> must match names in <a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1a99736a44462543179cb434ebd4512ade.html#c.TVM_FFI_EMBED_CUBIN" title="TVM_FFI_EMBED_CUBIN"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN</span></code></a></p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">cuda_sources</span></code> parameter (instead of <code class="docutils literal notranslate"><span class="pre">cpp_sources</span></code>) to automatically link with CUDA libraries</p></li>
<li><p>The C++ wrapper handles device management, stream handling, and kernel launching</p></li>
</ul>
</section>
<section id="example-using-triton-kernels">
<h3>Example: Using Triton Kernels<a class="headerlink" href="#example-using-triton-kernels" title="Link to this heading">#</a></h3>
<p>You can compile Triton kernels to CUBIN and launch them through TVM-FFI.</p>
<p><strong>Step 1: Define and compile Triton kernel</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the kernel dynamically</span>
<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">square_kernel</span><span class="p">(</span><span class="n">X_ptr</span><span class="p">,</span> <span class="n">Y_ptr</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span> <span class="o">=</span> <span class="mi">1024</span><span class="p">):</span>  <span class="c1"># noqa</span>
    <span class="n">pid</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">start</span> <span class="o">=</span> <span class="n">pid</span> <span class="o">*</span> <span class="n">BLOCK</span>
    <span class="n">offsets</span> <span class="o">=</span> <span class="n">start</span> <span class="o">+</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">offsets</span> <span class="o">&lt;</span> <span class="n">n</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">X_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">other</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">*</span> <span class="n">x</span>
    <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">Y_ptr</span> <span class="o">+</span> <span class="n">offsets</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>

<span class="c1"># Trigger kernel compilation by doing a dummy call</span>
<span class="n">x_dummy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">y_dummy</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="mi">1024</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">square_kernel</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">](</span><span class="n">x_dummy</span><span class="p">,</span> <span class="n">y_dummy</span><span class="p">,</span> <span class="mi">1024</span><span class="p">)</span>

<span class="c1"># Extract compiled CUBIN from the device cache</span>
<span class="n">device_caches</span> <span class="o">=</span> <span class="n">square_kernel</span><span class="o">.</span><span class="n">device_caches</span>
<span class="n">device_id</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">device_caches</span><span class="o">.</span><span class="n">keys</span><span class="p">()))</span>
<span class="n">cache_tuple</span> <span class="o">=</span> <span class="n">device_caches</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span>
<span class="n">compiled_kernel</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">cache_tuple</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

<span class="c1"># Get CUBIN bytes</span>
<span class="n">cubin_bytes</span> <span class="o">=</span> <span class="n">compiled_kernel</span><span class="o">.</span><span class="n">kernel</span>
</pre></div>
</div>
<p><strong>Step 2: Define C++ wrapper to launch the Triton kernel</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define C++ code inline to load and launch the Triton kernel using embedded CUBIN</span>
<span class="n">sources</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">#include &lt;tvm/ffi/container/tensor.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/error.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/extra/c_env_api.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/extra/cuda/cubin_launcher.h&gt;</span>
<span class="s2">#include &lt;tvm/ffi/function.h&gt;</span>

<span class="s2">// Embed CUBIN module with name &quot;triton_cubin&quot;</span>
<span class="s2">TVM_FFI_EMBED_CUBIN(triton_cubin);</span>

<span class="s2">namespace triton_loader {</span>

<span class="s2">void LaunchSquare(tvm::ffi::TensorView x, tvm::ffi::TensorView y) {</span>
<span class="s2">// Get kernel from embedded CUBIN (cached in static variable for efficiency)</span>
<span class="s2">static auto kernel = TVM_FFI_EMBED_CUBIN_GET_KERNEL(triton_cubin, &quot;square_kernel&quot;);</span>

<span class="s2">TVM_FFI_CHECK(x.ndim() == 1, ValueError) &lt;&lt; &quot;Input must be 1D tensor&quot;;</span>
<span class="s2">TVM_FFI_CHECK(y.ndim() == 1, ValueError) &lt;&lt; &quot;Output must be 1D tensor&quot;;</span>
<span class="s2">TVM_FFI_CHECK(x.size(0) == y.size(0), ValueError) &lt;&lt; &quot;Sizes must match&quot;;</span>

<span class="s2">uint32_t n = static_cast&lt;uint32_t&gt;(x.size(0));</span>
<span class="s2">void* x_ptr = x.data_ptr();</span>
<span class="s2">void* y_ptr = y.data_ptr();</span>
<span class="s2">uint64_t dummy_ptr = 0;</span>

<span class="s2">// Workaround for Triton extra params: pass dummy addresses for unused parameters</span>
<span class="s2">void* args[] = {&amp;x_ptr, &amp;y_ptr, &amp;n, &amp;dummy_ptr, &amp;dummy_ptr};</span>

<span class="s2">// Kernel was compiled with .reqntid 128, not 1024</span>
<span class="s2">tvm::ffi::dim3 grid((n + 127) / 128);</span>
<span class="s2">tvm::ffi::dim3 block(128);</span>

<span class="s2">DLDevice device = x.device();</span>
<span class="s2">cudaStream_t stream = static_cast&lt;cudaStream_t&gt;(TVMFFIEnvGetStream(device.device_type, device.device_id));</span>

<span class="s2">TVM_FFI_CHECK_CUBIN_LAUNCHER_CUDA_ERROR(kernel.Launch(args, grid, block, stream));</span>
<span class="s2">}</span>

<span class="s2">}  // namespace triton_loader</span>

<span class="s2">TVM_FFI_DLL_EXPORT_TYPED_FUNC(launch_square, triton_loader::LaunchSquare);</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Compiling C++ sources with tvm_ffi.cpp.load_inline...&quot;</span><span class="p">)</span>
<span class="c1"># Find CUDA include path</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">cpp</span><span class="o">.</span><span class="n">load_inline</span><span class="p">(</span>
    <span class="s2">&quot;triton_loader&quot;</span><span class="p">,</span>
    <span class="n">cuda_sources</span><span class="o">=</span><span class="n">sources</span><span class="p">,</span>
    <span class="n">embed_cubin</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;triton_cubin&quot;</span><span class="p">:</span> <span class="n">cubin_bytes</span><span class="p">},</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Successfully compiled and loaded C++ sources with embedded CUBIN</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Triton kernels may require extra dummy parameters in the argument list. Check the compiled kernel’s signature to determine the exact parameter count needed.</p>
</div>
</section>
</section>
<section id="c-usage">
<h2>C++ Usage<a class="headerlink" href="#c-usage" title="Link to this heading">#</a></h2>
<section id="embedding-cubin-at-compile-time">
<h3>Embedding CUBIN at Compile Time<a class="headerlink" href="#embedding-cubin-at-compile-time" title="Link to this heading">#</a></h3>
<p>The most convenient way to embed CUBIN/FATBIN data in C++ is using the TVM-FFI build utilities. There are three main approaches:</p>
<ol class="arabic simple">
<li><p><strong>Object Linking (Standard)</strong>: Use CMake utilities to compile and link the CUBIN data.</p></li>
<li><p><strong>Header Inclusion (Portable)</strong>: Convert CUBIN to a C header file using <code class="docutils literal notranslate"><span class="pre">bin2c</span></code>.</p></li>
<li><p><strong>C++ Embedding (Modern)</strong>: Use C++23 <code class="docutils literal notranslate"><span class="pre">#embed</span></code> (or compiler extensions).</p></li>
</ol>
<p><strong>Method 1: Object Linking (Standard)</strong></p>
<p>This approach uses CMake utilities to compile and link the CUBIN data. It works across all supported compilers and handles the low-level details of object file generation and symbol naming.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Embed CUBIN module with name &quot;env&quot;</span>
<span class="c1">// This creates the necessary symbols and singleton struct for accessing the embedded CUBIN</span>
<span class="n">TVM_FFI_EMBED_CUBIN</span><span class="p">(</span><span class="n">env</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Method 2: Header Inclusion (Portable)</strong></p>
<p>You can use tools like <code class="docutils literal notranslate"><span class="pre">bin2c</span></code> to generate a header file containing the byte array and include it.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">TVM_FFI_EMBED_CUBIN_FROM_BYTES</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">imageBytes</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Method 3: C++ Embedding (Modern)</strong></p>
<p>Using C++23 <code class="docutils literal notranslate"><span class="pre">#embed</span></code> (or compiler extensions like <code class="docutils literal notranslate"><span class="pre">#embed</span></code> in Clang/GCC) allows you to include the binary data directly.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">constexpr</span><span class="w"> </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="n">image</span><span class="p">[]{</span>
<span class="c1">// clang &gt;= 20 or gcc &gt;= 14</span>
<span class="cp">#embed &quot;kernel_fatbin.fatbin&quot;</span>
<span class="p">};</span>

<span class="n">TVM_FFI_EMBED_CUBIN_FROM_BYTES</span><span class="p">(</span><span class="n">env</span><span class="p">,</span><span class="w"> </span><span class="n">image</span><span class="p">);</span>
</pre></div>
</div>
<p><strong>Key Points:</strong></p>
<ul class="simple">
<li><p>Use <code class="docutils literal notranslate"><span class="pre">static</span> <span class="pre">auto</span> <span class="pre">kernel</span></code> to cache the kernel lookup for efficiency</p></li>
<li><p>Kernel arguments must be pointers to the actual values (use <code class="docutils literal notranslate"><span class="pre">&amp;</span></code> for addresses)</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/structtvm_1_1ffi_1_1dim3.html#_CPPv4N3tvm3ffi4dim3E" title="tvm::ffi::dim3"><code class="xref cpp cpp-type docutils literal notranslate"><span class="pre">tvm::ffi::dim3</span></code></a> supports 1D, 2D, or 3D configurations: <code class="docutils literal notranslate"><span class="pre">dim3(x)</span></code>, <code class="docutils literal notranslate"><span class="pre">dim3(x,</span> <span class="pre">y)</span></code>, <code class="docutils literal notranslate"><span class="pre">dim3(x,</span> <span class="pre">y,</span> <span class="pre">z)</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TVMFFIEnvGetStream</span></code> retrieves the correct CUDA stream for the device</p></li>
<li><p>Always check kernel launch results with <a class="reference internal" href="../reference/cpp/generated/define_cuda_2base_8h_1a8b7e871d0d7206eacc60705f11827ee1.html#c.TVM_FFI_CHECK_CUDA_ERROR" title="TVM_FFI_CHECK_CUDA_ERROR"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_CHECK_CUDA_ERROR</span></code></a> (which checks CUDA Runtime API or Driver API errors depending on configuration)</p></li>
</ul>
</section>
<section id="loading-cubin-at-runtime">
<h3>Loading CUBIN at Runtime<a class="headerlink" href="#loading-cubin-at-runtime" title="Link to this heading">#</a></h3>
<p>You can also load CUBIN modules dynamically from memory:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;tvm/ffi/container/tensor.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;tvm/ffi/error.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;tvm/ffi/extra/c_env_api.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;tvm/ffi/extra/cuda/cubin_launcher.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;tvm/ffi/function.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;tvm/ffi/string.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;cstdint&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;memory&gt;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">cubin_dynamic</span><span class="w"> </span><span class="p">{</span>

<span class="c1">// Global CUBIN module and kernels (loaded dynamically)</span>
<span class="k">static</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">CubinModule</span><span class="o">&gt;</span><span class="w"> </span><span class="n">g_cubin_module</span><span class="p">;</span>
<span class="k">static</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">CubinKernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">g_add_one_kernel</span><span class="p">;</span>
<span class="k">static</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">unique_ptr</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">CubinKernel</span><span class="o">&gt;</span><span class="w"> </span><span class="n">g_mul_two_kernel</span><span class="p">;</span>

<span class="cm">/*!</span>
<span class="cm"> * \brief Set CUBIN module from binary data.</span>
<span class="cm"> * \param cubin CUBIN binary data as Bytes object.</span>
<span class="cm"> */</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">SetCubin</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">Bytes</span><span class="o">&amp;</span><span class="w"> </span><span class="n">cubin</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="c1">// Load CUBIN module from memory</span>
<span class="w">  </span><span class="n">g_cubin_module</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">CubinModule</span><span class="o">&gt;</span><span class="p">(</span><span class="n">cubin</span><span class="p">);</span>
<span class="w">  </span><span class="n">g_add_one_kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">CubinKernel</span><span class="o">&gt;</span><span class="p">((</span><span class="o">*</span><span class="n">g_cubin_module</span><span class="p">)[</span><span class="s">&quot;add_one_cuda&quot;</span><span class="p">]);</span>
<span class="w">  </span><span class="n">g_mul_two_kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_unique</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">CubinKernel</span><span class="o">&gt;</span><span class="p">((</span><span class="o">*</span><span class="n">g_cubin_module</span><span class="p">)[</span><span class="s">&quot;mul_two_cuda&quot;</span><span class="p">]);</span>
<span class="p">}</span>

<span class="cm">/*!</span>
<span class="cm"> * \brief Launch add_one_cuda kernel on input tensor.</span>
<span class="cm"> * \param x Input tensor (float32, 1D)</span>
<span class="cm"> * \param y Output tensor (float32, 1D, same shape as x)</span>
<span class="cm"> */</span>
<span class="kt">void</span><span class="w"> </span><span class="nf">AddOne</span><span class="p">(</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">TensorView</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">TensorView</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">TVM_FFI_CHECK</span><span class="p">(</span><span class="n">g_cubin_module</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="k">nullptr</span><span class="p">,</span><span class="w"> </span><span class="n">RuntimeError</span><span class="p">)</span>
<span class="w">      </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;CUBIN module not loaded. Call set_cubin first.&quot;</span><span class="p">;</span>

<span class="w">  </span><span class="n">TVM_FFI_CHECK</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">ndim</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">ValueError</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input must be 1D tensor&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">TVM_FFI_CHECK</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">ndim</span><span class="p">()</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">ValueError</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Output must be 1D tensor&quot;</span><span class="p">;</span>
<span class="w">  </span><span class="n">TVM_FFI_CHECK</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">y</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="w"> </span><span class="n">ValueError</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;Input and output must have same size&quot;</span><span class="p">;</span>

<span class="w">  </span><span class="kt">int64_t</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">x_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">data_ptr</span><span class="p">();</span>
<span class="w">  </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">y_ptr</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">y</span><span class="p">.</span><span class="n">data_ptr</span><span class="p">();</span>

<span class="w">  </span><span class="c1">// Prepare kernel arguments</span>
<span class="w">  </span><span class="kt">void</span><span class="o">*</span><span class="w"> </span><span class="n">args</span><span class="p">[]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">{</span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">x_ptr</span><span class="p">),</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">y_ptr</span><span class="p">),</span>
<span class="w">                  </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="kt">void</span><span class="o">*&gt;</span><span class="p">(</span><span class="o">&amp;</span><span class="n">n</span><span class="p">)};</span>

<span class="w">  </span><span class="c1">// Launch configuration</span>
<span class="w">  </span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">dim3</span><span class="w"> </span><span class="n">grid</span><span class="p">((</span><span class="n">n</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">255</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">256</span><span class="p">);</span>
<span class="w">  </span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">dim3</span><span class="w"> </span><span class="n">block</span><span class="p">(</span><span class="mi">256</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Get CUDA stream</span>
<span class="w">  </span><span class="n">DLDevice</span><span class="w"> </span><span class="n">device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">();</span>
<span class="w">  </span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">cuda_api</span><span class="o">::</span><span class="n">StreamHandle</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">cuda_api</span><span class="o">::</span><span class="n">StreamHandle</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">      </span><span class="n">TVMFFIEnvGetStream</span><span class="p">(</span><span class="n">device</span><span class="p">.</span><span class="n">device_type</span><span class="p">,</span><span class="w"> </span><span class="n">device</span><span class="p">.</span><span class="n">device_id</span><span class="p">));</span>

<span class="w">  </span><span class="c1">// Launch kernel</span>
<span class="w">  </span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">cuda_api</span><span class="o">::</span><span class="n">ResultType</span><span class="w"> </span><span class="n">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">g_add_one_kernel</span><span class="o">-&gt;</span><span class="n">Launch</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">);</span>
<span class="w">  </span><span class="n">TVM_FFI_CHECK_CUBIN_LAUNCHER_CUDA_ERROR</span><span class="p">(</span><span class="n">result</span><span class="p">);</span>
<span class="p">}</span>

<span class="p">}</span><span class="w">  </span><span class="c1">// namespace cubin_dynamic</span>
</pre></div>
</div>
</section>
<section id="embedding-cubin-with-cmake-utilities">
<h3>Embedding CUBIN with CMake Utilities<a class="headerlink" href="#embedding-cubin-with-cmake-utilities" title="Link to this heading">#</a></h3>
<p>TVM-FFI provides CMake utility functions that simplify the CUBIN embedding process. This is the recommended approach for CMake-based projects.</p>
<p><strong>Using CMake Utilities:</strong></p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="c"># Step 1: Compile kernel.cu to FATBIN using add_tvm_ffi_fatbin utility or `CUDA_FATBIN_COMPILATION`</span>
<span class="nb">set</span><span class="p">(</span><span class="s">CMAKE_CUDA_ARCHITECTURES</span><span class="w"> </span><span class="s">75;80;86;89;90;100;120</span><span class="p">)</span>
<span class="nb">if</span><span class="w"> </span><span class="p">(</span><span class="s">CMAKE_VERSION</span><span class="w"> </span><span class="s">VERSION_LESS</span><span class="w"> </span><span class="s2">&quot;3.27.0&quot;</span><span class="p">)</span>
<span class="w">  </span><span class="nb">add_tvm_ffi_fatbin</span><span class="p">(</span><span class="s">kernel_fatbin</span><span class="w"> </span><span class="s">CUDA</span><span class="w"> </span><span class="s">src/kernel.cu</span><span class="p">)</span>
<span class="nb">else</span><span class="w"> </span><span class="p">()</span>
<span class="w">  </span><span class="nb">add_library</span><span class="p">(</span><span class="s">kernel_fatbin</span><span class="w"> </span><span class="s">OBJECT</span><span class="w"> </span><span class="s">src/kernel.cu</span><span class="p">)</span>
<span class="w">  </span><span class="nb">set_target_properties</span><span class="p">(</span><span class="s">kernel_fatbin</span><span class="w"> </span><span class="s">PROPERTIES</span><span class="w"> </span><span class="s">CUDA_FATBIN_COMPILATION</span><span class="w"> </span><span class="s">ON</span><span class="p">)</span>
<span class="nb">endif</span><span class="w"> </span><span class="p">()</span>

<span class="c"># Step 2: Build lib_embedded shared library</span>
<span class="nb">add_library</span><span class="p">(</span><span class="s">lib_embedded</span><span class="w"> </span><span class="s">SHARED</span><span class="w"> </span><span class="s">src/lib_embedded.cc</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">lib_embedded</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">tvm_ffi::header</span><span class="w"> </span><span class="s">tvm_ffi::shared</span><span class="p">)</span>
<span class="nb">set_target_properties</span><span class="p">(</span><span class="s">lib_embedded</span><span class="w"> </span><span class="s">PROPERTIES</span><span class="w"> </span><span class="s">POSITION_INDEPENDENT_CODE</span><span class="w"> </span><span class="s">ON</span><span class="p">)</span>

<span class="c"># Step 3: Link against CUDA Driver API or Runtime API based on config</span>
<span class="nb">if</span><span class="w"> </span><span class="p">(</span><span class="s">CMAKE_TVM_FFI_CUBIN_LAUNCHER_USE_DRIVER_API</span><span class="p">)</span>
<span class="w">  </span><span class="nb">add_compile_definitions</span><span class="p">(</span><span class="s">TVM_FFI_CUBIN_LAUNCHER_USE_DRIVER_API=1</span><span class="p">)</span>
<span class="w">  </span><span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">lib_embedded</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">cuda</span><span class="p">)</span>
<span class="nb">else</span><span class="w"> </span><span class="p">()</span>
<span class="w">  </span><span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">lib_embedded</span><span class="w"> </span><span class="s">PRIVATE</span><span class="w"> </span><span class="s">CUDA::cudart</span><span class="p">)</span>
<span class="nb">endif</span><span class="w"> </span><span class="p">()</span>

<span class="c"># Step 4: Embed CUBIN into shared library just defined, using tvm_ffi_embed_cubin utility This</span>
<span class="c"># creates symbols: __tvm_ffi__cubin_env (local)</span>
<span class="nb">tvm_ffi_embed_bin_into</span><span class="p">(</span><span class="s">lib_embedded</span><span class="w"> </span><span class="s">SYMBOL</span><span class="w"> </span><span class="s">env</span><span class="w"> </span><span class="s">BIN</span><span class="w"> </span><span class="s2">&quot;$&lt;TARGET_OBJECTS:kernel_fatbin&gt;&quot;</span><span class="p">)</span>

<span class="nb">set_target_properties</span><span class="p">(</span>
<span class="w">  </span><span class="s">lib_embedded</span>
<span class="w">  </span><span class="s">PROPERTIES</span><span class="w"> </span><span class="s">PREFIX</span><span class="w"> </span><span class="s2">&quot;&quot;</span>
<span class="w">             </span><span class="s">SUFFIX</span><span class="w"> </span><span class="s2">&quot;.so&quot;</span>
<span class="w">             </span><span class="s">LINKER_LANGUAGE</span><span class="w"> </span><span class="s">CXX</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Available CMake Functions:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">add_tvm_ffi_cubin(&lt;target&gt;</span> <span class="pre">CUDA</span> <span class="pre">&lt;source&gt;)</span></code>:
Creates an object library that compiles CUDA source to CUBIN format.
This is a compatibility wrapper; for CMake &gt;= 3.27, you can use standard <code class="docutils literal notranslate"><span class="pre">CUDA_CUBIN_COMPILATION</span></code> property.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add_tvm_ffi_fatbin(&lt;target&gt;</span> <span class="pre">CUDA</span> <span class="pre">&lt;source&gt;)</span></code>:
Creates an object library that compiles CUDA source to FATBIN format.
This is a compatibility wrapper; for CMake &gt;= 3.27, you can use standard <code class="docutils literal notranslate"><span class="pre">CUDA_FATBIN_COMPILATION</span></code> property.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tvm_ffi_embed_bin_into(&lt;target&gt;</span> <span class="pre">SYMBOL</span> <span class="pre">&lt;symbol&gt;</span> <span class="pre">BIN</span> <span class="pre">&lt;bin_file&gt;)</span></code>:
Embeds a CUBIN/FATBIN file into an existing object library target.
This works by linking the binary data into the target, allowing access via <code class="docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN(&lt;name&gt;)</span></code>.</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">target</span></code>: The target to embed into (must be an object library or have object files).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">symbol</span></code>: Symbol name to use (must match <code class="docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN(symbol)</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">BIN</span></code>: Path to the CUBIN/FATBIN file (e.g., from <code class="docutils literal notranslate"><span class="pre">$&lt;TARGET_OBJECTS:...&gt;</span></code>).</p></li>
</ul>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When including <code class="docutils literal notranslate"><span class="pre">cmake/Utils/EmbedCubin.cmake</span></code>, if <code class="docutils literal notranslate"><span class="pre">CMAKE_CUDA_RUNTIME_LIBRARY</span></code> is not set, it defaults to <code class="docutils literal notranslate"><span class="pre">Shared</span></code>.
This prevents static linking of cudart, which requires an exact driver version match.
If you intend to use the Driver API only (e.g., via <code class="docutils literal notranslate"><span class="pre">TVM_FFI_CUBIN_LAUNCHER_USE_DRIVER_API=1</span></code>),
you should explicitly set <code class="docutils literal notranslate"><span class="pre">CMAKE_CUDA_RUNTIME_LIBRARY</span></code> to <code class="docutils literal notranslate"><span class="pre">None</span></code> in your CMake configuration before including this utility to avoid linking against the CUDA runtime library.
And link with CUDA Driver API.</p>
</div>
</section>
<section id="embedding-cubin-with-python-utility">
<h3>Embedding CUBIN with Python Utility<a class="headerlink" href="#embedding-cubin-with-python-utility" title="Link to this heading">#</a></h3>
<p>For more advanced use cases or non-CMake build systems, you can use the Python command-line utility to embed CUBIN data into existing object files.</p>
<p><strong>Command-Line Usage:</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Step 1: Compile C++ source to object file</span>
g++<span class="w"> </span>-c<span class="w"> </span>-fPIC<span class="w"> </span>-std<span class="o">=</span>c++17<span class="w"> </span>-I/path/to/tvm-ffi/include<span class="w"> </span>mycode.cc<span class="w"> </span>-o<span class="w"> </span>mycode.o

<span class="c1"># Step 2: Embed CUBIN into the object file</span>
python<span class="w"> </span>-m<span class="w"> </span>tvm_ffi.utils.embed_cubin<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--output-obj<span class="w"> </span>mycode_with_cubin.o<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--input-obj<span class="w"> </span>mycode.o<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--cubin<span class="w"> </span>kernel.cubin<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--name<span class="w"> </span>my_kernels

<span class="c1"># Step 3: Link into final library</span>
g++<span class="w"> </span>-o<span class="w"> </span>mylib.so<span class="w"> </span>-shared<span class="w"> </span>mycode_with_cubin.o<span class="w"> </span>-lcudart
</pre></div>
</div>
<p><strong>Python API:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm_ffi.utils.embed_cubin</span><span class="w"> </span><span class="kn">import</span> <span class="n">embed_cubin</span>

<span class="n">embed_cubin</span><span class="p">(</span>
    <span class="n">cubin_path</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;kernel.cubin&quot;</span><span class="p">),</span>
    <span class="n">input_obj_path</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;mycode.o&quot;</span><span class="p">),</span>
    <span class="n">output_obj_path</span><span class="o">=</span><span class="n">Path</span><span class="p">(</span><span class="s2">&quot;mycode_with_cubin.o&quot;</span><span class="p">),</span>
    <span class="n">name</span><span class="o">=</span><span class="s2">&quot;my_kernels&quot;</span><span class="p">,</span>
    <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span>  <span class="c1"># Optional: print detailed progress</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The Python utility performs these steps:</p>
<ol class="arabic simple">
<li><p>Creates intermediate CUBIN object file using <code class="docutils literal notranslate"><span class="pre">ld</span> <span class="pre">-r</span> <span class="pre">-b</span> <span class="pre">binary</span></code></p></li>
<li><p>Adds <code class="docutils literal notranslate"><span class="pre">.note.GNU-stack</span></code> section for security</p></li>
<li><p>Renames symbols to match TVM-FFI format (<code class="docutils literal notranslate"><span class="pre">__tvm_ffi__cubin_&lt;name&gt;</span></code>)</p></li>
<li><p>Merges with input object file using relocatable linking</p></li>
<li><p>Localizes symbols to prevent conflicts when multiple object files use the same name</p></li>
</ol>
</section>
<section id="manual-cubin-embedding">
<h3>Manual CUBIN Embedding<a class="headerlink" href="#manual-cubin-embedding" title="Link to this heading">#</a></h3>
<p>For reference, here’s how to manually embed CUBIN using objcopy and ld:</p>
<p><strong>Step 1: Compile CUDA kernel to CUBIN</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>nvcc<span class="w"> </span>--cubin<span class="w"> </span>-arch<span class="o">=</span>sm_75<span class="w"> </span>kernel.cu<span class="w"> </span>-o<span class="w"> </span>kernel.cubin
</pre></div>
</div>
<p><strong>Step 2: Convert CUBIN to object file</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>ld<span class="w"> </span>-r<span class="w"> </span>-b<span class="w"> </span>binary<span class="w"> </span>-o<span class="w"> </span>kernel_data.o<span class="w"> </span>kernel.cubin
</pre></div>
</div>
<p><strong>Step 3: Rename symbols with objcopy</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>objcopy<span class="w"> </span>--rename-section<span class="w"> </span>.data<span class="o">=</span>.rodata,alloc,load,readonly,data,contents<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--redefine-sym<span class="w"> </span><span class="nv">_binary_kernel_cubin_start</span><span class="o">=</span>__tvm_ffi__cubin_my_kernels<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>--redefine-sym<span class="w"> </span><span class="nv">_binary_kernel_cubin_end</span><span class="o">=</span>__tvm_ffi__cubin_my_kernels_end<span class="w"> </span><span class="se">\</span>
<span class="w">        </span>kernel_data.o
</pre></div>
</div>
<p><strong>Step 4: Link with your library</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>g++<span class="w"> </span>-o<span class="w"> </span>mylib.so<span class="w"> </span>-shared<span class="w"> </span>mycode.cc<span class="w"> </span>kernel_data.o<span class="w"> </span>-Wl,-z,noexecstack<span class="w"> </span>-lcudart
</pre></div>
</div>
<p>The symbol names must match the name used in <a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1a99736a44462543179cb434ebd4512ade.html#c.TVM_FFI_EMBED_CUBIN" title="TVM_FFI_EMBED_CUBIN"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN</span></code></a>.</p>
<p><strong>When to Use Each Approach:</strong></p>
<ul class="simple">
<li><p><strong>CMake utilities</strong>: Best for CMake-based projects, provides cleanest integration (recommended)</p></li>
<li><p><strong>Python utility</strong>: Best for custom build systems, Makefile-based projects, or advanced workflows (recommended)</p></li>
<li><p><strong>Manual objcopy</strong>: Low-level approach, useful for understanding the process or debugging (only for customized use cases)</p></li>
</ul>
</section>
</section>
<section id="advanced-topics">
<h2>Advanced Topics<a class="headerlink" href="#advanced-topics" title="Link to this heading">#</a></h2>
<section id="multi-gpu-support">
<h3>Multi-GPU Support<a class="headerlink" href="#multi-gpu-support" title="Link to this heading">#</a></h3>
<p>The CUBIN launcher automatically handles multi-GPU execution through CUDA primary contexts. Kernels will execute on the device associated with the input tensors:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="kt">void</span><span class="w"> </span><span class="nf">MultiGPUExample</span><span class="p">(</span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">TensorView</span><span class="w"> </span><span class="n">x_gpu0</span><span class="p">,</span><span class="w"> </span><span class="n">tvm</span><span class="o">::</span><span class="n">ffi</span><span class="o">::</span><span class="n">TensorView</span><span class="w"> </span><span class="n">x_gpu1</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">static</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">kernel</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">TVM_FFI_EMBED_CUBIN_GET_KERNEL</span><span class="p">(</span><span class="n">my_kernels</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;process&quot;</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Launch on GPU 0 (device determined by x_gpu0.device())</span>
<span class="w">  </span><span class="n">LaunchOnDevice</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">x_gpu0</span><span class="p">);</span>

<span class="w">  </span><span class="c1">// Launch on GPU 1 (device determined by x_gpu1.device())</span>
<span class="w">  </span><span class="n">LaunchOnDevice</span><span class="p">(</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="n">x_gpu1</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The <a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinKernel.html#_CPPv4N3tvm3ffi11CubinKernelE" title="tvm::ffi::CubinKernel"><code class="xref cpp cpp-class docutils literal notranslate"><span class="pre">tvm::ffi::CubinKernel</span></code></a> automatically uses the device context from the input tensors.</p>
</section>
<section id="kernel-launch-configuration">
<h3>Kernel Launch Configuration<a class="headerlink" href="#kernel-launch-configuration" title="Link to this heading">#</a></h3>
<p>When writing the C++ wrapper, important considerations include:</p>
<ul class="simple">
<li><p><strong>Grid/Block Dimensions</strong>: Use <a class="reference internal" href="../reference/cpp/generated/structtvm_1_1ffi_1_1dim3.html#_CPPv4N3tvm3ffi4dim3E" title="tvm::ffi::dim3"><code class="xref cpp cpp-type docutils literal notranslate"><span class="pre">tvm::ffi::dim3</span></code></a> for 1D, 2D, or 3D configurations</p>
<ul>
<li><p>1D: <code class="docutils literal notranslate"><span class="pre">dim3(x)</span></code> → <code class="docutils literal notranslate"><span class="pre">(x,</span> <span class="pre">1,</span> <span class="pre">1)</span></code></p></li>
<li><p>2D: <code class="docutils literal notranslate"><span class="pre">dim3(x,</span> <span class="pre">y)</span></code> → <code class="docutils literal notranslate"><span class="pre">(x,</span> <span class="pre">y,</span> <span class="pre">1)</span></code></p></li>
<li><p>3D: <code class="docutils literal notranslate"><span class="pre">dim3(x,</span> <span class="pre">y,</span> <span class="pre">z)</span></code> → <code class="docutils literal notranslate"><span class="pre">(x,</span> <span class="pre">y,</span> <span class="pre">z)</span></code></p></li>
</ul>
</li>
<li><p><strong>Kernel Arguments</strong>: Must be pointers to actual values</p>
<ul>
<li><p>For device pointers: <code class="docutils literal notranslate"><span class="pre">void*</span> <span class="pre">ptr</span> <span class="pre">=</span> <span class="pre">tensor.data_ptr();</span> <span class="pre">args[]</span> <span class="pre">=</span> <span class="pre">{&amp;ptr}</span></code></p></li>
<li><p>For scalars: <code class="docutils literal notranslate"><span class="pre">int</span> <span class="pre">n</span> <span class="pre">=</span> <span class="pre">42;</span> <span class="pre">args[]</span> <span class="pre">=</span> <span class="pre">{&amp;n}</span></code></p></li>
</ul>
</li>
<li><p><strong>Stream Management</strong>: Use <code class="docutils literal notranslate"><span class="pre">TVMFFIEnvGetStream</span></code> to get the correct CUDA stream for synchronization with DLPack tensors</p></li>
<li><p><strong>Error Checking</strong>: Always use <a class="reference internal" href="../reference/cpp/generated/define_cuda_2base_8h_1a8b7e871d0d7206eacc60705f11827ee1.html#c.TVM_FFI_CHECK_CUDA_ERROR" title="TVM_FFI_CHECK_CUDA_ERROR"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_CHECK_CUDA_ERROR</span></code></a> to validate CUDA Runtime API results</p></li>
</ul>
</section>
<section id="dynamic-shared-memory">
<h3>Dynamic Shared Memory<a class="headerlink" href="#dynamic-shared-memory" title="Link to this heading">#</a></h3>
<p>To use dynamic shared memory, specify the size in the <a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinKernel.html#_CPPv4N3tvm3ffi11CubinKernel6LaunchEPPv4dim34dim3N8cuda_api12StreamHandleE8uint32_t" title="tvm::ffi::CubinKernel::Launch"><code class="xref cpp cpp-func docutils literal notranslate"><span class="pre">tvm::ffi::CubinKernel::Launch()</span></code></a> call:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="c1">// Allocate 1KB of dynamic shared memory</span>
<span class="kt">uint32_t</span><span class="w"> </span><span class="n">shared_mem_bytes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="n">TVM_FFI_CHECK_CUBIN_LAUNCHER_CUDA_ERROR</span><span class="p">(</span><span class="n">kernel</span><span class="p">.</span><span class="n">Launch</span><span class="p">(</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="n">grid</span><span class="p">,</span><span class="w"> </span><span class="n">block</span><span class="p">,</span><span class="w"> </span><span class="n">stream</span><span class="p">,</span><span class="w"> </span><span class="n">shared_mem_bytes</span><span class="p">));</span>
</pre></div>
</div>
</section>
<section id="integration-with-different-compilers">
<h3>Integration with Different Compilers<a class="headerlink" href="#integration-with-different-compilers" title="Link to this heading">#</a></h3>
<p>The CUBIN launcher works with various CUDA compilation tools:</p>
<ul class="simple">
<li><p><strong>NVCC</strong>: Standard NVIDIA compiler, produces highly optimized CUBIN</p></li>
<li><p><strong>NVRTC</strong>: Runtime compilation for JIT scenarios (via <code class="xref py py-mod docutils literal notranslate"><span class="pre">tvm_ffi.cpp.nvrtc</span></code>)</p></li>
<li><p><strong>Triton</strong>: High-level DSL that compiles to CUBIN</p></li>
<li><p><strong>Custom compilers</strong>: Any tool that generates valid CUDA CUBIN</p></li>
</ul>
</section>
</section>
<section id="complete-examples">
<h2>Complete Examples<a class="headerlink" href="#complete-examples" title="Link to this heading">#</a></h2>
<p>For complete working examples, see the <code class="docutils literal notranslate"><span class="pre">examples/cubin_launcher/</span></code> directory:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">embedded_cubin/</span></code> - Pre-compiled CUBIN embedded at build time</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dynamic_cubin/</span></code> - CUBIN data passed dynamically at runtime</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">example_nvrtc_cubin.py</span></code> - NVRTC runtime compilation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">example_triton_cubin.py</span></code> - Triton kernel compilation</p></li>
</ul>
<p>These examples demonstrate:</p>
<ul class="simple">
<li><p>Compiling CUDA kernels to CUBIN</p></li>
<li><p>Embedding CUBIN in C++ modules</p></li>
<li><p>Launching kernels with proper error handling</p></li>
<li><p>Testing and verification</p></li>
</ul>
</section>
<section id="api-reference">
<h2>API Reference<a class="headerlink" href="#api-reference" title="Link to this heading">#</a></h2>
<section id="c-classes">
<h3>C++ Classes<a class="headerlink" href="#c-classes" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinModule.html#_CPPv4N3tvm3ffi11CubinModuleE" title="tvm::ffi::CubinModule"><code class="xref cpp cpp-class docutils literal notranslate"><span class="pre">tvm::ffi::CubinModule</span></code></a>: RAII wrapper for CUBIN module lifecycle</p>
<ul>
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinModule.html#_CPPv4N3tvm3ffi11CubinModule11CubinModuleERK5Bytes" title="tvm::ffi::CubinModule::CubinModule"><code class="xref cpp cpp-func docutils literal notranslate"><span class="pre">tvm::ffi::CubinModule::CubinModule()</span></code></a>: Load CUBIN from memory</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinModule.html#_CPPv4N3tvm3ffi11CubinModule9GetKernelEPKc" title="tvm::ffi::CubinModule::GetKernel"><code class="xref cpp cpp-func docutils literal notranslate"><span class="pre">tvm::ffi::CubinModule::GetKernel()</span></code></a>: Get kernel by name</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinModule.html#_CPPv4N3tvm3ffi11CubinModule35GetKernelWithMaxDynamicSharedMemoryEPKc7int64_t" title="tvm::ffi::CubinModule::GetKernelWithMaxDynamicSharedMemory"><code class="xref cpp cpp-func docutils literal notranslate"><span class="pre">tvm::ffi::CubinModule::GetKernelWithMaxDynamicSharedMemory()</span></code></a>: Get kernel by name with maximum dynamic shared memory set</p></li>
<li><p><code class="xref cpp cpp-func docutils literal notranslate"><span class="pre">tvm::ffi::CubinModule::operator[]()</span></code>: Convenient kernel access</p></li>
</ul>
</li>
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinKernel.html#_CPPv4N3tvm3ffi11CubinKernelE" title="tvm::ffi::CubinKernel"><code class="xref cpp cpp-class docutils literal notranslate"><span class="pre">tvm::ffi::CubinKernel</span></code></a>: Handle for launching kernels</p>
<ul>
<li><p><a class="reference internal" href="../reference/cpp/generated/classtvm_1_1ffi_1_1CubinKernel.html#_CPPv4N3tvm3ffi11CubinKernel6LaunchEPPv4dim34dim3N8cuda_api12StreamHandleE8uint32_t" title="tvm::ffi::CubinKernel::Launch"><code class="xref cpp cpp-func docutils literal notranslate"><span class="pre">tvm::ffi::CubinKernel::Launch()</span></code></a>: Launch kernel with specified parameters</p></li>
</ul>
</li>
<li><p><a class="reference internal" href="../reference/cpp/generated/structtvm_1_1ffi_1_1dim3.html#_CPPv4N3tvm3ffi4dim3E" title="tvm::ffi::dim3"><code class="xref cpp cpp-type docutils literal notranslate"><span class="pre">tvm::ffi::dim3</span></code></a>: 3D dimension structure</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3()</span></code>: Default (1, 1, 1)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3(unsigned</span> <span class="pre">int</span> <span class="pre">x)</span></code>: 1D</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3(unsigned</span> <span class="pre">int</span> <span class="pre">x,</span> <span class="pre">unsigned</span> <span class="pre">int</span> <span class="pre">y)</span></code>: 2D</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dim3(unsigned</span> <span class="pre">int</span> <span class="pre">x,</span> <span class="pre">unsigned</span> <span class="pre">int</span> <span class="pre">y,</span> <span class="pre">unsigned</span> <span class="pre">int</span> <span class="pre">z)</span></code>: 3D</p></li>
</ul>
</li>
</ul>
</section>
<section id="c-macros">
<h3>C++ Macros<a class="headerlink" href="#c-macros" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1a99736a44462543179cb434ebd4512ade.html#c.TVM_FFI_EMBED_CUBIN" title="TVM_FFI_EMBED_CUBIN"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN</span></code></a>: Declare embedded CUBIN module</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1a55832b50e83cf39108f1e306f031433d.html#c.TVM_FFI_EMBED_CUBIN_FROM_BYTES" title="TVM_FFI_EMBED_CUBIN_FROM_BYTES"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN_FROM_BYTES</span></code></a>: Load CUBIN from byte array</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/define_cubin__launcher_8h_1ae8d64fa1cc7db9d38632e32054df72fc.html#c.TVM_FFI_EMBED_CUBIN_GET_KERNEL" title="TVM_FFI_EMBED_CUBIN_GET_KERNEL"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN_GET_KERNEL</span></code></a>: Get kernel from embedded module</p></li>
<li><p><a class="reference internal" href="../reference/cpp/generated/define_cuda_2base_8h_1a8b7e871d0d7206eacc60705f11827ee1.html#c.TVM_FFI_CHECK_CUDA_ERROR" title="TVM_FFI_CHECK_CUDA_ERROR"><code class="xref c c-macro docutils literal notranslate"><span class="pre">TVM_FFI_CHECK_CUDA_ERROR</span></code></a>: Check CUDA Runtime API result</p></li>
</ul>
</section>
<section id="python-functions">
<h3>Python Functions<a class="headerlink" href="#python-functions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.nvrtc.nvrtc_compile.html#tvm_ffi.cpp.nvrtc.nvrtc_compile" title="tvm_ffi.cpp.nvrtc.nvrtc_compile"><code class="xref py py-func docutils literal notranslate"><span class="pre">tvm_ffi.cpp.nvrtc.nvrtc_compile()</span></code></a>: Compile CUDA source to CUBIN</p></li>
<li><p><a class="reference internal" href="../reference/python/cpp/generated/tvm_ffi.cpp.load_inline.html#tvm_ffi.cpp.load_inline" title="tvm_ffi.cpp.load_inline"><code class="xref py py-func docutils literal notranslate"><span class="pre">tvm_ffi.cpp.load_inline()</span></code></a>: Load inline module with embedded CUBIN</p></li>
</ul>
</section>
<section id="python-utilities">
<h3>Python Utilities<a class="headerlink" href="#python-utilities" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">tvm_ffi.utils.embed_cubin</span></code>: Command-line utility to embed CUBIN into object files</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">--output-obj</span> <span class="pre">PATH</span></code>: Output combined object file path</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--input-obj</span> <span class="pre">PATH</span></code>: Input compiled object file containing C++ code with <code class="docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--cubin</span> <span class="pre">PATH</span></code>: Input CUBIN file to embed</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--name</span> <span class="pre">NAME</span></code>: Symbol name matching <code class="docutils literal notranslate"><span class="pre">TVM_FFI_EMBED_CUBIN(name)</span></code> macro</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">--verbose</span></code>: Print detailed command output (optional)</p></li>
</ul>
</li>
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">tvm_ffi.utils.embed_cubin.embed_cubin()</span></code>: Python API for embedding CUBIN</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">cubin_path</span></code>: Path to input CUBIN file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">input_obj_path</span></code>: Path to existing object file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_obj_path</span></code>: Path to output combined object file</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">name</span></code>: Symbol name for the embedded CUBIN</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">verbose</span></code>: Enable detailed output (default: False)</p></li>
</ul>
</li>
</ul>
</section>
<section id="cmake-functions">
<h3>CMake Functions<a class="headerlink" href="#cmake-functions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">add_tvm_ffi_cubin(&lt;target&gt;</span> <span class="pre">CUDA</span> <span class="pre">&lt;source&gt;)</span></code>: Compile CUDA source to CUBIN</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">add_tvm_ffi_fatbin(&lt;target&gt;</span> <span class="pre">CUDA</span> <span class="pre">&lt;source&gt;)</span></code>: Compile CUDA source to FATBIN</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tvm_ffi_embed_bin_into(&lt;target&gt;</span> <span class="pre">SYMBOL</span> <span class="pre">&lt;symbol&gt;</span> <span class="pre">BIN</span> <span class="pre">&lt;bin_file&gt;)</span></code>: Embed CUBIN/FATBIN into object target</p></li>
</ul>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="compiler_integration.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Compiler Integration</p>
      </div>
    </a>
    <a class="right-next"
       href="python_lang_guide.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Python Guide</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-usage">Python Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-workflow">Basic Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-nvrtc-compilation">Example: NVRTC Compilation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-using-triton-kernels">Example: Using Triton Kernels</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-usage">C++ Usage</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-cubin-at-compile-time">Embedding CUBIN at Compile Time</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-cubin-at-runtime">Loading CUBIN at Runtime</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-cubin-with-cmake-utilities">Embedding CUBIN with CMake Utilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#embedding-cubin-with-python-utility">Embedding CUBIN with Python Utility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-cubin-embedding">Manual CUBIN Embedding</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-topics">Advanced Topics</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-gpu-support">Multi-GPU Support</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-launch-configuration">Kernel Launch Configuration</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-shared-memory">Dynamic Shared Memory</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-with-different-compilers">Integration with Different Compilers</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-examples">Complete Examples</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#api-reference">API Reference</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-classes">C++ Classes</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-macros">C++ Macros</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-functions">Python Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-utilities">Python Utilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cmake-functions">CMake Functions</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Apache TVM FFI contributors
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
  Last updated on Feb 08, 2026.
  <br/>
</p>
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  
  <div class="footer-container" style="margin: 5px 0; font-size: 0.9em; color: #6c757d;">
      <div class="footer-line1" style="display: flex; justify-content: space-between; align-items: center; margin-bottom: 3px;">
          <div class="footer-copyright-short">
              Copyright © 2025, Apache Software Foundation
          </div>
          <div class="footer-dropdown">
              <div class="dropdown">
                  <button class="btn btn-link dropdown-toggle" type="button" id="footerDropdown" data-bs-toggle="dropdown"
                  aria-expanded="false" style="font-size: 0.9em; color: #6c757d; text-decoration: none; padding: 0; border: none; background: none;">
                      ASF
                  </button>
                  <ul class="dropdown-menu" aria-labelledby="footerDropdown" style="font-size: 0.9em;">
<li><a class="dropdown-item" href="https://apache.org/" target="_blank" style="font-size: 0.9em;">ASF Homepage</a></li>
<li><a class="dropdown-item" href="https://www.apache.org/licenses/" target="_blank" style="font-size: 0.9em;">License</a></li>
<li><a class="dropdown-item" href="https://www.apache.org/foundation/sponsorship.html" target="_blank" style="font-size: 0.9em;">Sponsorship</a></li>
<li><a class="dropdown-item" href="https://tvm.apache.org/docs/reference/security.html" target="_blank" style="font-size: 0.9em;">Security</a></li>
<li><a class="dropdown-item" href="https://www.apache.org/foundation/thanks.html" target="_blank" style="font-size: 0.9em;">Thanks</a></li>
<li><a class="dropdown-item" href="https://www.apache.org/events/current-event" target="_blank" style="font-size: 0.9em;">Events</a></li>
                  </ul>
              </div>
          </div>
      </div>
      <div class="footer-line2" style="font-size: 0.9em; color: #6c757d;">
          Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.
      </div>
  </div>
  
</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>