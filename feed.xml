<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2024-12-03T13:56:53+00:00</updated><id>/feed.xml</id><title type="html">TVM</title><author><name>{&quot;name&quot;=&gt;nil}</name></author><entry><title type="html">Apache TVM Unity: a vision for the ML software &amp;amp; hardware ecosystem in 2022</title><link href="/2021/12/15/tvm-unity" rel="alternate" type="text/html" title="Apache TVM Unity: a vision for the ML software &amp;amp; hardware ecosystem in 2022" /><published>2021-12-15T00:00:00+00:00</published><updated>2021-12-15T00:00:00+00:00</updated><id>/2021/12/15/tvm-unity</id><content type="html" xml:base="/2021/12/15/tvm-unity"><![CDATA[<p>Apache TVM Unity is a roadmap for the TVM ecosystem in 2022. We see a broader shift coming in the way that machine learning system stacks optimize for flexibility and agility in the face of a rapidly changing hardware landscape. TVM will evolve to break down the boundaries that constrain the ways current ML systems adapt to rapid changes in ML models and the accelerators that implement them.</p>

<h2 id="boundaries-in-the-modern-ml-system-stack">Boundaries in the Modern ML System Stack</h2>

<p><img src="/images/tvm-unity/image4.png" alt="image" style="width: 40%; margin: auto; display: block;" /></p>

<p>The system stack for modern machine learning consists of four kinds of abstractions:</p>
<ol>
  <li>The <em>computational graph</em> abstraction encodes the flow of data between coarse-grained tensor operators. Computational graphs are the high-level abstraction users interact with in <a href="https://www.tensorflow.org/">TensorFlow</a>, <a href="https://mxnet.apache.org/">MXNet</a>, and <a href="https://pytorch.org/">PyTorch</a>.</li>
  <li><em>Tensor programs</em> implement the code for the operators in the computational graph. Deep learning compilers generate the low-level C++ or CUDA code for computations like convolutions or matrix multiplications.</li>
  <li>Similarly, <em>libraries and runtimes</em> include pre-written code to execute and orchestrate tensor operations. BLAS packages and libraries like cuDNN provide extensively tuned operator implementations for specific hardware targets.</li>
  <li><em>Hardware primitives</em> are at the bottom of the stack. Here, low-level assembly languages and hardware accelerator interfaces expose the raw capabilities of the machine.</li>
</ol>

<p>There are <em>vertical</em> boundaries between the abstraction levels that prohibit cross-layer interactions and feedback between the levels. There is also a <em>horizontal</em> boundary between two opposing ways that software stacks can treat the central tensor computation level. The horizontal boundary divides <em>library-based</em> and <em>compilation-based</em> approaches to tensor computation.</p>

<p><img src="/images/tvm-unity/image1.png" alt="image" style="width: 70%; margin: auto; display: block;" /></p>

<p>Library-based frameworks rely on collections of pre-made, carefully tuned operator implementations as their computational workhorse. Compilation-based frameworks instead generate their own custom tensor operation code from scratch.  Modern software stacks typically use one style or the other, but they don’t combine them: most deep learning frameworks are library-based, while most deep learning compilers cannot incorporate libraries and runtimes.</p>

<p>In the current landscape of ML systems, the boundaries between these layers tend to be strict. Neither approach is better than the other, but they have trade-offs. Library-based stacks excel on standard styles of ML models because they benefit from years of engineering investment common operators. On the other side, the flexibility and automation in compilation-based frameworks can be better for emerging models that require new operators.</p>

<p>Vertical boundaries exist in both styles of software stack. AI applications start at the top of the stack and march through the layers from top to bottom. Frameworks choose data layout and operator fusion strategies at the graph level; then the tensor computations carry out the operators selected in the computational graph; and these operators map onto a fixed set of hardware primitives. It’s a one-shot, unidirectional workflow: performance constraints at the level of tensor programs, for example, cannot feed back to influence the data layout at the computational graph level. And incorporating custom hardware typically means manually propagating new features through all three layers.</p>

<p>Both vertical and horizontal boundaries are slowing down the pace of innovation in machine learning. New hardware accelerators are emerging with new levels of capability and performance, but harnessing them will require fluid collaboration between ML scientists, ML engineers, hardware vendors that these boundaries prevent. To cope with the rapid pace of change in ML systems, frameworks need to support <strong>incremental</strong> evolution: Incorporating new capabilities should require effort proportional to the change, not wholesale re-engineering at each level.</p>

<h2 id="tvm-unity">TVM Unity</h2>

<p>The TVM Unity vision is about breaking down these barriers. The goal is to enable cross-layer interactions and automate their optimization. It is not to collapse the abstraction layers into a monolith: there is no “silver bullet” representation for AI programs that simultaneously enables optimization at every level. Instead, TVM Unity will build interfaces for the abstractions to interact and exchange information.</p>

<p>Removing the strict barriers between the levels in the system stack will enable new kinds of optimization that work jointly across the layers. A unified view of the entire system will let TVM automatically co-optimize decisions in the computation graph, the tensor operators, and the hardware mapping to search for the best possible implementation of an AI application. At the same time, TVM Unity will also serve as a communication substrate for interactions between ML scientists, ML engineers, and hardware engineers. This collaboration will be crucial for adapting to the rapid changes that are coming in the next phase of hardware acceleration for ML.</p>

<h3 id="unifying-abstractions">Unifying Abstractions</h3>

<p><img src="/images/tvm-unity/image2.png" alt="image" style="width: 70%; margin: auto; display: block;" /></p>

<p>TVM Unity will focus on letting AI applications fluidly cross the boundaries between operator graphs, tensor programs, and hardware primitives. In TVM, a single Python program can define a core tensor operation, incorporate a custom hardware primitive, and invoke the operation from a larger operator graph.
This example shows all of these capabilities:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">tvm.script</span>
<span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">tir</span> <span class="k">as</span> <span class="n">T</span><span class="p">,</span> <span class="n">relax</span> <span class="k">as</span> <span class="n">R</span>

<span class="o">@</span><span class="n">tvm</span><span class="p">.</span><span class="n">script</span><span class="p">.</span><span class="n">ir_module</span>
<span class="k">class</span> <span class="nc">MyIRModule</span><span class="p">:</span>
    <span class="c1"># Define a TIR based operation.
</span>	<span class="o">@</span><span class="n">T</span><span class="p">.</span><span class="n">prim_func</span>
	<span class="k">def</span> <span class="nf">tir_mm</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">T</span><span class="p">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="s">"float32"</span><span class="p">],</span>
                   <span class="n">W</span><span class="p">:</span> <span class="n">T</span><span class="p">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="s">"float32"</span><span class="p">],</span>
                   <span class="n">Y</span><span class="p">:</span> <span class="n">T</span><span class="p">.</span><span class="n">Buffer</span><span class="p">[(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="s">"float32"</span><span class="p">]):</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span>  <span class="ow">in</span> <span class="n">T</span><span class="p">.</span><span class="n">grid</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">d</span><span class="p">):</span>
            <span class="k">with</span> <span class="n">T</span><span class="p">.</span><span class="n">block</span><span class="p">(</span><span class="s">"body"</span><span class="p">):</span>
                <span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">,</span> <span class="n">vk</span> <span class="o">=</span> <span class="n">T</span><span class="p">.</span><span class="n">axis</span><span class="p">.</span><span class="n">remap</span><span class="p">(</span><span class="s">"SSR"</span><span class="p">,</span> <span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
		<span class="k">with</span> <span class="n">T</span><span class="p">.</span><span class="n">init</span><span class="p">():</span>
            <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="c1"># Can be mapped on to HW intrinsics.
</span>        <span class="n">Y</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vj</span><span class="p">]</span> <span class="o">+=</span> <span class="n">X</span><span class="p">[</span><span class="n">vi</span><span class="p">,</span> <span class="n">vk</span><span class="p">]</span> <span class="o">*</span> <span class="n">W</span><span class="p">[</span><span class="n">vk</span><span class="p">,</span> <span class="n">wj</span><span class="p">]</span>

	<span class="o">@</span><span class="n">R</span><span class="p">.</span><span class="n">function</span>
	<span class="k">def</span> <span class="nf">relax_func</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="p">.</span><span class="n">Tensor</span><span class="p">[(</span><span class="n">n</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="s">"float32"</span><span class="p">],</span> <span class="n">w</span><span class="p">:</span> <span class="n">R</span><span class="p">.</span><span class="n">Tensor</span><span class="p">[(</span><span class="n">d</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="s">"float32"</span><span class="p">]):</span>
        <span class="k">with</span> <span class="n">R</span><span class="p">.</span><span class="n">dataflow</span><span class="p">()</span>
            <span class="c1"># Invoke the TIR code.
</span>            <span class="n">lv0</span><span class="p">:</span> <span class="n">R</span><span class="p">.</span><span class="n">Tensor</span><span class="p">[(</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="s">"float32"</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span><span class="p">.</span><span class="n">call_dps</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">m</span><span class="p">),</span> <span class="n">tir_mm</span><span class="p">,</span> <span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">])</span>
            <span class="n">lv1</span><span class="p">:</span> <span class="n">R</span><span class="p">.</span><span class="n">Tensor</span><span class="p">[(</span><span class="n">n</span> <span class="o">*</span> <span class="n">m</span><span class="p">,),</span> <span class="s">"float32"</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">lv0</span><span class="p">)</span>
            <span class="n">gv0</span><span class="p">:</span> <span class="n">R</span><span class="p">.</span><span class="n">Tensor</span><span class="p">[</span><span class="n">lv2</span><span class="p">,</span> <span class="s">"float32"</span><span class="p">]</span> <span class="o">=</span> <span class="n">R</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="n">lv1</span><span class="p">)</span>
            <span class="n">R</span><span class="p">.</span><span class="n">output</span><span class="p">(</span><span class="n">gv0</span><span class="p">)</span>

        <span class="c1"># Invoke external update rule.
</span>        <span class="n">R</span><span class="p">.</span><span class="n">call_packed</span><span class="p">(</span><span class="s">"custom_inplace_update"</span><span class="p">,</span> <span class="n">gv0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">gv0</span>
</code></pre></div></div>

<p>This code has both a tensor program (<code class="language-plaintext highlighter-rouge">tir_mm</code>) and computational graph that includes it (<code class="language-plaintext highlighter-rouge">relax_func</code>). The high-level data flow can directly invoke the low-level tensor manipulation to build up a larger computation. The TVM runtime unifies the operator graph and compiler-based tensor computation to optimize the entire program. This code also uses <code class="language-plaintext highlighter-rouge">call_packed</code> to invoke a pre-baked operator—showing how TVM can smoothly integrate library-based operators with the custom computation.</p>

<p>Additionally, TensorIR opens doors to exploit hardware primitives through tensorization. Tensorization transforms loop-level programs to implementations that map onto the primitives that a particular hardware target declares.</p>

<p>The key to highlight here is <strong>cross layer interactions</strong>. Our particular example shows interactions between: (1) computational graph and tensor programs; (2) computational graph and runtime libraries; (3) Finally tensor programs and hardware primitives through on-going automatic tensorization developments in TensorIR. These cross layer interactions open doors for making <strong>incremental optimizations</strong> at the boundary. For example, we can build a customized pass to the lower part of the subgraph to a set of runtime libraries then pass on to the rest of the pipeline.</p>

<p>In addition to the unification of abstraction layers, we are also working on unifying the shape representation, to enable <strong>first class symbolic shape support</strong> across the stack. In our particular example, the symbolic shape dimensions(n, m) can flow across the abstractions and enable advanced optimizations for dynamic workloads. The additional capabilities will open doors for both training and inference workload optimizations.</p>

<h3 id="unifying-perspectives">Unifying Perspectives</h3>

<p>Better ML systems require collaboration between ML scientists, ML engineers, and hardware engineers. The coming era of diverse specialized ML hardware will require coordinated effort from teams that include all three groups. By building rich, bidirectional interfaces between the layers in the system stack, TVM Unity aims to be the medium through which this collaboration and iteration happens.</p>

<p>Abstractions in TVM can catalyze the lifecycle of an improvement to an AI application. At the highest level, an ML scientist can specify the operator they need to construct the next generation of a model. ML engineers can work at the tensor computation level to make this new operation efficient. Finally, these tensor computations can rely on hardware primitives written by hardware engineers. The work at each level will interact through Python APIs within the TVM ecosystem. The ability to work together within TVM, rather than invasively modifying a framework with each new feature, will be the key to fast iteration in the face of rapidly evolving hardware.</p>

<h3 id="automation">Automation</h3>

<p>A unified ML system creates a new, larger search space than a system stack with strict boundaries. Decisions within tensor computations can influence the structure of the operator graph, and new hardware primitives can drastically change the optimal mappings at every other layer.</p>

<p>TVM Unity will expose all these cross-layer interactions for automated optimization. Finding the best implementation for a given application will require learning-driven optimization: using ML to optimize ML by exploring the expanded joint search space and minimize the computational cost.</p>

<p>In addition to that, we also want to leverage domain experts’ help when possible, and create mechanisms to effectively incorporate domain information to help guide the automatic optimizations.</p>

<h2 id="new-capabilities-with-unity">New Capabilities with Unity</h2>

<p>The Unity vision guides the technical roadmap for TVM’s evolution over the next year. The unified approach will position TVM to offer new forms of automation and ecosystem integration that are not possible with today’s system stacks.</p>

<p>With Unity, TVM will unify library-based computation with compiler-based automation. AI applications will be able to combine the world’s best known code for common operators with automatically optimized code for computations that don’t map neatly onto any existing operator. Developers will be able to smoothly transition between both strategies without a steep “performance cliff” when switching from built-in to generated code. Teams will be able to iterate rapidly with compiled code for new model designs and then, as models mature and stabilize, fluidly incorporate optimized operator libraries to maximize performance. By erasing the boundary between operator-based and compiler-based stacks, TVM will enable automatic exploration of the trade-off space between the two extremes.</p>

<p>TVM also aims to serve as a bridge to unify the broader ML and hardware ecosystems. In the ML ecosystem, TVM offers a minimal runtime that does not constrain teams’ choice of frameworks. TVM models will be easy to embed into other frameworks and runtimes as subgraphs for both training and inference. Through exchange formats like <a href="https://onnx.ai/">ONNX</a> and <a href="https://pytorch.org/docs/stable/jit.html">TorchScript</a>, TVM models can fluidly integrate into larger applications built on any infrastructure. In the hardware ecosystem, TVM is already the best way for accelerator designers to integrate with ML applications. With TVM Unity, hardware vendors will easily onboard into TVM via a simple set of operators and then incrementally transition to compilation-based integration for better flexibility. This way, new hardware capabilities can get started improving AI applications without reinventing the whole system stack.</p>

<p><img src="/images/tvm-unity/image3.png" alt="image" style="width: 50%; margin: auto; display: block;" /></p>

<p>Beyond TVM alone, the same forces that are driving TVM Unity exist across the theory and practice of modern ML. Rapid changes to models, emerging alternative hardware, and aging abstraction boundaries all point toward the need for an integrated approach. We expect TVM to lead the way into the next great industry-wide shift in ML systems.</p>

<p>For more details about our vision for TVM, check out <a href="https://www.tvmcon.org">TVMCon 2021</a> for more talks and discussion.</p>]]></content><author><name>Adrian Sampson, Tianqi Chen, Jared Roesch</name></author><summary type="html"><![CDATA[Apache TVM Unity is a roadmap for the TVM ecosystem in 2022. We see a broader shift coming in the way that machine learning system stacks optimize for flexibility and agility in the face of a rapidly changing hardware landscape. TVM will evolve to break down the boundaries that constrain the ways current ML systems adapt to rapid changes in ML models and the accelerators that implement them.]]></summary></entry><entry><title type="html">Introducing TVM Auto-scheduler (a.k.a. Ansor)</title><link href="/2021/03/03/intro-auto-scheduler" rel="alternate" type="text/html" title="Introducing TVM Auto-scheduler (a.k.a. Ansor)" /><published>2021-03-03T00:00:00+00:00</published><updated>2021-03-03T00:00:00+00:00</updated><id>/2021/03/03/intro-auto-scheduler</id><content type="html" xml:base="/2021/03/03/intro-auto-scheduler"><![CDATA[<p>Optimizing the execution speed of deep neural networks is extremely hard with the growing
model size, operator diversity, and hardware heterogeneity.
From a computational perspective, deep neural networks are just layers and layers of tensor computations.
These tensor computations, such as matmul and conv2d, can be easily described by mathematical expressions.
However, providing high-performance implementations for them on modern hardware can be very challenging.
We have to apply various low-level optimizations and utilize special hardware intrinsics to achieve high performance.
It takes huge engineering effort to build linear algebra and neural network acceleration libraries like CuBLAS, CuDNN, oneMKL, and oneDNN.</p>

<p>Our life will be much easier if we can just write mathematical expressions and have something
magically turn them into efficient code implementations.
Three years ago, deep learning compiler TVM and its search module AutoTVM were built as the first step towards this goal.
AutoTVM employs a template-based search algorithm to find efficient implementations for a given tensor computation.
However, it is a template-based approach, so it still requires domain experts to implement a non-trivial manual template
for every operator on every platform.
Today, there are more than 15k lines of code for these templates in the TVM code repository.
Besides being very hard to develop, these templates often have inefficient and limited search spaces,
making them unable to achieve optimal performance.</p>

<p>To address the limitations of AutoTVM, we started project Ansor aiming at a fully automated auto-scheduler for 
generating code for tensor computations.
Ansor auto-scheduler only takes tensor expressions as input and generates high-performance code without manual templates.
We made innovations in the search space construction and search algorithm.
As a result, the auto-scheduler can achieve better performance with less search time in a more automated way.</p>

<p>Ansor auto-scheduler is now integrated into Apache TVM as <code class="language-plaintext highlighter-rouge">tvm.auto_scheduler</code> package.
This is a joint effort by collaborators from UC Berkeley, Alibaba, AWS and OctoML.
Detailed tutorials are available for Intel CPUs, ARM CPUs, NVIDIA GPUs, and Mali GPUs on the TVM website [1].
In this blog post, we will give a high-level introduction and show some benchmark results.</p>

<h1 id="system-overview">System Overview</h1>

<h2 id="autotvm-vs-auto-scheduler">AutoTVM vs Auto-scheduler</h2>
<p style="text-align: center"><img src="/images/intro-auto-scheduler/workflow.png" alt="image" width="75%" /></p>
<center> Table 1. Workflow Comparision </center>
<p></p>

<p>Table 1 compares the workflow for generating code for an operator in AutoTVM and auto-scheduler.
In AutoTVM, the developer has to go through three steps.
In step 1, the developer has to write the compute definition in TVM’s tensor expression language.
This part is relatively easy because TVM’s tensor expression language looks just like math expressions.
In step 2, the developer has to write a schedule template, which typically consists of 20-100 lines of tricky DSL code.
This part requires domain expertise of both the target hardware architecture and operator semantics, so it is difficult.
The last step, step 3, is automated by a search algorithm.</p>

<p>In auto-scheduler, we eliminate the most difficult step 2 by automatic search space construction and accelerate step 3 with a better search algorithm.
By doing automatic search space construction, we not only eliminate huge manual effort, 
but also enabling the exploration of much more optimization combinations.
This automation does not come for free, because we still need to design rules to generate the search space.
However, these rules are very general. They are based on static analysis of the tensor expressions.
We only need to design a few general rules once and can apply them to almost all tensor computations in deep learning.</p>

<h2 id="search-process">Search Process</h2>
<p style="text-align: center"><img src="/images/intro-auto-scheduler/search_overview.png" alt="image" width="40%" /></p>
<center> Figure 1. Search Process Overview  </center>
<p></p>

<p>Figure 1. shows the search process of auto-scheduler when optimizing a whole neural network.
The system takes deep learning models as input.
It then partitions the big model into small subgraphs with Relay’s operator fusion pass.
A task scheduler is utilized to allocate the time resource for optimizing many subgraphs.
At each iteration, it picks a subgraph that has the most potential to increase the end-to-end performance.
For this subgraph, we analyze its tensor expression and generate several sketches for it.
Then we run evolutionary search with a learned cost model to get a batch of optimized programs.
The optimized programs are sent to actual hardware for measurements.
When the measurements are finished, the profiling results are used as feedback to update all components of the system.
This process is repeated iteratively until the optimization converges or we run out of time budget.
More technical details can be found in our paper [3] and our code.</p>

<p>It is worth notiing that since the auto-scheduler generates schedules from scratch, 
it reuses the existing computation definitions in TOPI but not schedule templates.</p>

<h1 id="benchmark-results">Benchmark Results</h1>
<p>In this section, we benchmark the performance of AutoTVM and Auto-scheduler.
The CPU benchmark is done on an AWS c5.9xlarge, which is equipped with an Intel 18-core skylake 8124-m CPU. 
The GPU benchmark is done on an AWS g4dn.4xlarge, which is equipped with an NVIDIA T4 GPU.
All benchmark code, raw data, tuning logs can be found in this repo [2].</p>

<h2 id="performance-of-the-generated-code">Performance of the generated code</h2>
<p>We benchmark the fp32 single-batch inference latency on three networks.
Figure 2 shows the relative speedup of auto-scheduler against AutoTVM.
We can see auto-scheduler outperforms AutoTVM in all cases with 1.02x to 8.95x speedup.
This is because auto-scheduler explores a larger search space, which covers more efficient combinations
of optimizations that are missed in TOPI manual templates.
The BERT-base@GPU is an extreme case where the manual templates are very badly designed.
In other words, the manual template for dense layers does not perform well for the shapes in BERT model.</p>

<p style="text-align: center"><img src="/images/intro-auto-scheduler/code_perf.png" alt="image" width="85%" /></p>
<center> Figure 2. Code Performance Comparision (Higher is better) </center>
<p></p>

<h2 id="search-time">Search Time</h2>
<p>As we know, the search-based approaches can be very time-consuming, so we also care about the search time.
It typically takes several hours to let the search converge for a single neural network.
Figure 3 compares the search time of AutoTVM and auto-scheduler.
Auto-scheduler requires much less time to converge in most cases, despite its larger search space.
This is mainly because of auto-scheduler has a better cost model and task scheduler.</p>

<p style="text-align: center"><img src="/images/intro-auto-scheduler/search_time.png" alt="image" width="85%" /></p>
<center> Figure 3. Search Time Comparision (Lower is better) </center>
<p></p>

<h2 id="more-results">More Results</h2>
<p>The repo above serves as an internal benchmark tool for TVM, so it only compares the latest AutoTVM and AutoScheduler.
You can find results for more libraries and backends in our paper [3].
Recently, this blog post [4] also tried auto-scheduler on an Apple M1 chip and got some good results.</p>

<h1 id="conclusion">Conclusion</h1>
<p>We build TVM auto-scheduler, a system that automatically generates high-performance code for tensor expressions.
Compared with the predecessor AutoTVM, auto-scheduler does not require manual templates.
Besides, auto-scheduler is capable of generating schedules with better performance in a shorter time.
We achieve this by making innovations in the search space construction and search algorithm.</p>

<p>We are excited about the current performance of auto-scheduler.
In the future, we are interested in extending the ability of auto-scheduler to support
sparse operators, low-precision operators, and dynamic shape better.</p>

<h1 id="links">Links</h1>
<p>[1] Tutorials: <a href="https://tvm.apache.org/docs/tutorials/index.html#autoscheduler-template-free-auto-scheduling">https://tvm.apache.org/docs/tutorials/index.html#autoscheduler-template-free-auto-scheduling</a><br />
[2] Benchmark repo: <a href="https://github.com/tlc-pack/TLCBench">https://github.com/tlc-pack/TLCBench</a><br />
[3] OSDI Paper: <a href="https://arxiv.org/abs/2006.06762">Ansor : Generating High-Performance Tensor Programs for Deep Learning</a><br />
[4] Results on Apple M1 chip: <a href="https://medium.com/octoml/on-the-apple-m1-beating-apples-core-ml-4-with-30-model-performance-improvements-9d94af7d1b2d">https://medium.com/octoml/on-the-apple-m1-beating-apples-core-ml-4-with-30-model-performance-improvements-9d94af7d1b2d</a>.</p>]]></content><author><name>Lianmin Zheng, Chengfan Jia, Minmin Sun, Zhao Wu, Cody Hao Yu</name></author><summary type="html"><![CDATA[Optimizing the execution speed of deep neural networks is extremely hard with the growing model size, operator diversity, and hardware heterogeneity. From a computational perspective, deep neural networks are just layers and layers of tensor computations. These tensor computations, such as matmul and conv2d, can be easily described by mathematical expressions. However, providing high-performance implementations for them on modern hardware can be very challenging. We have to apply various low-level optimizations and utilize special hardware intrinsics to achieve high performance. It takes huge engineering effort to build linear algebra and neural network acceleration libraries like CuBLAS, CuDNN, oneMKL, and oneDNN.]]></summary></entry><entry><title type="html">Bring Your Own Datatypes: Enabling Custom Datatype Exploration in TVM</title><link href="/2020/09/26/bring-your-own-datatypes" rel="alternate" type="text/html" title="Bring Your Own Datatypes: Enabling Custom Datatype Exploration in TVM" /><published>2020-09-26T00:00:00+00:00</published><updated>2020-09-26T00:00:00+00:00</updated><id>/2020/09/26/bring-your-own-datatypes</id><content type="html" xml:base="/2020/09/26/bring-your-own-datatypes"><![CDATA[<p>In this post, we describe the Bring Your Own Datatypes framework, which enables the use of custom datatypes within TVM.</p>

<h2 id="introduction">Introduction</h2>

<p>When designing accelerators, an important decision is how one will approximately represent real numbers in hardware.
This problem has had a longstanding, industry-standard solution: the IEEE 754 floating-point standard.<sup id="fnref:ieee"><a href="#fn:ieee" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>
Yet,
  when trying to squeeze
  the most out of hardware
  by building highly specialized designs,
  does it make sense to use
  general-purpose IEEE 754 floats?
If we know the numerical requirements
  of our workload,
  could we build a smaller,
  faster,
  or more power efficient datatype?
The answer is yes!
Researchers have already begun experimenting with new datatypes in academic and industrial accelerator designs.
For example, Google’s Tensor Processing Unit (the TPU) uses the <code class="language-plaintext highlighter-rouge">bfloat</code> type: a single-precision IEEE float which has been truncated to 16 bits.
Due to the lax numerical requirements
  of many deep learning workloads,
  this truncation often has no effect
  on model accuracy,
  while instantly cutting the storage cost
  in half.<sup id="fnref:jouppi2017datacenter"><a href="#fn:jouppi2017datacenter" class="footnote" rel="footnote" role="doc-noteref">2</a></sup><sup id="fnref:tensorflowbfloat"><a href="#fn:tensorflowbfloat" class="footnote" rel="footnote" role="doc-noteref">3</a></sup></p>

<p>Before researchers begin building hardware for their datatype, however, they first need to determine how their datatype will behave numerically in the workloads they care about.
This often involves first building a software-emulated version of their datatype
  (e.g. <a href="http://www.jhauser.us/arithmetic/SoftFloat.html" target="_blank">Berkeley SoftFloat</a> or <a href="https://github.com/cjdelisle/libposit" target="_blank">libposit</a>),
  and then hacking the datatype directly into workloads,
  to see how the workload performs
  using the datatype.
Even better
  is to integrate the datatype 
  directly into compilers themselves,
  so that many different workloads
  can be compiled
  to use the datatype.
Both routes can be tedious, with the latter route often becoming unmanageable given the size and complexity of modern compilers.
<a href="https://github.com/xman/tensorflow" target="_blank">One example taken from GitHub</a> shows someone hacking the <em>posit</em> datatype into TensorFlow.
The result is 237 commits, adding nearly 6000 lines of code and touching over 200 files across the codebase—and that’s just to add one datatype!
This amount of work is prohibitive for many researchers.</p>

<p>To address these problems, we present the Bring Your Own Datatypes framework.
The framework enables easy exploration of new datatypes in deep learning workloads by allowing users to plug their simulated datatype into TVM.
Unlike the posits-in-Tensorflow example above, which enables a single new datatype in a compiler, the Bring Your Own Datatype framework enables a huge variety of user-defined types.</p>

<h2 id="bring-your-own-datatypes">Bring Your Own Datatypes</h2>

<p>The goal of the Bring Your Own Datatypes framework
  is to enable users to run deep learning workloads
  using custom datatypes.
In the Bring Your Own Datatypes framework,
  “datatype” means a scalar type:
  <code class="language-plaintext highlighter-rouge">float</code>
  or <code class="language-plaintext highlighter-rouge">uint</code>, for example.
We do not handle more complicated data formats
  such as <a href="https://en.wikipedia.org/wiki/Block_floating_point" target="_blank">block floating point</a>
  or Intel’s <a href="https://arxiv.org/abs/1711.02213" target="_blank">Flexpoint</a>.
Additionally,
  we only claim to support
  <em>software emulated</em> versions of these scalar datatypes;
  we do not explicitly support compiling and running on custom datatype hardware.</p>

<p>Each tensor in TVM
  is assigned a type code,
  which defines the datatype of the scalars
  within the tensor.
A number of these type codes
  have hard-coded meanings in TVM,
  mapping to common datatypes
  such as <code class="language-plaintext highlighter-rouge">int</code> and <code class="language-plaintext highlighter-rouge">float</code>.
However,
  the vast majority of type codes
  are unused.
The Bring Your Own Datatypes framework
  allows users to 
  claim these unused type codes
  and add their own new datatypes
  at runtime.</p>

<p>The framework is implemented as
  a registry 
  which sits alongside
  TVM’s normal datatype facilities.
There are two primary ways
  in which the user interacts with
  the datatype registry:
  first, <strong>datatype registration,</strong>
  and second, <strong>lowering function registration.</strong>
These steps are akin to
  <em>declaration</em> and <em>implementation</em> of the datatype,
  respectively.</p>

<p>Please note that all referred code in this post are based on TVM repository’s master branch commit <a href="https://github.com/apache/incubator-tvm/tree/4cad71d19fda6d8f7b750c791284c6dfdddf1f07" target="_blank">4cad71d</a>. We will use an example <code class="language-plaintext highlighter-rouge">posit</code> datatype which can be found under <code class="language-plaintext highlighter-rouge">src/target/datatype/posit/posit-wrapper.cc</code> and can be compiled in TVM with the <code class="language-plaintext highlighter-rouge">USE_BYODT_POSIT</code> flag.<sup id="fnref:posit"><a href="#fn:posit" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p>

<h3 id="datatype-registration">Datatype Registration</h3>

<p>To register the datatype,
  the user assigns the datatype
  a name and a type code,
  where the type code comes from
  the range of unused type codes
  available to custom datatypes.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tvm</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">datatype</span><span class="p">.</span><span class="n">register</span><span class="p">(</span><span class="s">'posit'</span><span class="p">,</span> <span class="mi">150</span><span class="p">)</span>
</code></pre></div></div>
<p>The above code registers
  the <code class="language-plaintext highlighter-rouge">'posit'</code> datatype
  with type code 150.
This registration step
  allows TVM to parse programs
  which use the custom type:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="s">'x'</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">var</span><span class="p">(</span><span class="s">'y'</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">x_posit</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'custom[posit]16'</span><span class="p">)</span>
<span class="n">y_posit</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'custom[posit]16'</span><span class="p">)</span>
<span class="n">z_posit</span> <span class="o">=</span> <span class="n">x_posit</span> <span class="o">+</span> <span class="n">y_posit</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">cast</span><span class="p">(</span><span class="n">z_posit</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s">'float32'</span><span class="p">)</span>
<span class="n">program</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">Function</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">],</span> <span class="n">z</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">program</span><span class="p">)</span>

<span class="c1"># v0.0.4
# fn (%x: Tensor[(3), float32], %y: Tensor[(3), float32]) {
#   %0 = cast(%x, dtype="custom[posit]16");
#   %1 = cast(%y, dtype="custom[posit]16");
#   %2 = add(%0, %1);
#   cast(%2, dtype="float32")
# }
</span></code></pre></div></div>
<p>The program above
  casts <code class="language-plaintext highlighter-rouge">float32</code> inputs <code class="language-plaintext highlighter-rouge">x</code> and <code class="language-plaintext highlighter-rouge">y</code>
  into <code class="language-plaintext highlighter-rouge">posit</code>s,
  adds them,
  and casts the result back to <code class="language-plaintext highlighter-rouge">float32</code>.
Once the <code class="language-plaintext highlighter-rouge">posit</code> type is registered,
  TVM is able to parse the special <code class="language-plaintext highlighter-rouge">dtype</code> syntax
  <code class="language-plaintext highlighter-rouge">custom[&lt;typename&gt;]</code>,
  where <code class="language-plaintext highlighter-rouge">&lt;typename&gt;</code> is the name registered for the type.
This syntax also supports the usual
  <code class="language-plaintext highlighter-rouge">&lt;bits&gt;x&lt;lanes&gt;</code> format;
  here, we use <code class="language-plaintext highlighter-rouge">16</code> to indicate that
  each <code class="language-plaintext highlighter-rouge">posit</code> is 16 bits wide.
(The number of lanes
  defaults to 1.)</p>

<h3 id="lowering-function-registration">Lowering Function Registration</h3>

<p>Though TVM can parse the above program,
  it cannot yet compile it,
  as TVM does not yet understand 
  how to compile operations 
  over the <code class="language-plaintext highlighter-rouge">posit</code> type.
To compile these programs,
  we register <em>lowering functions</em> for the custom datatype,
  which help TVM convert the operations
  into something it can understand and compile.</p>

<p>Generally, the user is not expected to 
  lower operations
  directly to LLVM or CUDA.
Instead, most code using custom datatypes
  can be lowered into code which <em>doesn’t</em> use custom datatypes,
  with some simple tricks.
We can then rely on native TVM
  to understand and compile the code.</p>

<p style="text-align: center"><img src="/images/bring-your-own-datatypes/lowering.png" alt="A lowering function lowering an add over `posit`s to a library call over `uint16_t`s" width="50%" /></p>
<center>
Figure 1: The expected result of a user's registered lowering function. A lowering function should convert a program using custom datatypes to a program which native TVM can understand and compile (in this case, a call to an external library, taking two <tt>uint16_t</tt>s).
</center>
<p></p>

<p>Figure 1 shows a common pattern.
Let’s assume we are
  interested in exploring the <code class="language-plaintext highlighter-rouge">posit</code> type,
  and have chosen to run some workloads
  by plugging a <code class="language-plaintext highlighter-rouge">posit</code> emulation library (e.g. <a href="https://github.com/stillwater-sc/universal" target="_blank">Stillwater Universal</a>) into TVM
  via the Bring Your Own Datatypes framework.
Our workload is a simple program
  which adds two <code class="language-plaintext highlighter-rouge">posit</code> inputs.
Native TVM does not understand
  how to implement <code class="language-plaintext highlighter-rouge">posit</code> addition—but it doesn’t need to,
  as we have a library implementing our datatype!
The library contains an implementation of <code class="language-plaintext highlighter-rouge">posit</code> addition,
  alongside other operators such as multiplication and square root.
To implement this <code class="language-plaintext highlighter-rouge">posit</code> addition,
  we’d just like to call into our library.
Thus, our Add node should become a Call node,
  calling out to a function (call it <code class="language-plaintext highlighter-rouge">Posit16es2Add</code>) in our library.
To store the bits of the input <code class="language-plaintext highlighter-rouge">posit</code>s
  inside a type that TVM understands,
  we use 16-bit unsigned integers.
The resulting program 
  is one that TVM can understand and compile—it
  is simply a call to an external library function,
  taking two unsigned integers.</p>

<p>To achieve the above lowering,
  we register a lowering function
  for <code class="language-plaintext highlighter-rouge">posit</code>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tvm</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">datatype</span><span class="p">.</span><span class="n">register_op</span><span class="p">(</span>
    <span class="n">tvm</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">datatype</span><span class="p">.</span><span class="n">create_lower_func</span><span class="p">({</span><span class="mi">16</span><span class="p">:</span> <span class="s">'Posit16es2Add'</span><span class="p">}),</span>
    <span class="s">'Add'</span><span class="p">,</span> <span class="s">'llvm'</span><span class="p">,</span> <span class="s">'posit'</span><span class="p">)</span>
</code></pre></div></div>
<p>The above code registers
  a lowering function
  for a specific operator (Add),
  compilation target (LLVM),
  datatype (<code class="language-plaintext highlighter-rouge">posit</code>), and bit length (16).
The first argument
  is the lowering function.
This can be any function
  taking a TVM IR node
  and returning a new TVM IR node.
In our case,
  we use a helper function
  provided by the Bring Your Own Datatypes framework.
<code class="language-plaintext highlighter-rouge">tvm.target.datatype.create_lower_func({16:'Posit16es2Add'})</code>
  creates a lowering function
  for the common pattern described above.
The resulting function
  converts the arguments of the given node
  to <code class="language-plaintext highlighter-rouge">uint16_t</code>,
  and then converts the node itself
  into a call to the given function name
  (in this case, <code class="language-plaintext highlighter-rouge">'Posit16es2Add'</code> for <code class="language-plaintext highlighter-rouge">posit</code>s of bit length 16).
  We pass a dictionary to <code class="language-plaintext highlighter-rouge">create_lower_func</code> so that TVM can dispatch
  to the appropriate function name based on the bit length of the datatype.</p>

<p>To implement a custom datatype,
  the user will need to register
  a lowering function for every operator
  in the workload they would like to run.
For a network like ResNet,
  this will be around 10 operators,
  including things like, Add, Div, various Casts, and Max.
In our tests,
  registering a datatype
  and all lowering functions
  takes around 40 lines of Python.
Once all needed operators
  are registered,
  custom datatype workloads
  can be run
  as easily as
  any other TVM program!</p>

<h1 id="wrapping-up">Wrapping Up</h1>

<p>The Bring Your Own Datatypes framework
  brings user-defined datatypes to TVM.
We hope this will encourage datatype researchers
  to use TVM in their research;
  similarly,
  we hope this will spark interest
  in custom datatypes
  within the deep learning community.
For more documentation about the Bring Your Own Datatypes framework
  please visit the <a href="https://tvm.apache.org/docs/tutorials/dev/bring_your_own_datatypes.html#sphx-glr-tutorials-dev-bring-your-own-datatypes-py" target="_blank">Bring Your Own Datatypes to TVM</a> developer tutorial.</p>

<hr />

<p><em>Gus Smith is a PhD student at the University of Washington working with Luis Ceze and Zachary Tatlock at the intersection of computer architecture and programming languages. His website is <a href="https://justg.us" target="_blank">justg.us</a>.</em></p>

<p><em><a href="https://github.com/hypercubestart" target="_blank">Andrew Liu</a> is an undergraduate student at the University of Washington and a member of UW CSE <a href="https://sampl.cs.washington.edu/" target="_blank">SAMPL</a> and <a href="https://uwplse.org/" target="_blank">PLSE</a> labs.</em></p>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:ieee">
      <p><a href="https://standards.ieee.org/standard/754-2019.html" target="_blank">754-2019 - IEEE Standard for Floating-Point Arithmetic</a> <a href="#fnref:ieee" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:jouppi2017datacenter">
      <p>Jouppi, Norman P., et al. “In-datacenter performance analysis of a tensor processing unit.” Proceedings of the 44th Annual International Symposium on Computer Architecture. 2017. <a href="#fnref:jouppi2017datacenter" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:tensorflowbfloat">
      <p><a href="https://cloud.google.com/tpu/docs/bfloat16" target="_blank">Using bfloat16 with TensorFlow models</a> <a href="#fnref:tensorflowbfloat" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:posit">
      <p><a href="https://posithub.org/docs/BeatingFloatingPoint.pdf" target="_blank">Beating Floating Point at its Own Game: Posit Arithmetic</a> <a href="#fnref:posit" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Gus Smith, Andrew Liu</name></author><summary type="html"><![CDATA[In this post, we describe the Bring Your Own Datatypes framework, which enables the use of custom datatypes within TVM.]]></summary></entry><entry><title type="html">How to Bring Your Own Codegen to TVM</title><link href="/2020/07/15/how-to-bring-your-own-codegen-to-tvm" rel="alternate" type="text/html" title="How to Bring Your Own Codegen to TVM" /><published>2020-07-15T00:00:00+00:00</published><updated>2020-07-15T00:00:00+00:00</updated><id>/2020/07/15/how-to-bring-your-own-codegen-to-tvm</id><content type="html" xml:base="/2020/07/15/how-to-bring-your-own-codegen-to-tvm"><![CDATA[<p>To free data scientists from worrying about the performance when developing a new model, hardware backend providers (e.g., Intel, NVIDIA, ARM, etc) either provide kernel libraries such as cuBLAS or cuDNN with many commonly used deep learning kernels, or provide frameworks such as DNNL or TensorRT with a graph engine to let users describe their models in a certain way to achieve high performance. In addition, emerging deep learning accelerators also have their own compilers, kernel libraries, or runtime frameworks.</p>

<p>However, users have to learn a new programming interface when they attempt to work on a new kernel library or a device. As a result, the demand for a unified programming interface becomes more and more important to let all users and hardware backend providers stand on the same page.</p>

<p>To share the programming interface with widely used deep learning frameworks, many hardware device providers have attempted to integrate their devices backend to TensorFlow. However, since TensorFlow does not provide an official backend interface for new backends, you have to hack the TensorFlow for registration, which involves many source file changes and makes the future maintenance difficult.</p>

<p>In this post, we demonstrate how you, as a hardware backend provider, can easily leverage the Bring Your Own Codegen (BYOC) framework to integrate the kernel library/compiler/framework of your hardware device to TVM. The most important advantage of leveraging BYOC framework is that <strong><em>all related source files of your devices are self-contained, so the codegen/runtime of your devices are pluggable to the TVM code base.</em></strong> It means that 1) the TVM code base with your codegen would be upstream compatible, and 2) TVM users can choose to enable the codegen/runtime based on their needs.</p>

<p>In the rest of this post, we first illustrate a scenario that you may need TVM with BYOC, followed by an overview of the BYOC compilation and runtime flows. Then, we step-by-step illustrate how to integrate a vendor library or an execution engine to TVM with BYOC by using Intel DNNL (a.k.a. MKL-DNN, OneDNN) as a running example.</p>

<h2 id="bring-an-asic-accelerator-to-tvm">Bring an ASIC Accelerator to TVM</h2>

<p>Let’s first make a scenario to illustrate why you want to bring your accelerator to TVM and what features you can expect from the BYOC framework. If you are not sure whether your case is suitable for BYOC, you are welcome to raise a discussion at <a href="https://discuss.tvm.ai">discuss.tvm.ai</a>.</p>

<p>Imagining that you just made an edge device platform with an ARM CPU and a fantastic accelerator that has achieved amazing performance for common image classification models. In other words, your accelerator does well on Conv2D, ReLU, GEMM, and other widely used CNN operators.</p>

<p>Unfortunately, object detection models are getting more and more popular as well, and your customers need to run both image classification and object detection models on your platform. Although your accelerator is capable of executing almost all operators in object detection models, one operator (e.g., non-maximum suppression, NMS) is missing.</p>

<h3 id="let-tvm-execute-unsupported-operators">Let TVM execute unsupported operators</h3>
<p>Since TVM has multiple codegens for different backends, it is easy for the open source community to implement new operators on CPU or GPU in a short time. Ideally, if you integrate the compilation flow of your accelerator to TVM with BYOC, TVM will perform Relay graph partitioning to offload a part of the graph to your accelerator while keeping others on TVM. As a result, you can claim that your platform is capable of running all models without worrying about new operators.</p>

<h3 id="customize-graph-level-optimization">Customize graph-level optimization</h3>
<p>Your ASIC accelerator must have its own compilation flow. Usually, it could be one of the following cases:</p>

<p><strong>Generate a graph representation and feed it to a graph engine</strong>:
You may have your own graph engine that is capable of executing a graph (or a neural network model) on your accelerator. For example, both Intel DNNL and NVIDIA TensorRT use an engine to run a whole graph or a model, so that they are able to 1) reduce memory transaction between operators and 2) optimize graph execution with operator fusion.</p>

<p>In order to achieve the above two optimizations, you may need to process the graph during the compilation time. For example, Conv2D and bias addition are two separate operators in TVM, but they may be one operator (Conv2D with bias addition capability) on your accelerator. In this case, you may want to optimize the graph by replacing the <code class="language-plaintext highlighter-rouge">conv2d - add</code> graph pattern to a <code class="language-plaintext highlighter-rouge">your_conv2d_with_bias</code> node.</p>

<p>If your compilation flow falls into this case, then we recommend reading all the rest sections in this post but skipping <a href="#bring-dnnl-to-tvm-c-source-codegen">Bring DNNL to TVM: C Source Codegen</a>.</p>

<p><strong>Generate assembly code and compile it to an executable binary</strong>:
If you do not have an end-to-end execution framework for your platform like the previous case, you may have a compiler to compile a program in assembly code of your ISA. In order to feed the assembly code to your compiler, you will need a codegen to generate and optimize the assembly code from a Relay graph.</p>

<p>If your compilation flow falls into this case, then we recommend reading all the rest sections in this post but skipping <a href="#bring-dnnl-to-tvm-json-codegenruntime">Bring DNNL to TVM: JSON Codegen/Runtime</a>.</p>

<h2 id="how-byoc-works">How BYOC Works</h2>

<p>We then briefly explain how BYOC framework works. For more detail explanations of underlying framework components and their implementations, please refer to the <a href="[https://tvm.apache.org/docs/dev/relay_bring_your_own_codegen.html](https://tvm.apache.org/docs/dev/relay_bring_your_own_codegen.html)">developer document</a>. In short, given a Relay graph in Figure 1, BYOC framework does the following steps:</p>

<p style="text-align: center"><img src="/images/bring-your-own-codegen/original_graph.png" alt="The original Relay graph" width="50%" /></p>
<center>
Figure 1: The Original Relay Graph.
</center>
<p></p>

<h3 id="1-graph-annotation">1. Graph Annotation</h3>
<p>Taking a user-provided Relay graph, our first step is to annotate the nodes that potentially can be offloaded to your accelerator in the graph. You will need to follow <a href="#bring-dnnl-to-tvm-annotation-rules">Bring DNNL to TVM: Annotation Rules</a> to implement a whitelist of supported operators, or a graph pattern list of customized composite operators. An example annotation result is shown in Figure 2.</p>

<p style="text-align: center"><img src="/images/bring-your-own-codegen/after_annotation.png" alt="The Graph with Annotations" width="50%" /></p>
<center>
Figure 2: The Graph with Annotations.
</center>
<p></p>

<h3 id="2-graph-transformation">2. Graph Transformation</h3>
<p>The second step is to transform and optimize the graph based on the annotations. Specifically, BYOC performs the following transformations.</p>

<p><strong>2.1: Merge compiler region</strong>: As can be seen in Figure 2, we now have many “regions” in the graph that can be offloaded to your accelerator, but some of them can actually be merged to reduce the data transfer and kernel launching overhead. Accordingly, step 2.1 uses a greedy algorithm to merge as many of those regions as possible while guaranteeing the functional correctness. The result is depicted in Figure 3.</p>

<p style="text-align: center"><img src="/images/bring-your-own-codegen/after_merging_regions.png" alt="After Merging Compiler Regions" width="50%" /></p>
<center>
Figure 3: After Merging Compiler Regions.
</center>
<p></p>

<p><strong>2.2: Partition Graph</strong>: For each region from the previous step, we create a Relay function with an attribute <code class="language-plaintext highlighter-rouge">Compiler</code> to indicate that this Relay function should be entirely offloaded to your accelerator, as shown in Figure 4.</p>

<p style="text-align: center"><img src="/images/bring-your-own-codegen/after_partitioning.png" alt="After Graph Partitioning" width="50%" /></p>
<center>
Figure 4: After Graph Partitioning.
</center>
<p></p>

<h3 id="3-code-generation">3. Code Generation</h3>
<p>Now we know which part of the Relay graph should be offloaded. In this step, we sequentially send every Relay function with <code class="language-plaintext highlighter-rouge">Compiler=your_accelerator</code> to your codegen. Your codegen should compile the Relay function to the form that matches your own compilation flow. It can be either C source code or any text formats.</p>

<p>Finally, all compiled functions will be serialized along with other non-offloaded Relay functions to a single <code class="language-plaintext highlighter-rouge">.so</code> file by the TVM <code class="language-plaintext highlighter-rouge">export_library</code> Python API. In other words, the user will get only one <code class="language-plaintext highlighter-rouge">.so</code> file after running this flow.</p>

<h3 id="4-runtime">4. Runtime</h3>
<p>You may also need to implement a runtime to initialize your graph engine (if applicable) and execute the compiled functions. During the inference, TVM runtime (i.e., graph runtime or VM) will leverage your runtime to invoke the offloaded functions when the TVM runtime encounters the corresponding function call in Figure 4. Your runtime is responsible for launching the compiled function with the given input tensor arrays and filling in the results to the output tensor arrays.</p>

<p>In the rest of this post, we use DNNL as an example to demonstrate how to achieve the above workflow using the BYOC framework. Please note that all referred code and line number in this post are based on the TVM repository’s master branch commit <a href="https://github.com/apache/incubator-tvm/tree/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8">8a0249c</a>.</p>

<h2 id="bring-dnnl-to-tvm-annotation-rules">Bring DNNL to TVM: Annotation Rules</h2>

<p>The BYOC framework provides two approaches for you to describe the supported operators and patterns. You can use both of them simultaneously. In this section, we use DNNL as an example to show how to make use of them. The complete implementation is available <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/python/tvm/relay/op/contrib/dnnl.py">here</a>. Note that we put the annotation rules for your codegen under <code class="language-plaintext highlighter-rouge">python/tvm/relay/op/contrib/your_codegen_name.py</code>.</p>

<h3 id="rules-for-single-operators">Rules for single operators</h3>
<p>You can intuitively specify which Relay operators are supported by your accelerator with the BYOC API. For example, we use the following code snippet to build a rule saying that our DNNL codegen supports Conv2D:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">tvm</span><span class="p">.</span><span class="n">ir</span><span class="p">.</span><span class="n">register_op_attr</span><span class="p">(</span><span class="s">"nn.conv2d"</span><span class="p">,</span> <span class="s">"target.dnnl"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">_dnnl_conv2d_wrapper</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
  <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div>
<p>This registers a new attribute <code class="language-plaintext highlighter-rouge">target.dnnl</code> to Relay <code class="language-plaintext highlighter-rouge">nn.conv2d</code> operator.  By this way, the BYOC annotation could invoke <code class="language-plaintext highlighter-rouge">target.dnnl()</code> for every operator in the graph to check if it is supported in DNNL codegen.</p>

<p>On the other hand, it might be tedious to write the above code snippet for every single operator. For the DNNL implementation, we implemented a helper function, <code class="language-plaintext highlighter-rouge">_register_external_op_helper</code>, to make our life easier:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">_register_external_op_helper</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="n">supported</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="o">@</span><span class="n">tvm</span><span class="p">.</span><span class="n">ir</span><span class="p">.</span><span class="n">register_op_attr</span><span class="p">(</span><span class="n">op_name</span><span class="p">,</span> <span class="s">"target.dnnl"</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_func_wrapper</span><span class="p">(</span><span class="n">attrs</span><span class="p">,</span> <span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">supported</span>
    <span class="k">return</span> <span class="n">_func_wrapper</span>

<span class="n">_register_external_op_helper</span><span class="p">(</span><span class="s">"nn.batch_norm"</span><span class="p">)</span>
<span class="n">_register_external_op_helper</span><span class="p">(</span><span class="s">"nn.conv2d"</span><span class="p">)</span>
<span class="n">_register_external_op_helper</span><span class="p">(</span><span class="s">"nn.dense"</span><span class="p">)</span>
<span class="n">_register_external_op_helper</span><span class="p">(</span><span class="s">"nn.relu"</span><span class="p">)</span>
<span class="n">_register_external_op_helper</span><span class="p">(</span><span class="s">"add"</span><span class="p">)</span>
<span class="n">_register_external_op_helper</span><span class="p">(</span><span class="s">"subtract"</span><span class="p">)</span>
<span class="n">_register_external_op_helper</span><span class="p">(</span><span class="s">"multiply"</span><span class="p">)</span>
</code></pre></div></div>
<p>In the above example, we specify a list of operators that can be supported by DNNL codegen.</p>

<h3 id="rules-for-graph-patterns">Rules for graph patterns</h3>
<p>Your accelerator or compiler may have optimized some patterns (e.g., Conv2D + add + ReLU) to be a single instruction or an API. In this case, you can specify a mapping from a graph pattern to your instruction/API. For the case of the DNNL, its Conv2D API already includes bias addition and it allows the next ReLU to be attached, so we can call DNNL as the following code snippet (the complete implementation can be found <a href="https://github.com/apache/incubator-tvm/blob/main/src/runtime/contrib/dnnl/dnnl_json_runtime.cc#L151">here</a>):</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">DNNLConv2d</span><span class="p">(</span><span class="k">const</span> <span class="n">bool</span> <span class="n">has_bias</span> <span class="o">=</span> <span class="nb">false</span><span class="p">,</span> <span class="k">const</span> <span class="n">bool</span> <span class="n">has_relu</span> <span class="o">=</span> <span class="nb">false</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// ... skip ...</span>
  <span class="k">auto</span> <span class="n">conv_desc</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">desc</span><span class="p">(</span>
    <span class="n">dnnl</span><span class="o">::</span><span class="n">prop_kind</span><span class="o">::</span><span class="n">forward_inference</span><span class="p">,</span>
    <span class="n">dnnl</span><span class="o">::</span><span class="n">algorithm</span><span class="o">::</span><span class="n">convolution_direct</span><span class="p">,</span>
    <span class="n">conv_src_md</span><span class="p">,</span> <span class="n">conv_weights_md</span><span class="p">,</span> <span class="n">conv_bias_md</span><span class="p">,</span> <span class="n">conv_dst_md</span><span class="p">,</span>
    <span class="n">strides_dims</span><span class="p">,</span> <span class="n">padding_dims_l</span><span class="p">,</span> <span class="n">padding_dims_r</span><span class="p">);</span>

  <span class="c1">// Attach ReLU</span>
  <span class="n">dnnl</span><span class="o">::</span><span class="n">primitive_attr</span> <span class="n">attr</span><span class="p">;</span>
  <span class="k">if</span> <span class="p">(</span><span class="n">has_relu</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">dnnl</span><span class="o">::</span><span class="n">post_ops</span> <span class="n">ops</span><span class="p">;</span>
    <span class="n">ops</span><span class="p">.</span><span class="n">append_eltwise</span><span class="p">(</span><span class="mi">1</span><span class="p">.</span><span class="n">f</span><span class="p">,</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">algorithm</span><span class="o">::</span><span class="n">eltwise_relu</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="n">f</span><span class="p">,</span> <span class="mi">0</span><span class="p">.</span><span class="n">f</span><span class="p">);</span>
    <span class="n">attr</span><span class="p">.</span><span class="n">set_post_ops</span><span class="p">(</span><span class="n">ops</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="k">auto</span> <span class="n">conv2d_prim_desc</span> <span class="o">=</span> <span class="n">dnnl</span><span class="o">::</span><span class="n">convolution_forward</span><span class="o">::</span><span class="n">primitive_desc</span><span class="p">(</span>
    <span class="n">conv_desc</span><span class="p">,</span> <span class="n">attr</span><span class="p">,</span> <span class="n">engine_</span><span class="p">);</span>
  <span class="c1">// ... skip ...</span>
</code></pre></div></div>
<p>In this case, except for a single <code class="language-plaintext highlighter-rouge">conv2d</code>, we would like to map the graph pattern <code class="language-plaintext highlighter-rouge">conv2d+relu</code> to <code class="language-plaintext highlighter-rouge">DNNLConv2d(false, true)</code>, and map <code class="language-plaintext highlighter-rouge">conv2d+add+relu</code> to <code class="language-plaintext highlighter-rouge">DNNLConv2d(true, true)</code>. We can achieve it with the following code snippet:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_pattern</span><span class="p">(</span><span class="n">with_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">wildcard</span><span class="p">()</span>
  <span class="n">weight</span> <span class="o">=</span> <span class="n">wildcard</span><span class="p">()</span>
  <span class="n">bias</span> <span class="o">=</span> <span class="n">wildcard</span><span class="p">()</span>
  <span class="n">conv</span> <span class="o">=</span> <span class="n">is_op</span><span class="p">(</span><span class="s">'nn.conv2d'</span><span class="p">)(</span><span class="n">data</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
  <span class="k">if</span> <span class="n">with_bias</span><span class="p">:</span>
    <span class="n">conv_out</span> <span class="o">=</span> <span class="n">is_op</span><span class="p">(</span><span class="s">'add'</span><span class="p">)(</span><span class="n">conv</span><span class="p">,</span> <span class="n">bias</span><span class="p">)</span>
  <span class="k">else</span><span class="p">:</span>
    <span class="n">conv_out</span> <span class="o">=</span> <span class="n">conv</span>
  <span class="k">return</span> <span class="n">is_op</span><span class="p">(</span><span class="s">'nn.relu'</span><span class="p">)(</span><span class="n">conv_out</span><span class="p">)</span>

<span class="o">@</span><span class="n">register_pattern_table</span><span class="p">(</span><span class="s">"dnnl"</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">pattern_table</span><span class="p">():</span>
  <span class="n">conv2d_bias_relu_pat</span> <span class="o">=</span> <span class="p">(</span><span class="s">"dnnl.conv2d_bias_relu"</span><span class="p">,</span> <span class="n">make_pattern</span><span class="p">(</span><span class="n">with_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">))</span>
  <span class="n">conv2d_relu_pat</span> <span class="o">=</span> <span class="p">(</span><span class="s">"dnnl.conv2d_relu"</span><span class="p">,</span> <span class="n">make_pattern</span><span class="p">(</span><span class="n">with_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
  <span class="n">dnnl_patterns</span> <span class="o">=</span> <span class="p">[</span><span class="n">conv2d_bias_relu_pat</span><span class="p">,</span> <span class="n">conv2d_relu_pat</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">dnnl_patterns</span>
</code></pre></div></div>

<p>In the DNNL example, we implemented two patterns with different names so that we can easily recognize them in the codegen. Note that the patterns are implemented in the Relay pattern language. You can follow <a href="https://tvm.apache.org/docs/langref/relay_pattern.html">this tutorial</a> to learn how to write your own patterns.</p>

<p>With the pattern table, we can then use a Relay pass to perform the transformation from</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%1 = nn.conv2d(%data, %weight, ...)
%2 = add(%1, %bias)
%3 = nn.relu(%2)
</code></pre></div></div>
<p>to</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>%1 = fn(%input1, %input2, %input3,
        Composite="dnnl.conv2d_bias_relu",
        PartitionedFromPattern="nn.conv2d_add_nn.relu_") {
  %1 = nn.conv2d(%input1, %input2, ...)
  %2 = add(%1, %input3)
  nn.relu(%2)
}
%2 = %1(%data, %weight, %bias)
</code></pre></div></div>
<p>Thus, the DNNL codegen can get the pattern name <code class="language-plaintext highlighter-rouge">conv2d_bias_relu</code> and map <code class="language-plaintext highlighter-rouge">%1</code> to <code class="language-plaintext highlighter-rouge">DNNLConv2d(true, true)</code>.</p>

<p>As you may have noticed that we also have an attribute called “PartitionedFromPattern” in the composite function. This could be helpful if your pattern contains <code class="language-plaintext highlighter-rouge">wildcard</code> operators. For example we may have a pattern table <code class="language-plaintext highlighter-rouge">("conv2d_with_something", conv2d -&gt; *)</code>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_pattern</span><span class="p">(</span><span class="n">with_bias</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
  <span class="n">data</span> <span class="o">=</span> <span class="n">wildcard</span><span class="p">()</span>
  <span class="n">weight</span> <span class="o">=</span> <span class="n">wildcard</span><span class="p">()</span>
  <span class="n">conv</span> <span class="o">=</span> <span class="n">is_op</span><span class="p">(</span><span class="s">'nn.conv2d'</span><span class="p">)(</span><span class="n">data</span><span class="p">,</span> <span class="n">weight</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">wildcard</span><span class="p">()(</span><span class="n">conv</span><span class="p">)</span>
</code></pre></div></div>
<p>In this case, you will get a composite function with <code class="language-plaintext highlighter-rouge">Composite=conv2d_with_something</code>, but you have no idea about what graph it actually matched. That’s where PartitionedFromPattern comes into play. You can know that if the matched graph is <code class="language-plaintext highlighter-rouge">conv2d -&gt; add</code> or <code class="language-plaintext highlighter-rouge">conv2d -&gt; relu</code> by looking at <code class="language-plaintext highlighter-rouge">PartitionedFromPattern</code> to see if it is <code class="language-plaintext highlighter-rouge">nn.conv2d_add_</code> or <code class="language-plaintext highlighter-rouge">nn.conv2d_nn.relu_</code>.</p>

<h2 id="bring-dnnl-to-tvm-relay-graph-transformation">Bring DNNL to TVM: Relay Graph Transformation</h2>
<p>With the annotation rules from the previous step, we can now apply a list of BYOC Relay passes to transform the Relay graph from Figure 1 to Figure 4:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">mod</span> <span class="o">=</span> <span class="n">create_relay_module_from_model</span><span class="p">()</span> <span class="c1"># Output: Figure 1
</span><span class="n">mod</span> <span class="o">=</span> <span class="n">transform</span><span class="p">.</span><span class="n">MergeComposite</span><span class="p">(</span><span class="n">pattern_table</span><span class="p">)(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">transform</span><span class="p">.</span><span class="n">AnnotateTarget</span><span class="p">([</span><span class="s">"dnnl"</span><span class="p">])(</span><span class="n">mod</span><span class="p">)</span> <span class="c1"># Output: Figure 2
</span><span class="n">mod</span> <span class="o">=</span> <span class="n">transform</span><span class="p">.</span><span class="n">MergeCompilerRegions</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span> <span class="c1"># Output: Figure 3
</span><span class="n">mod</span> <span class="o">=</span> <span class="n">transform</span><span class="p">.</span><span class="n">PartitionGraph</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span> <span class="c1"># Output: Figure 4
</span></code></pre></div></div>
<p>As can be seen, each Relay pass can be mapped to a step we have introduced in <a href="#how-byoc-works">How BYOC Works</a>.</p>

<h2 id="bring-dnnl-to-tvm-json-codegenruntime">Bring DNNL to TVM: JSON Codegen/Runtime</h2>
<p>Now let’s implement the DNNL codegen that serializes a Relay graph to a JSON representation, and then implement the DNNL JSON runtime to deserialize and execute the graph. <em>Note that if you attempt to implement a codegen to generate C-compatible programs, you may want to directly proceed to the next section.</em></p>

<p>To enable DNNL JSON codegen/runtime in TVM to work on this example, please make sure DNNL is available on your machine, and build the TVM with <code class="language-plaintext highlighter-rouge">set(USE_DNNL_CODEGEN ON)</code> in <code class="language-plaintext highlighter-rouge">config.cmake</code>.</p>

<p>The DNNL codegen is implemented in <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc"><code class="language-plaintext highlighter-rouge">src/relay/backend/contrib/dnnl/codegen.cc</code></a>. Since we implemented DNNL codegen in both forms in this file for illustration purpose, you could focus on the part covered by <code class="language-plaintext highlighter-rouge">USE_JSON_RUNTIME</code> macro when tracing the code.</p>

<p>We first register the codegen with TVM registration API (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L510">L510</a>). This registration makes TVM compile engine dispatch the Relay function with <code class="language-plaintext highlighter-rouge">Compiler=&lt;your codegen&gt;</code>  to <code class="language-plaintext highlighter-rouge">relay.ext.&lt;your codegen&gt;</code>. Then we implement the entry function of the DNNL compiler (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L490">L490</a>). Please read the comments embedded in the code snippet for details:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span> <span class="nf">DNNLCompiler</span><span class="p">(</span><span class="k">const</span> <span class="n">ObjectRef</span><span class="o">&amp;</span> <span class="n">ref</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// "ref" should be the paritioned Relay function with kCompiler=dnnl.</span>
  <span class="n">CHECK</span><span class="p">(</span><span class="n">ref</span><span class="o">-&gt;</span><span class="n">IsInstance</span><span class="o">&lt;</span><span class="n">FunctionNode</span><span class="o">&gt;</span><span class="p">());</span>
  <span class="k">auto</span> <span class="n">func</span> <span class="o">=</span> <span class="n">Downcast</span><span class="o">&lt;</span><span class="n">Function</span><span class="o">&gt;</span><span class="p">(</span><span class="n">ref</span><span class="p">);</span>

  <span class="c1">// Get the function name as the symbol to match in runtime.</span>
  <span class="k">auto</span> <span class="n">func_name</span> <span class="o">=</span> <span class="n">GetExtSymbol</span><span class="p">(</span><span class="n">func</span><span class="p">);</span>

  <span class="c1">// Serialize the function to a JSON string (introduce later).</span>
  <span class="n">DNNLJSONSerializer</span> <span class="n">serializer</span><span class="p">(</span><span class="n">func_name</span><span class="p">,</span> <span class="n">func</span><span class="p">);</span>
  <span class="n">serializer</span><span class="p">.</span><span class="n">serialize</span><span class="p">();</span>
  <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">graph_json</span> <span class="o">=</span> <span class="n">serializer</span><span class="p">.</span><span class="n">GetJSON</span><span class="p">();</span>

  <span class="c1">// The constant tensor names that have been bound to the module.</span>
  <span class="c1">// All constant tensors will be serialzied along with the JSON graph</span>
  <span class="c1">// when export_library is invoked.</span>
  <span class="k">auto</span> <span class="n">params</span> <span class="o">=</span> <span class="n">serializer</span><span class="p">.</span><span class="n">GetParams</span><span class="p">();</span>

  <span class="c1">// The function to create DNNL JSON runtime (introduce later).</span>
  <span class="k">const</span> <span class="k">auto</span><span class="o">*</span> <span class="n">pf</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">::</span><span class="n">Registry</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="s">"runtime.DNNLJSONRuntimeCreate"</span><span class="p">);</span>
  <span class="n">CHECK</span><span class="p">(</span><span class="n">pf</span> <span class="o">!=</span> <span class="n">nullptr</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">"Cannot find JSON runtime module to create"</span><span class="p">;</span>

  <span class="c1">// Create a DNNL runtime module that can run the serialized function.</span>
  <span class="k">auto</span> <span class="n">mod</span> <span class="o">=</span> <span class="p">(</span><span class="o">*</span><span class="n">pf</span><span class="p">)(</span><span class="n">func_name</span><span class="p">,</span> <span class="n">graph_json</span><span class="p">,</span> <span class="n">params</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">mod</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">"relay.ext.dnnl"</span><span class="p">).</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">DNNLCompiler</span><span class="p">);</span>
</code></pre></div></div>

<p>Note that <strong><em>each runtime module is only responsible for one Relay function, meaning that you may have several DNNL runtime modules in a single <code class="language-plaintext highlighter-rouge">.so</code> file.</em></strong></p>

<h3 id="dnnl-json-serialization">DNNL JSON Serialization</h3>
<p>Next, we implement DNNL JSON serializer (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L429">L429</a>). We derived it from the BYOC JSON codegen (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/codegen_json/codegen_json.h">src/relay/backend/contrib/codegen_json/codegen_json.h</a>). The special process in DNNL JSON serializer attempts to serialize a composite function call to a JSON node that can be interpreted by DNNL JSON runtime. Assuming we have a composite function which matches the pattern <code class="language-plaintext highlighter-rouge">dnnl.conv2d_relu</code>, then the BYOC JSON codegen will generate the following JSON node:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="err">op:</span><span class="w"> </span><span class="s2">"kernel"</span><span class="p">,</span><span class="w">
  </span><span class="err">name:</span><span class="w"> </span><span class="s2">"dnnl.conv2d_relu"</span><span class="p">,</span><span class="w">
  </span><span class="err">inputs:</span><span class="w"> </span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]],</span><span class="w">
  </span><span class="err">attrs:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="err">PartitionedFromPattern:</span><span class="w"> </span><span class="p">[</span><span class="s2">"nn.conv2d_nn.relu_"</span><span class="p">],</span><span class="w">
    </span><span class="err">shape:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">14</span><span class="p">,</span><span class="w"> </span><span class="mi">14</span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>
<p>The problem is that we still need the Conv2D attributes such as padding and strides in runtime, but the BYOC JSON serializer only attaches the attributes of the composite function instead of the body operators. On the other hand, the customized DNNL JSON serializer attaches the attributes of the first and only Conv2D in the composite function to generate the following JSON node:</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="err">op:</span><span class="w"> </span><span class="s2">"kernel"</span><span class="p">,</span><span class="w">
  </span><span class="err">name:</span><span class="w"> </span><span class="s2">"dnnl.conv2d_relu"</span><span class="p">,</span><span class="w">
  </span><span class="err">inputs:</span><span class="w"> </span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">],</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">]],</span><span class="w">
  </span><span class="err">attrs:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="err">shape:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">32</span><span class="p">,</span><span class="w"> </span><span class="mi">14</span><span class="p">,</span><span class="w"> </span><span class="mi">14</span><span class="p">],</span><span class="w">
    </span><span class="err">data_layout:</span><span class="w"> </span><span class="p">[</span><span class="s2">"NCHW"</span><span class="p">],</span><span class="w">
    </span><span class="err">kernel_layout:</span><span class="w"> </span><span class="p">[</span><span class="s2">"OIHW"</span><span class="p">],</span><span class="w">
    </span><span class="err">strides:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">],</span><span class="w">
    </span><span class="err">padding:</span><span class="w"> </span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">]</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div>

<p>As can be seen from the DNNL JSON serializer, you can customize the serializer to generate any forms in JSON you like as long as your JSON runtime could interpret them.</p>

<h3 id="dnnl-json-runtime">DNNL JSON Runtime</h3>

<p>We then implement a DNNL JSON runtime to interpret and execute the serialized JSON graph. We put it under <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/runtime/contrib/dnnl/dnnl_json_runtime.cc"><code class="language-plaintext highlighter-rouge">src/runtime/contrib/dnnl/dnnl_json_runtime.cc</code></a>.</p>

<p>Again, we first register two APIs to create the runtime so that we can use them anywhere. The <code class="language-plaintext highlighter-rouge">runtime.DNNLJSONRuntimeCreate</code> is used in the previous part after serialization, and <code class="language-plaintext highlighter-rouge">runtime.module.loadbinary_dnnl_json</code> could be used when loading the <code class="language-plaintext highlighter-rouge">.so</code> back.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Create a DNNL JSON runtime to interpret and execute the given JSON graph.</span>
<span class="n">runtime</span><span class="o">::</span><span class="n">Module</span> <span class="nf">DNNLJSONRuntimeCreate</span><span class="p">(</span><span class="n">String</span> <span class="n">symbol_name</span><span class="p">,</span> <span class="n">String</span> <span class="n">graph_json</span><span class="p">,</span>
                                      <span class="k">const</span> <span class="n">Array</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;&amp;</span> <span class="n">const_names</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">auto</span> <span class="n">n</span> <span class="o">=</span> <span class="n">make_object</span><span class="o">&lt;</span><span class="n">DNNLJSONRuntime</span><span class="o">&gt;</span><span class="p">(</span><span class="n">symbol_name</span><span class="p">,</span> <span class="n">graph_json</span><span class="p">,</span> <span class="n">const_names</span><span class="p">);</span>
  <span class="k">return</span> <span class="n">runtime</span><span class="o">::</span><span class="n">Module</span><span class="p">(</span><span class="n">n</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">"runtime.DNNLJSONRuntimeCreate"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">DNNLJSONRuntimeCreate</span><span class="p">);</span>

<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">"runtime.module.loadbinary_dnnl_json"</span><span class="p">)</span>
    <span class="p">.</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">JSONRuntimeBase</span><span class="o">::</span><span class="n">LoadFromBinary</span><span class="o">&lt;</span><span class="n">DNNLJSONRuntime</span><span class="o">&gt;</span><span class="p">);</span>
</code></pre></div></div>

<p>Now we explain DNNL JSON runtime implementation. The basic class structure is:</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">class</span> <span class="n">DNNLJSONRuntime</span> <span class="o">:</span> <span class="n">public</span> <span class="n">JSONRuntimeBase</span> <span class="p">{</span>
  <span class="k">const</span>  <span class="kt">char</span><span class="o">*</span> <span class="n">type_key</span><span class="p">()</span> <span class="k">const</span> <span class="p">{</span> <span class="k">return</span>  <span class="s">"dnnl_json"</span><span class="p">;</span> <span class="p">}</span> 
  <span class="kt">void</span> <span class="n">Init</span><span class="p">(</span><span class="k">const</span> <span class="n">Array</span><span class="o">&lt;</span><span class="n">NDArray</span><span class="o">&gt;&amp;</span> <span class="n">consts</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
    <span class="c1">// Initialize the DNNL graph engine.</span>
    <span class="n">BuildEngine</span><span class="p">();</span>
    
    <span class="c1">// Setup constants entries for weights.</span>
    <span class="n">CHECK_EQ</span><span class="p">(</span><span class="n">consts</span><span class="p">.</span><span class="n">size</span><span class="p">(),</span> <span class="n">const_idx_</span><span class="p">.</span><span class="n">size</span><span class="p">())</span>
      <span class="o">&lt;&lt;</span> <span class="s">"The number of input constants must match the number of required."</span><span class="p">;</span>
    <span class="n">SetupConstants</span><span class="p">(</span><span class="n">consts</span><span class="p">);</span>
  <span class="p">}</span>

  <span class="kt">void</span> <span class="n">Run</span><span class="p">()</span> <span class="n">override</span> <span class="p">{</span>
   <span class="c1">// 1. Fill in the input buffers.</span>
   <span class="c1">// 2. Invoke the engine through intepreting the stream.</span>
   <span class="c1">// 3. Read and fill output buffers.</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>The <code class="language-plaintext highlighter-rouge">Init</code> function is in charge of building the DNNL engine by interpreting the JSON graph string (see <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/runtime/contrib/dnnl/dnnl_json_runtime.cc#L93">L93</a> for <code class="language-plaintext highlighter-rouge">BuildEngine</code>), and filling the constant weights to the corresponding data entry buffers (the <code class="language-plaintext highlighter-rouge">SetupConstant</code> is implemented in the JSON runtime base class so you only need to invoke it in <code class="language-plaintext highlighter-rouge">Init</code>). Note that this function will be called only once even we run multiple times of inferences.</p>

<p>Next, the <code class="language-plaintext highlighter-rouge">Run</code> function (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/runtime/contrib/dnnl/dnnl_json_runtime.cc#L64">L64</a>) first writes the input tensors, which may come from user inputs or constant weights, to the corresponding DNNL memory buffers we initialized when building the DNNL engine. Then launch the DNNL engine to execute the JSON graph. Finally, it writes the DNNL output memory buffers back to the corresponding output tensors.</p>

<p>Since the rest implementation in DNNL JSON runtime are too DNNL specific to be dived into details in this post, we will stop here. We would like to emphasize that while the DNNL JSON runtime is a good reference to start with, your JSON runtime could be fully customized to fit your requirements.</p>

<h2 id="bring-dnnl-to-tvm-c-source-codegen">Bring DNNL to TVM: C Source Codegen</h2>
<p>Now let’s implement the DNNL codegen that generates C source code which invokes DNNL APIs to execute the Relay graph.<em>Note that if you attempt to implement a codegen to generate other graph representation like in JSON format, you may want to read <a href="#bring-dnnl-to-tvm-json-codegenruntime">Bring DNNL to TVM: JSON Codegen/Runtime</a> and skip this section.</em></p>

<p>To enable DNNL C source codegen in TVM to work on this example, please make sure DNNL is available on your machine, and build the TVM with <code class="language-plaintext highlighter-rouge">set(USE_DNNL_CODEGEN C_SRC)</code> in <code class="language-plaintext highlighter-rouge">config.cmake</code>.</p>

<p>The DNNL codegen is implemented in <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc"><code class="language-plaintext highlighter-rouge">src/relay/backend/contrib/dnnl/codegen.cc</code></a>. Since we implemented DNNL codegen in both forms in this file for illustration purpose, you could focus on the part <strong>NOT</strong> covered by <code class="language-plaintext highlighter-rouge">USE_JSON_RUNTIME</code> macro when tracing the code.</p>

<p>We first register the codegen with TVM registration API (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L510">L510</a>). This registration makes TVM compile engine dispatch the Relay function with <code class="language-plaintext highlighter-rouge">Compiler=&lt;your codegen&gt;</code>  to <code class="language-plaintext highlighter-rouge">relay.ext.&lt;your codegen&gt;</code>. Then we implement the entry function of the DNNL compiler (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L490">L490</a>):</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span> <span class="nf">DNNLCompiler</span><span class="p">(</span><span class="k">const</span> <span class="n">ObjectRef</span><span class="o">&amp;</span> <span class="n">ref</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">DNNLModuleCodegen</span> <span class="n">dnnl</span><span class="p">;</span>
  <span class="k">return</span> <span class="n">dnnl</span><span class="p">.</span><span class="n">CreateCSourceModule</span><span class="p">(</span><span class="n">ref</span><span class="p">);</span>
<span class="p">}</span>
<span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">"relay.ext.dnnl"</span><span class="p">).</span><span class="n">set_body_typed</span><span class="p">(</span><span class="n">DNNLCompiler</span><span class="p">);</span>
</code></pre></div></div>

<p>Note that <strong><em>each runtime module is only responsible for one Relay function, meaning that you may have several DNNL runtime modules in a single <code class="language-plaintext highlighter-rouge">.so</code> file.</em></strong></p>

<p>Then, we derive <code class="language-plaintext highlighter-rouge">CSourceModuleCodegenBase</code> to implement  <code class="language-plaintext highlighter-rouge">DNNLModuleCodegen</code> in <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L362">L362</a>. While <code class="language-plaintext highlighter-rouge">CSourceModuleCodegenBase</code> is in charge of other module level processes such as serialization, we only need to implement the DNNL code generation in the <code class="language-plaintext highlighter-rouge">CreateCSourceModule</code> function (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L389">L389</a>):</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">runtime</span><span class="o">::</span><span class="n">Module</span> <span class="n">CreateCSourceModule</span><span class="p">(</span><span class="k">const</span> <span class="n">ObjectRef</span><span class="o">&amp;</span> <span class="n">ref</span><span class="p">)</span> <span class="n">override</span> <span class="p">{</span>
    <span class="c1">// Include headers</span>
    <span class="c1">// ...skip...</span>
    <span class="n">code_stream_</span> <span class="o">&lt;&lt;</span> <span class="s">"#include &lt;dnnl/dnnl_kernel.h&gt;</span><span class="se">\n</span><span class="s">"</span><span class="p">;</span>
    <span class="c1">// ...skip...</span>

    <span class="c1">// "ref" should be the paritioned Relay function with kCompiler=dnnl.</span>
    <span class="n">CHECK</span><span class="p">(</span><span class="n">ref</span><span class="o">-&gt;</span><span class="n">IsInstance</span><span class="o">&lt;</span><span class="n">FunctionNode</span><span class="o">&gt;</span><span class="p">());</span>
    <span class="k">auto</span> <span class="n">res</span> <span class="o">=</span> <span class="n">GenDNNLFunc</span><span class="p">(</span><span class="n">Downcast</span><span class="o">&lt;</span><span class="n">Function</span><span class="o">&gt;</span><span class="p">(</span><span class="n">ref</span><span class="p">));</span>

    <span class="c1">// "code" is the generated C code with DNNL APIs.</span>
    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">code</span> <span class="o">=</span> <span class="n">code_stream_</span><span class="p">.</span><span class="n">str</span><span class="p">();</span>

    <span class="c1">// "res" is a tuple of constant weights (symbols, values).</span>
    <span class="c1">// All constant tensors will be serialzied along with the generated C code</span>
    <span class="c1">// when export_library is invoked.</span>
    <span class="n">String</span> <span class="n">sym</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">0</span><span class="o">&gt;</span><span class="p">(</span><span class="n">res</span><span class="p">);</span>
    <span class="n">Array</span><span class="o">&lt;</span><span class="n">String</span><span class="o">&gt;</span> <span class="n">variables</span> <span class="o">=</span> <span class="n">std</span><span class="o">::</span><span class="n">get</span><span class="o">&lt;</span><span class="mi">1</span><span class="o">&gt;</span><span class="p">(</span><span class="n">res</span><span class="p">);</span>

    <span class="c1">// Create a CSource module with all above artifacts.</span>
    <span class="k">const</span> <span class="k">auto</span><span class="o">*</span> <span class="n">pf</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">::</span><span class="n">Registry</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="s">"runtime.CSourceModuleCreate"</span><span class="p">);</span>
    <span class="n">CHECK</span><span class="p">(</span><span class="n">pf</span> <span class="o">!=</span> <span class="n">nullptr</span><span class="p">)</span> <span class="o">&lt;&lt;</span> <span class="s">"Cannot find csource module to create the external runtime module"</span><span class="p">;</span>
    <span class="k">return</span> <span class="p">(</span><span class="o">*</span><span class="n">pf</span><span class="p">)(</span><span class="n">code</span><span class="p">,</span> <span class="s">"c"</span><span class="p">,</span> <span class="n">sym</span><span class="p">,</span> <span class="n">variables</span><span class="p">);</span>
  <span class="p">}</span>
</code></pre></div></div>

<p>Next, we implement <code class="language-plaintext highlighter-rouge">GenDNNLFunc</code> (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L365">L365</a>) to generate the compilable C code with DNNL APIs as follows. Please see the embedded comments for the explanations of TVM C source runtime module compatible function interfaces.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// The example Relay graph: conv2d -&gt; add -&gt; relu.</span>
<span class="cp">#include</span> <span class="cpf">&lt;cstdint&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cstdlib&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;cstring&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;vector&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;tvm/runtime/c_runtime_api.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;tvm/runtime/container.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;tvm/runtime/packed_func.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;dlpack/dlpack.h&gt;</span><span class="cp">
#include</span> <span class="cpf">&lt;dnnl/dnnl_kernel.h&gt;</span><span class="cp">
</span><span class="n">using</span> <span class="n">namespace</span> <span class="n">tvm</span><span class="o">::</span><span class="n">runtime</span><span class="p">;</span>
<span class="n">using</span> <span class="n">namespace</span> <span class="n">tvm</span><span class="o">::</span><span class="n">runtime</span><span class="o">::</span><span class="n">contrib</span><span class="p">;</span>

<span class="c1">// Execute the conv2d-&gt;add-&gt;relu graph with DNNL.</span>
<span class="k">extern</span> <span class="s">"C"</span> <span class="kt">void</span> <span class="nf">dnnl_0_</span><span class="p">(</span><span class="kt">float</span><span class="o">*</span> <span class="n">dnnl_0_i0</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">dnnl_0_i1</span><span class="p">,</span>
                        <span class="kt">float</span><span class="o">*</span> <span class="n">dnnl_0_i2</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">out0</span><span class="p">)</span> <span class="p">{</span>
  <span class="c1">// Allocate intermediate buffers.</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">buf_0</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">malloc</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">4608</span><span class="p">);</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">buf_1</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">malloc</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">4608</span><span class="p">);</span>
  <span class="kt">float</span><span class="o">*</span> <span class="n">buf_2</span> <span class="o">=</span> <span class="p">(</span><span class="kt">float</span><span class="o">*</span><span class="p">)</span><span class="n">std</span><span class="o">::</span><span class="n">malloc</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="mi">4608</span><span class="p">);</span>

  <span class="c1">// Pre-implemented op-based DNNL functions.</span>
  <span class="n">dnnl_conv2d</span><span class="p">(</span><span class="n">dnnl_0_i0</span><span class="p">,</span> <span class="n">dnnl_0_i1</span><span class="p">,</span> <span class="n">buf_0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
  <span class="n">dnnl_add</span><span class="p">(</span><span class="n">buf_0</span><span class="p">,</span> <span class="n">dnnl_0_i2</span><span class="p">,</span> <span class="n">buf_1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">);</span>
  <span class="n">dnnl_relu</span><span class="p">(</span><span class="n">buf_1</span><span class="p">,</span> <span class="n">buf_2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">12</span><span class="p">);</span>

  <span class="c1">// Copy the final output to the corresponding buffer.</span>
  <span class="n">std</span><span class="o">::</span><span class="n">memcpy</span><span class="p">(</span><span class="n">out0</span><span class="p">,</span> <span class="n">buf_2</span><span class="p">,</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">4608</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">free</span><span class="p">(</span><span class="n">buf_0</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">free</span><span class="p">(</span><span class="n">buf_1</span><span class="p">);</span>
  <span class="n">std</span><span class="o">::</span><span class="n">free</span><span class="p">(</span><span class="n">buf_2</span><span class="p">);</span>
<span class="p">}</span>

<span class="c1">// The wrapper function with all arguments in DLTensor type.</span>
<span class="k">extern</span> <span class="s">"C"</span> <span class="kt">int</span> <span class="nf">dnnl_0_wrapper_</span><span class="p">(</span><span class="n">DLTensor</span><span class="o">*</span> <span class="n">arg0</span><span class="p">,</span>
        <span class="n">DLTensor</span><span class="o">*</span> <span class="n">arg1</span><span class="p">,</span>
        <span class="n">DLTensor</span><span class="o">*</span> <span class="n">arg2</span><span class="p">,</span>
        <span class="n">DLTensor</span><span class="o">*</span> <span class="n">out0</span><span class="p">)</span> <span class="p">{</span>

  <span class="c1">// Cast all DLTensor to primitive type buffers and invoke the above</span>
  <span class="c1">// execution function.</span>
  <span class="n">dnnl_0_</span><span class="p">(</span><span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">arg0</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">),</span>
  <span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">arg1</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">),</span>
  <span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">arg2</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">),</span>
  <span class="n">static_cast</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">out0</span><span class="o">-&gt;</span><span class="n">data</span><span class="p">));</span>
  <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>

<span class="c1">// The TVM macro to generate TVM runtime compatible function "dnnl_0"</span>
<span class="c1">// from our generated "dnnl_0_wrapper_".</span>
<span class="n">TVM_DLL_EXPORT_TYPED_FUNC</span><span class="p">(</span><span class="n">dnnl_0</span><span class="p">,</span> <span class="n">dnnl_0_wrapper_</span><span class="p">);</span>
</code></pre></div></div>

<p>Note that the pre-implemented op-based DNNL functions are in <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/runtime/contrib/dnnl/dnnl.cc">src/runtime/contrib/dnnl/dnnl.cc</a>.</p>

<p>Since the rest implementation in <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc"><code class="language-plaintext highlighter-rouge">src/relay/backend/contrib/dnnl/codegen.cc</code></a> are too DNNL specific to be dived into details in this post, we will stop here. The main idea is implementing a Relay graph visitor (<a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/src/relay/backend/contrib/dnnl/codegen.cc#L138">L138</a>) to visit the given Relay function and generate the above C code. As long as your codegen is able to generate the TVM runtime compatible C code, you can fully customize the codegen to fit your requirements.</p>

<h3 id="c-source-compilation">C Source Compilation</h3>
<p>As you may have noticed, the output of <code class="language-plaintext highlighter-rouge">DNNLCompiler</code> is a module with the generated C code in text format, which has not been compiled by <code class="language-plaintext highlighter-rouge">gcc</code> to be executable binary. In fact, the generated C code will be compiled when users call <code class="language-plaintext highlighter-rouge">export_libray(mod)</code>, like the following code snippet:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">update_lib</span><span class="p">(</span><span class="n">lib</span><span class="p">):</span>
    <span class="c1"># Include the path of src/runtime/contrib/dnnl/dnnl.cc
</span>    <span class="n">test_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">realpath</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">expanduser</span><span class="p">(</span><span class="n">__file__</span><span class="p">)))</span>
    <span class="n">source_dir</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">test_dir</span><span class="p">,</span> <span class="s">".."</span><span class="p">,</span> <span class="s">".."</span><span class="p">,</span> <span class="s">".."</span><span class="p">)</span>
    <span class="n">contrib_path</span> <span class="o">=</span> <span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">source_dir</span><span class="p">,</span> <span class="s">"src"</span><span class="p">,</span> <span class="s">"runtime"</span><span class="p">,</span> <span class="s">"contrib"</span><span class="p">)</span>

    <span class="c1"># Setup the gcc flag to compile DNNL code.
</span>    <span class="n">kwargs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s">"options"</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="s">"-O2"</span><span class="p">,</span> <span class="s">"-std=c++14"</span><span class="p">,</span> <span class="s">"-I"</span> <span class="o">+</span> <span class="n">contrib_path</span><span class="p">]</span>
    <span class="n">tmp_path</span> <span class="o">=</span> <span class="n">util</span><span class="p">.</span><span class="n">tempdir</span><span class="p">()</span>
    <span class="n">lib_name</span> <span class="o">=</span> <span class="s">'lib.so'</span>
    <span class="n">lib_path</span> <span class="o">=</span> <span class="n">tmp_path</span><span class="p">.</span><span class="n">relpath</span><span class="p">(</span><span class="n">lib_name</span><span class="p">)</span>

    <span class="c1"># The generated C code with DNNL APIs is compiled to a binary lib.so.
</span>    <span class="n">lib</span><span class="p">.</span><span class="n">export_library</span><span class="p">(</span><span class="n">lib_path</span><span class="p">,</span> <span class="n">fcompile</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Load the lib.so back to a runtime module.
</span>    <span class="n">lib</span> <span class="o">=</span> <span class="n">runtime</span><span class="p">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lib</span>

<span class="k">with</span> <span class="n">tvm</span><span class="p">.</span><span class="n">transform</span><span class="p">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">json</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <span class="n">param</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
<span class="n">lib</span> <span class="o">=</span> <span class="n">update_lib</span><span class="p">(</span><span class="n">lib</span><span class="p">)</span>
<span class="n">rt_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">graph_runtime</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">json</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="bring-dnnl-to-tvm-build-tvm-with-dnnl-codegenruntime">Bring DNNL to TVM: Build TVM with DNNL Codegen/Runtime</h2>
<p>Finally, we create <a href="https://github.com/apache/incubator-tvm/blob/8a0249cd4d12a2eb1a4e7a692a9265bc63fec5c8/cmake/modules/contrib/DNNL.cmake">cmake/modules/contrib/DNNL.cmake</a> to include the DNNL codegen when building TVM. For demonstration purpose our DNNL codegen has two implementations in the same cmake file. You can only focus on one of them based on your need.</p>

<p>With the cmake file ready, now users can specify <code class="language-plaintext highlighter-rouge">set(USE_DNNL_CODEGEN ON)</code> in their <code class="language-plaintext highlighter-rouge">build/config.cmake</code> to enable the DNNL codegen.</p>

<hr />
<ul>
  <li>
    <p><a href="https://github.com/zhiics">Zhi Chen</a> is a TVM PMC member as well as a senior engineer at SageMaker Neo, Amazon AI, AWS.</p>
  </li>
  <li>
    <p><a href="https://comaniac.github.io">Cody Yu</a> is a TVM reviewer as well as an applied scientist at Amazon AI, AWS.</p>
  </li>
</ul>

<h2 id="acknowledgment">Acknowledgment</h2>

<p>We would like to thank our colleague Animesh Jain for valuable discussions in the framework design; Tianqi Chen and Jared Roesch from OctoML for system design discussions and prototyping; Masahiro Masuda from the TVM community to help code review and improve the DNNL integration. We would also like to thank Ramana Radhakrishnan, Matthew Barrett, Manupa Karunaratne, and Luke Hutton from ARM, U.K. for contributing several helpful ideas, related Relay passes, and the Arm Compute Library (ACL) integration with BYOC.</p>]]></content><author><name>Zhi Chen and Cody Yu, Amazon Web Services, Inc</name></author><summary type="html"><![CDATA[To free data scientists from worrying about the performance when developing a new model, hardware backend providers (e.g., Intel, NVIDIA, ARM, etc) either provide kernel libraries such as cuBLAS or cuDNN with many commonly used deep learning kernels, or provide frameworks such as DNNL or TensorRT with a graph engine to let users describe their models in a certain way to achieve high performance. In addition, emerging deep learning accelerators also have their own compilers, kernel libraries, or runtime frameworks.]]></summary></entry><entry><title type="html">Bridging PyTorch and TVM</title><link href="/2020/07/14/bert-pytorch-tvm" rel="alternate" type="text/html" title="Bridging PyTorch and TVM" /><published>2020-07-14T00:00:00+00:00</published><updated>2020-07-14T00:00:00+00:00</updated><id>/2020/07/14/bert-pytorch-tvm</id><content type="html" xml:base="/2020/07/14/bert-pytorch-tvm"><![CDATA[<p>(A more code-heavy variant is crossposted on the more PyTorch affine <a href="https://lernapparat.de/transformers-pytorch-tvm/">Lernapparat</a>,
 the Jupyter Notebook to follow along is on <a href="https://github.com/t-vi/pytorch-tvmisc/tree/master/transformers-pytorch-tvm/">github</a>.)</p>

<p>Some of the most intriguing applications of Artificial Intelligence have been in Natural Language Processing.
Models like BERT or GPT-2 and their variants can seemingly grasp enough of a text to continue it in a way that needs a second look to recognize as gibberish.</p>

<p>These models belong to a class of neural network architectures called <em>Transformers</em>. One of the favourite libraries
implementing them is the <a href="https://github.com/huggingface/transformers/">HuggingFace transformers library</a>.</p>

<p>But, in contrast to convolutional models or LSTMs where we have heavily optimized implementations, this is not as much the case for transformers.
So here we explore how TVM can fill the gap. We will do so in two steps:</p>

<ul>
  <li>First we look at BERT inference and tuning that on TVM.</li>
  <li>Secondly, we start some more fundamental exploration of how one could use TVM for training in PyTorch.
Given the experimental nature, we focus on feasibility more than on the performance in this part.</li>
</ul>

<h1 id="optimizing-bert-inference-with-tvm">Optimizing BERT Inference with TVM</h1>

<p>So how do we get BERT from the transformer library to TVM?</p>

<p>Helpfully, transformers supports tracing their model with the PyTorch JIT. We use their <a href="https://huggingface.co/transformers/torchscript.html">tutorial on it</a>,
specifically the part until we have a traced model.</p>

<p>The PyTorch traced model takes around 0.65-0.7 seconds for 100 runs on my AMD Radeon VII with the example inputs, which means 6.5-7ms per run.
We can try to see if we can use TVM get faster. Let converting our model to TVM is a breeze:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">shape_list</span> <span class="o">=</span> <span class="p">[(</span><span class="n">i</span><span class="p">.</span><span class="n">debugName</span><span class="p">().</span><span class="n">split</span><span class="p">(</span><span class="s">'.'</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="n">i</span><span class="p">.</span><span class="nb">type</span><span class="p">().</span><span class="n">sizes</span><span class="p">())</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>  <span class="nb">list</span><span class="p">(</span><span class="n">traced_model</span><span class="p">.</span><span class="n">graph</span><span class="p">.</span><span class="n">inputs</span><span class="p">())[</span><span class="mi">1</span><span class="p">:]]</span>

<span class="n">mod_bert</span><span class="p">,</span> <span class="n">params_bert</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">frontend</span><span class="p">.</span><span class="n">pytorch</span><span class="p">.</span><span class="n">from_pytorch</span><span class="p">(</span><span class="n">traced_model</span><span class="p">,</span>
                        <span class="n">shape_list</span><span class="p">,</span> <span class="n">default_dtype</span><span class="o">=</span><span class="s">"float32"</span><span class="p">)</span>
</code></pre></div></div>

<p>There will be a few warnings about not finding dtype information, but it goes well!
We can now build and run it. Building follows the standard TVM recipe. We also convert the PyTorch (cpu) tensors to TVM arrays.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target</span> <span class="o">=</span> <span class="s">'rocm -model=gfx906'</span>  <span class="c1"># use what matches your GPU
</span>
<span class="n">target_host</span> <span class="o">=</span> <span class="s">'llvm'</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">context</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

<span class="n">tt_a</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">tokens_tensor</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">ctx</span><span class="p">)</span>
<span class="n">st_a</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">segments_tensors</span><span class="p">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">ctx</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">backend</span><span class="p">.</span><span class="n">compile_engine</span><span class="p">.</span><span class="n">get</span><span class="p">().</span><span class="n">clear</span><span class="p">()</span> <span class="c1"># just to be sure, see https://github.com/apache/incubator-tvm/pull/5724
</span>
<span class="k">with</span> <span class="n">tvm</span><span class="p">.</span><span class="n">transform</span><span class="p">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">graph</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod_bert</span><span class="p">,</span>
                                     <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span>
                                     <span class="n">target_host</span><span class="o">=</span><span class="n">target_host</span><span class="p">,</span>
                                     <span class="n">params</span><span class="o">=</span><span class="n">params_bert</span><span class="p">)</span>
<span class="n">module</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">contrib</span><span class="p">.</span><span class="n">graph_runtime</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <span class="n">ctx</span><span class="p">)</span>
</code></pre></div></div>

<p>This will warn us a few times times:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    WARNING:autotvm:Cannot find config for ... batch_matmul.cuda .... A fallback configuration is used, which may bring great performance regression.
</code></pre></div></div>

<p>Uh oh, <em>may bring great performance regression</em>. Let us see.</p>

<p>But first we run the model and see if the outputs match:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    <span class="p">(</span><span class="mf">8.583069e-06</span><span class="p">,</span> <span class="mf">8.493662e-07</span><span class="p">)</span>
</code></pre></div></div>

<p>Looks good. Remember that we’re computing in float32, so $10^{-6}$ish is a good result.</p>

<p>After building our model and setting the parameters, we time our model like this:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">x</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
        <span class="n">module</span><span class="p">.</span><span class="n">run</span><span class="p">()</span>
    <span class="n">ctx</span><span class="p">.</span><span class="n">sync</span><span class="p">()</span>
<span class="n">x</span><span class="p">()</span>
<span class="o">%</span><span class="n">timeit</span> <span class="n">x</span><span class="p">()</span>
</code></pre></div></div>

<p>Ouch, it takes 6.65s per 100 runs, or 67ms per run of the model. That’s slow indeed. But the warning said that is was because it could not find (tuned) configurations. Let us then tune the tasks.</p>

<p>Tuning does take half a day or so (I’m basically following the TVM tuning tutorial for ResNet tuning with autotvm.)</p>

<p>After this, we can again build the model, this time with the new configuration. This time we should see no comments about missing configurations.
Now it’s in the region of 6.5-7ms per run, similar to PyTorch. This is what we get from this very elementary optimization of our operators. We can push it a little further, though.</p>

<p>To see how, let us dive deep into BERT modeling and TVM.</p>

<p>If you don’t want to get the full details, do skip the next section and scroll down to <em>Results</em>. I should add that I would hope that this tuning part of the tutorial will obsolete itself in the sense that in some near future, you will get much better speed right out of the box or at least after some initial tuning. So if you don’t see a speedup between here and <em>Results</em>, that’s because I did my homework in submitting patches.</p>

<h2 id="the-bert-model">The BERT model</h2>

<p>Let us take a closer look at what’s going on in BERT.</p>

<p>Like many deep learning models, BERT comes with a bit some prologue (vocabulary embeddings) and epilogue (pooling) and the bulk is organized into similar-looking blocks, here we have 12 <code class="language-plaintext highlighter-rouge">BertLayer</code> modules.
The <code class="language-plaintext highlighter-rouge">attention_mask</code> is jsut to prevent BERT from looking at the answer when dealing with the question.</p>

<p><img src="/images/bert-pytorch/bert_model.svg" alt="Bert Model" width="100%" /></p>

<p>So let us zoom in and look at a BertLayer in detail, since that ultimately is what we need make fast.
As we see in the net diagram, the main part of the <code class="language-plaintext highlighter-rouge">BertLayer</code> module is a submodule <code class="language-plaintext highlighter-rouge">BertSelfAttention</code>.</p>

<p><img src="/images/bert-pytorch/bert_layer.svg" alt="BertLayer" width="100%" /></p>

<p>Now the <code class="language-plaintext highlighter-rouge">BertSelfAttention</code> captures the famed self-attention mechanism that is the hallmark of transformer models. (I cannot recommend Sascha Rush’s <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">Annotated Transformer</a> enough as a detailed walkthrough.)</p>

<h2 id="putting-the-bertlayer-under-the-microscope">Putting the BertLayer under the Microscope</h2>

<p>If we want go into details, we should want to run a BertLayer individually.
We grab the inputs of a BertLayer (see the Notebook for how) and convert a single <code class="language-plaintext highlighter-rouge">BertLayer</code> to TVM as we did for the entire model.</p>

<p>To look at the TVM module, we define a little visualization helper (loosely based on TVM <a href="https://github.com/apache/incubator-tvm/pull/4370">PR#4370</a>).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">graphviz</span>
<span class="k">def</span> <span class="nf">visualize</span><span class="p">(</span><span class="n">expr</span><span class="p">,</span> <span class="n">collapse_small</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">node_attr_dict</span> <span class="o">=</span> <span class="p">{}):</span>
    <span class="k">def</span> <span class="nf">collect_ops</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
        <span class="n">ops</span> <span class="o">=</span> <span class="nb">set</span><span class="p">()</span>
        <span class="k">def</span> <span class="nf">visitor</span><span class="p">(</span><span class="n">e</span><span class="p">):</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ir</span><span class="p">.</span><span class="n">Op</span><span class="p">):</span>
                <span class="n">ops</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">e</span><span class="p">.</span><span class="n">name</span><span class="p">)</span>
        <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">post_order_visit</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">visitor</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ops</span>

    <span class="c1"># node_dict maps a Relay node to an index (node ID)
</span>    <span class="k">def</span> <span class="nf">_traverse_expr</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">node_dict</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">node</span> <span class="ow">in</span> <span class="n">node_dict</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">node_dict</span><span class="p">[</span><span class="n">node</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">node_dict</span><span class="p">)</span>

    <span class="n">node_dict</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">analysis</span><span class="p">.</span><span class="n">post_order_visit</span><span class="p">(</span><span class="n">expr</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">_traverse_expr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">node_dict</span><span class="p">))</span>

    <span class="n">relayviz_nodes</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">dot</span> <span class="o">=</span> <span class="n">graphviz</span><span class="p">.</span><span class="n">Digraph</span><span class="p">(</span><span class="nb">format</span><span class="o">=</span><span class="s">'svg'</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">dot</span><span class="p">.</span><span class="n">attr</span><span class="p">(</span><span class="s">'node'</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="s">'box'</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">to_str</span><span class="p">(</span><span class="n">node</span><span class="p">):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Constant</span><span class="p">):</span>
            <span class="k">return</span> <span class="nb">repr</span><span class="p">(</span><span class="n">node</span><span class="p">).</span><span class="n">lstrip</span><span class="p">(</span><span class="s">'Constant('</span><span class="p">)[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">NotImplementedError</span><span class="p">(</span><span class="s">"to_str:"</span> <span class="o">+</span> <span class="nb">repr</span><span class="p">(</span><span class="n">node</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">is_small_const</span><span class="p">(</span><span class="n">c</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">collapse_small</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Constant</span><span class="p">)):</span>
            <span class="k">return</span> <span class="bp">False</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">data</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">runtime</span><span class="p">.</span><span class="n">ndarray</span><span class="p">.</span><span class="n">NDArray</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">numpy</span><span class="p">.</span><span class="n">prod</span><span class="p">(</span><span class="n">c</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">10</span>
        <span class="k">return</span> <span class="bp">True</span>

    <span class="c1"># Sort by node ID
</span>    <span class="k">for</span> <span class="n">node</span><span class="p">,</span> <span class="n">node_id</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">node_dict</span><span class="p">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Function</span><span class="p">):</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span> <span class="s">'Function'</span><span class="p">,</span> <span class="o">**</span><span class="n">node_attr_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">{}))</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_dict</span><span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">body</span><span class="p">]),</span> <span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Var</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">node</span><span class="p">.</span><span class="n">type_annotation</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">type_annotation</span><span class="p">,</span> <span class="s">'shape'</span><span class="p">):</span>
                    <span class="n">shape</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">type_annotation</span><span class="p">.</span><span class="n">shape</span><span class="p">])</span>
                    <span class="n">dtype</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">type_annotation</span><span class="p">.</span><span class="n">dtype</span>
                    <span class="n">typstr</span> <span class="o">=</span> <span class="s">'Tensor[{}, {}]'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">typstr</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">type_annotation</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">typstr</span> <span class="o">=</span> <span class="s">'?'</span>
            <span class="n">d</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="s">'ellipse'</span><span class="p">)</span>
            <span class="n">d</span><span class="p">.</span><span class="n">update</span><span class="p">(</span><span class="n">node_attr_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">{}))</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span>
                     <span class="s">'{}: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
                         <span class="n">node</span><span class="p">.</span><span class="n">name_hint</span><span class="p">,</span> <span class="n">typstr</span>
                     <span class="p">),</span> <span class="o">**</span><span class="n">d</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Tuple</span><span class="p">):</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span> <span class="s">'Tuple[...])'</span><span class="p">,</span> <span class="o">**</span><span class="n">node_attr_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">{}))</span>
            <span class="k">for</span> <span class="n">field</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">fields</span><span class="p">:</span>
                <span class="n">dot</span><span class="p">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_dict</span><span class="p">[</span><span class="n">field</span><span class="p">]),</span> <span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Constant</span><span class="p">):</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">is_small_const</span><span class="p">(</span><span class="n">node</span><span class="p">):</span> <span class="c1"># small consts are shown in ops
</span>                <span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span> <span class="s">'Constant({}, {})'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">node</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">dtype</span><span class="p">),</span>
                        <span class="o">**</span><span class="n">node_attr_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">{}))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Call</span><span class="p">):</span>
            <span class="n">args_with_edge</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">arg_str_list</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">args</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">is_small_const</span><span class="p">(</span><span class="n">arg</span><span class="p">):</span>
                    <span class="n">arg_str_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">to_str</span><span class="p">(</span><span class="n">arg</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">arg_str_list</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="s">'·'</span><span class="p">)</span>
                    <span class="n">args_with_edge</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span>
            <span class="n">arg_str</span> <span class="o">=</span> <span class="s">', '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">arg_str_list</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ir</span><span class="p">.</span><span class="n">Op</span><span class="p">):</span>
                <span class="n">name</span> <span class="o">=</span> <span class="n">node</span><span class="p">.</span><span class="n">op</span><span class="p">.</span><span class="n">name</span>
                <span class="n">attrs</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="nb">getattr</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">attrs</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">node</span><span class="p">.</span><span class="n">attrs</span><span class="p">.</span><span class="n">keys</span><span class="p">()}</span> <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">attrs</span><span class="p">,</span> <span class="s">'keys'</span><span class="p">)</span> <span class="k">else</span> <span class="p">{}</span>
                <span class="c1">#attrs = inspect.getmembers(node.attrs)
</span>                <span class="n">attr_str_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">k</span><span class="o">+</span><span class="s">'='</span><span class="o">+</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">))</span><span class="o">&lt;</span><span class="mi">20</span> <span class="k">else</span> <span class="s">"..."</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">attrs</span><span class="p">.</span><span class="n">items</span><span class="p">()]</span>
                <span class="k">if</span> <span class="n">attr_str_list</span><span class="p">:</span>
                    <span class="n">attr_str</span> <span class="o">=</span> <span class="s">'| '</span><span class="o">+</span> <span class="s">', '</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">attr_str_list</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">attr_str</span> <span class="o">=</span> <span class="s">''</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ops</span> <span class="o">=</span> <span class="n">collect_ops</span><span class="p">(</span><span class="n">node</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">ops</span><span class="p">:</span>
                    <span class="n">name</span> <span class="o">=</span> <span class="s">'_'</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">ops</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">name</span> <span class="o">=</span> <span class="s">'...'</span>
                <span class="n">attr_str</span> <span class="o">=</span> <span class="s">''</span>
            <span class="n">s</span> <span class="o">=</span> <span class="sa">f</span><span class="s">'</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s">(</span><span class="si">{</span><span class="n">arg_str</span><span class="si">}{</span><span class="n">attr_str</span><span class="si">}</span><span class="s">)'</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span> <span class="n">s</span><span class="p">,</span> <span class="o">**</span><span class="n">node_attr_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">{}))</span>
            <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args_with_edge</span><span class="p">:</span>
                <span class="n">dot</span><span class="p">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_dict</span><span class="p">[</span><span class="n">arg</span><span class="p">]),</span> <span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">ir</span><span class="p">.</span><span class="n">Op</span><span class="p">):</span>
            <span class="c1"># dot.node(str(node_id), 'Op {}'.format(node.name))
</span>            <span class="k">pass</span> <span class="c1"># covered in call
</span>        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">TupleGetItem</span><span class="p">):</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span> <span class="s">'TupleGetItem(idx={})'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">node</span><span class="p">.</span><span class="n">index</span><span class="p">),</span> <span class="o">**</span><span class="n">node_attr_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">{}))</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_dict</span><span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">tuple_value</span><span class="p">]),</span> <span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">Let</span><span class="p">):</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">node</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span> <span class="s">'Let(XX)'</span><span class="p">,</span> <span class="o">**</span><span class="n">node_attr_dict</span><span class="p">.</span><span class="n">get</span><span class="p">(</span><span class="n">node</span><span class="p">,</span> <span class="p">{}))</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_dict</span><span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">value</span><span class="p">]),</span> <span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">))</span>
            <span class="n">dot</span><span class="p">.</span><span class="n">edge</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">node_id</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="n">node_dict</span><span class="p">[</span><span class="n">node</span><span class="p">.</span><span class="n">var</span><span class="p">]))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="nb">RuntimeError</span><span class="p">(</span>
                <span class="s">'Unknown node type. node_id: {}, node: {}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">node_id</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">node</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">dot</span>

</code></pre></div></div>

<p>Let’s run that on our main function. For some reason (well, to be fully general, probably) the PyTorch converter will convert <code class="language-plaintext highlighter-rouge">Linear</code> layers to <code class="language-plaintext highlighter-rouge">batch_matmul</code> rather than just <code class="language-plaintext highlighter-rouge">dense</code>. We’ll get back to this in a bit. As TVM’s <code class="language-plaintext highlighter-rouge">batch_matmul</code> has the contraction axis last on both operands (unlike PyTorch), there are quite a few transpose operations, too.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">visualize</span><span class="p">(</span><span class="n">mod</span><span class="p">[</span><span class="s">'main'</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/images/bert-pytorch/bert-tvm_49_0.svg" alt="svg" width="100%" /></p>

<p>In addition to our named inputs, we see a number of unnamed (numbered) variables. These are the neural network parameters.</p>

<p>Let us compile our model.</p>

<p>Just like the full model, we can run and time our submodule after checking that it computes the same quantities.</p>

<p>100 runs take 20.2ms. The back of the envelope calculation here is that with <code class="language-plaintext highlighter-rouge">BertLayer</code> in PyTorch we are spending about 0.2ms in this layer, so about 2.4ms on 12 layers - a not the majority but a sizeable part of the 6-7ms overall runtime. Let’s compare to TVM. (A good rule is to never optimize without measuring.)</p>

<p>Similarly, TVM clocks in at 18.2ms for 100 runs. So here we are again roughly on par with PyTorch.</p>

<p>One thing we see from the picture is that the input is reshaped three times. There is a TVM optimization pass call Common Subexpression Elimination (CSE) that combines the three reshapes.
(A while ago, this did not succeed because it had distinct shape arguments, but this was since solved by the TVM developers in the dynamic to static conversion pass.)
Also, the model parameters that are reshaped and transposed. Can we get rid of that, too?
Yes. And for that we would first <em>bind</em> the parameters, i.e. put them into the model. Then the parameters have become constants instead of input nodes.
With the <code class="language-plaintext highlighter-rouge">Foldconstant</code> pass, we can propagate the constants through the <code class="language-plaintext highlighter-rouge">transpose</code>s and <code class="language-plaintext highlighter-rouge">reshape</code>s to move them closer to the matmuls.</p>

<p>After these three (which TVM will do when we compile a relay model), our model looks like this:</p>

<p><img src="/images/bert-pytorch/bert-tvm_72_0.svg" alt="svg" width="100%" /></p>

<p>And now comes an interesting trick. It is more efficient to merge the three batch matmuls with the same input into a single <code class="language-plaintext highlighter-rouge">batch_matmul</code>. We implemented a pass doing this in <a href="https://github.com/apache/incubator-tvm/pull/5791">TVM PR 5791</a>. So let’s call it and also have another constant-folding pass.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">transform</span><span class="p">.</span><span class="n">CombineParallelBatchMatmul</span><span class="p">()(</span><span class="n">new_mod</span><span class="p">)</span>
<span class="n">new_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">relay</span><span class="p">.</span><span class="n">transform</span><span class="p">.</span><span class="n">FoldConstant</span><span class="p">()(</span><span class="n">new_mod</span><span class="p">)</span>
<span class="n">visualize</span><span class="p">(</span><span class="n">new_mod</span><span class="p">[</span><span class="s">"main"</span><span class="p">])</span>
</code></pre></div></div>

<p><img src="/images/bert-pytorch/bert-tvm_74_0.svg" alt="svg" width="100%" /></p>

<p>Awesome. After checking that we still get the same result.
We can time again: 25.2 ms for 100 runs. It’s a bit slow again because we need to tune for the new shapes.
After tuning, we are at 12.6ms for 100 runs, so we went from about 0.2ms to about 0.13-0.15ms, a nice speedup.
By our handwavy calculation, this should cut 0.6-0.8ms from the total runtime, or somewhere between 5%-10%. Let’s check.</p>

<h2 id="results-on-the-overall-bert-model-after-optimization">Results on the overall BERT model after optimization</h2>

<p>Let’s define a function combining the optimization passes from above and run it on the entire BERT model.
We go through the same exercise as above.</p>

<p>We get to 624ms for 100 runs. So yay, we went from 6.5-7ms in PyTorch to ~6.2ms in TVM. This is a 5%-10% speedup. Note that we have only taking a particular, not very large shape. A more serious analysis would consider more problem shapes.</p>

<p>We could probably take it a bit further yet - e.g. fusing the additions after the batch matmul by handling the reshape, but we’ll leave it at this for now. Also we will benefit from further improvements to TVM, so it will be interesting to see how the benchmark improves over time. In particular, the upcoming Ansor tuning mechanism seems promising.</p>

<h2 id="a-peek-under-the-hood">A peek under the hood</h2>

<h3 id="comparing-implementation-of-models">Comparing implementation of models</h3>

<p>As you can see, I have always compared PyTorch with TVM outputs to see if they’re good.
Also, when I investigated some inner layer, I grabbed the inputs to that to convert and feed into the TVM model. I do believe that this is a very effective technique.</p>

<p>Sometimes, however, it is difficult to assess whether a deviation between the results is from numerical accuracy or from an error somewhere.
When I initially converted the model, the the <code class="language-plaintext highlighter-rouge">SelfAttention</code> submodule output was replicated by the TVM model to about 1e-6.
However, the BertLayer conversion had something like 1-e3. I was not entirely clear whether that might be due to accumulated numerical errors or some material deviation somewhere.
(This turned out to be the GELU activation, which was converted to FastGELU.)</p>

<p>One of the things I like to do in this case is jump to double precision and check there. Numerical errors should get much smaller, while other deviations would remain of the same order.
With the PyTorch frontend, you can trace the model converted to float64 on the PyTorch side if you pass <code class="language-plaintext highlighter-rouge">default_dtype="float64"</code> to the conversion function.</p>

<p>Running the module and comparing to PyTorch should now have 1e-14 or so deviation.</p>

<h3 id="improvements-in-tvm-to-facilitate-this-usecase">Improvements in TVM to facilitate this usecase</h3>

<p>Before this worked as shown here, we had to close some gaps (but a recent git checkout will include all of them):</p>
<ul>
  <li>The TVM PyTorch converter did not support inputs other than fp32. We <a href="https://github.com/t-vi/tvm/tree/pytorch_frontend_type_fix">implemented improved conversion</a>, now also included in TVM upsteam.</li>
  <li>The TVM schedule, i.e. the organization of the computation, of the workhorse operation, batch_matmul, was fixed and it was very slow (similar to running without a tuned schedule now). So we <a href="https://github.com/apache/incubator-tvm/pull/5752">implemented a tuneable schedule</a>.</li>
  <li>The PyTorch converter produces batch matmul operations (it could probably also be changed to produce dense layers instead). But as we saw, one of the larger speed advantages is to combine Query Key and Value linear layers, so we implemented <a href="https://github.com/apache/incubator-tvm/pull/5791">fusing batch matmul operations</a>.</li>
  <li>When comparing the computation results, we noticed that the <a href="https://pytorch.org/docs/master/generated/torch.nn.GELU.html">GELU</a> function was converted to its FastGELU variant. We fixed that. (There is a <em>fast math</em> optimization pass in TVM that does some replacement of the error function, though we didn’t check if it yields FastGELU for the GELU expressed with the error function.)</li>
  <li>TVM was initially (and still is to a some extent) focussed on static shapes. Recently it experiments with dynamic operations. The dynamic reshape - taking an argument for the target shape - is an early of these experiments, but as seen above, it prevented the fusion of batch matmuls because the common subexpression elimination pass didn’t detect that it could merge the identical input reshaping. This has improved recently.</li>
</ul>

<h1 id="training-pytorch-models-with-tvm-computation">Training Pytorch models with TVM computation</h1>

<p>In this second part we want see if we could use TVM while training BERT in PyTorch.
Of course, this opens an entire new can of worms as we need to deal with autodifferentiation.
While we stay with the theme from above and take <code class="language-plaintext highlighter-rouge">BertLayer</code> as the example, our methodology is representative of non-trivial modules in general.
We will want to divert the computation during training to TVM.</p>

<p>So the user can take a (traceable) module and do</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>add_tvm_dispatch(module, sample_input)
</code></pre></div></div>
<p>and then if she calls module with inputs of the same shape as the sample_input, she’ll get the outputs computed by TVM (as PyTorch tensors, of course) and if not, it’ll just use the regular forward.</p>

<p>The but so we already hinted at the bad news: In this part we will see how to do these things. We will not yet achieve a great speedup.</p>

<p>But enough talk, let us dive right in!
Again, we get our relay model with running a traced <code class="language-plaintext highlighter-rouge">BertLayer</code> from the transformer <code class="language-plaintext highlighter-rouge">Bert</code> model through <code class="language-plaintext highlighter-rouge">tvm.relay.frontend.from_pytorch</code>.</p>

<p>One thing we’ll do in between is to move from a modular interface in PyTorch - with named parameters - to a functional
interface (which is what TVM can do for us). The first thing we want to do for that is arrange for the function arguments to be in an order that we can work with - i.e. first the direct inputs to the module and then the parameters in the same order that PyTorch uses them. After this operation, our <code class="language-plaintext highlighter-rouge">BertLayer </code> in TVM looks like this:</p>

<p><img src="/images/bert-pytorch/pytorch-tvm-training_20_0.svg" alt="svg" width="100%" /></p>

<p>As in the BERT inference, we want to run some optimization passes.</p>

<p>But we also have a few new transformations:</p>

<ul>
  <li>One particularity of the Autodifferentiation is that it’ll use a lot of <code class="language-plaintext highlighter-rouge">..._like</code> operations to broadcast or “unbroadcast” (summation is the dual of broadcasting w.r.t. autodifferentiation) things. But this means that you now have two tensor arguments, even if the latter doesn’t really need a gradient. <code class="language-plaintext highlighter-rouge">ZappLike</code> replaces those operations with the corresponding functions taking a shape parameter instead.</li>
  <li>Another thing is the “rooting” of derivatives. TVM generates a tensors with all ones of the same shape as the return values of our function as the starting point for the chain rule. These are then multiplied to the derivatives of our operations. But multiplication with ones is not doing much, so we strike that. Similarly, TVM initializes the gradient of a variable (an input) to zeros of the same shape. If it isn’t used, the gradient will be zero, but if it is, the “real gradient” will be added to that zero. But adding zero can be eliminated as well. These are taken care off by ZeroZapp and OneZapp.</li>
  <li>TVM doesn’t have a training variant for the <code class="language-plaintext highlighter-rouge">LayerNorm</code> (or <code class="language-plaintext highlighter-rouge">BatchNorm</code> or others). So we implement a pass to spell out the computation.</li>
  <li>TVM also doesn’t have training dropout. Here the problem is somewhat harder to fix, as TVM doesn’t have random currently. We instead replace the dropout by a construct taking a random bernoulli draw (of 0/1 values) and mimicking dropout with that. The idea is that we’ll use PyTorch to generate this mask for us. This has the added benefit that (if we generate dropout masks in the same order as PyTorch) we’ll get the exact same result.</li>
</ul>

<p>As hinted at above, TVM’s gradient taking assumes that it is the last element in the computation (the ones-Tensors discussed above). This isn’t a good fit with PyTorch’s modular view which expects a <code class="language-plaintext highlighter-rouge">grad_out</code> for each output to be given. Happily, this is computationally equivalent to multiplying by grad out and summation, so we amend our function with that. We wish to be flexible, so we allow both functions returning a single tensor and those returning a tuple of tensors.</p>

<p>With these modificaitons applied, our model looks like this:</p>

<p><img src="/images/bert-pytorch/pytorch-tvm-training_25_0.svg" alt="svg" width="100%" /></p>

<p>Finally we can take the grad. As we get a lot of <code class="language-plaintext highlighter-rouge">let</code> nodes, we bring it to normal form using the <code class="language-plaintext highlighter-rouge">ToGraphNormalForm</code> pass.
TVM’s gradient-taking returns a function that has the same parameters as the original function (in our case amended with the <code class="language-plaintext highlighter-rouge">grad_out</code> and dropout) and then returns a tuple of the original return and a tuple containing gradients for all inputs.
The first thing we do is to drop all the gradients for <code class="language-plaintext highlighter-rouge">grad_out</code> and <code class="language-plaintext highlighter-rouge">dropout</code> which we don’t need.
Then we run our simplification passes.</p>

<p>So this is the graph we have now for forward and backward:</p>

<p><img src="/images/bert-pytorch/pytorch-tvm-training_31_0.svg" alt="svg" width="100%" /></p>

<p>But in PyTorch, we first compute the forward and then the backwards, so we have to take out the saw and
split our graph. One of the difficult problems is what to do with things computed for both forward and backward. It is a hard problem, related to the MinCut problem.</p>

<p>Our extremal options could be:</p>
<ul>
  <li>One could only keep the inputs and recompute everything as needed.</li>
  <li>If we had a salar output, we could compute the gradient and multiply with the derivative of the later layers on backward. (Loss functions might do that.) This does not, however, work for non-scalar tensor outputs.</li>
</ul>

<p>We’ll do the following: We compute the forward normally, but we keep all things that will be used in the backward. This is too much, unfortunately, and it is very likely the reason we don’t see an end to end speedup. We’ll discuss some potential heuristics below.</p>

<p>We use a coloring here. First we color all nodes of the forward computation in red. Then we traverse the gradient calculation and then color the nodes it needs from the backward blue. This gives us a chance to show off the attribute support in our visualization.</p>

<p>A bit of (PyTorch) terminology: When we have a function <em>Layer : x ↦ y</em> followed by some <em>Loss: y ↦ l ∈ ℝ</em>, the backward is <em>BackwardOfLayer : grad<code class="language-plaintext highlighter-rouge">_</code>out ↦ grad<code class="language-plaintext highlighter-rouge">_</code>in</em> with <em>grad<code class="language-plaintext highlighter-rouge">_</code>out = dl/dy</em> and *grad<code class="language-plaintext highlighter-rouge">_</code>in = dl/dx`.</p>

<p><img src="/images/bert-pytorch/pytorch-tvm-training_34_0.svg" alt="svg" width="100%" /></p>

<p>In order to split the function as described above, we collect the blue nodes as to capture - but constants will
just be duplicated and inputs (<code class="language-plaintext highlighter-rouge">Var</code> nodes) need to be treated separately.
Now we can split out the backward, replacing all the blue nodes with variables.</p>

<p>Next we take the forward and amend it to also return the required intermediates. The forward then looks like this:</p>

<p><img src="/images/bert-pytorch/pytorch-tvm-training_40_0.svg" alt="svg" width="100%" /></p>

<p>TVM cannot return nested tuples, so we flatten the output in the function. Again we differentiate between tensor-valued functions and tuple valued ones (i.e. those returning potentially multiple tensors).</p>

<p>And at last, we can let TVM do its magic and compile our functions, say to <code class="language-plaintext highlighter-rouge">gr_only_compiled_module</code>
and <code class="language-plaintext highlighter-rouge">fw_and_cap_compiled_module</code>.
Time to give it a spin. We define convenience functions to move tensors between PyTorch and TVM and get the model parameters as a TVM dictionary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tensor_to_tvm</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">dlpack</span><span class="p">.</span><span class="n">to_dlpack</span><span class="p">(</span><span class="n">t</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">tensor_from_tvm</span><span class="p">(</span><span class="n">a</span><span class="p">):</span>
    <span class="k">return</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">dlpack</span><span class="p">.</span><span class="n">from_dlpack</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="n">to_dlpack</span><span class="p">()))</span>

<span class="n">model_params_tvm</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">tensor_to_tvm</span><span class="p">(</span><span class="n">v</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pytorch_model</span><span class="p">.</span><span class="n">state_dict</span><span class="p">().</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></div>

<p>Similarly, we get the inputs on the GPU in PyTorch and TVM.</p>

<p>We need to deal with the dropout. It will turn out that our record of the three dropout random draws happens in the same order as the dropout in the model. We did a depth-first search on the computational graph to find them and if the values of the the dropout are connected in the graph rather than being on independent branches, this will be the order in which PyTorch draws the matrices, too. If not, good luck fiddeling with the order.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
<span class="n">drop_c</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">dropout_info</span><span class="p">.</span><span class="n">keys</span><span class="p">():</span> <span class="c1"># we don't know the order
</span>    <span class="n">p</span><span class="p">,</span> <span class="n">typ</span> <span class="o">=</span> <span class="n">dropout_info</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
    <span class="n">drop_c</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">([</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">typ</span><span class="p">.</span><span class="n">shape</span><span class="p">],</span>
                                              <span class="n">dtype</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">torch</span><span class="p">,</span> <span class="n">typ</span><span class="p">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">),</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">p</span><span class="p">)</span>

<span class="n">drop_tvm</span> <span class="o">=</span> <span class="p">{</span><span class="n">n</span><span class="p">:</span> <span class="n">tensor_to_tvm</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="k">for</span> <span class="n">n</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">drop_c</span><span class="p">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></div>

<p>Now we can run the forward.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">fw_and_cap_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="s">'input'</span><span class="p">,</span> <span class="n">inp_tvm</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">fw_and_cap_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="s">'attention_mask'</span><span class="p">,</span> <span class="n">inp_tvm</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">fw_and_cap_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="o">**</span><span class="n">model_params_tvm</span><span class="p">)</span>
<span class="n">fw_and_cap_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="o">**</span><span class="n">drop_tvm</span><span class="p">)</span>
<span class="n">fw_and_cap_compiled_module</span><span class="p">.</span><span class="n">run</span><span class="p">()</span>
</code></pre></div></div>

<p>And we can compare the output to PyTorch’s:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
<span class="n">pytorch_model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pytorch_model</span><span class="p">(</span><span class="o">*</span><span class="n">inp_c</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">numpy</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">fw_and_cap_compiled_module</span><span class="p">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">asnumpy</span><span class="p">()</span><span class="o">-</span><span class="n">res</span><span class="p">.</span><span class="n">detach</span><span class="p">().</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()).</span><span class="nb">max</span><span class="p">()</span>
</code></pre></div></div>

<p>This gives <code class="language-plaintext highlighter-rouge">2.1457672e-06</code>.</p>

<p>Supergood. Let’s also try the backward. We generate a <code class="language-plaintext highlighter-rouge">grad_out</code>, set all the variables and run the backward model and run the backward model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">gr_out_c</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="n">res</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s">"cuda"</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">res</span><span class="p">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_captures</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">capture_vars</span><span class="p">)</span>
<span class="n">num_regular_outputs</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fw_and_cap_fn_flattened</span><span class="p">.</span><span class="n">body</span><span class="p">.</span><span class="n">fields</span><span class="p">)</span> <span class="o">-</span> <span class="n">num_captures</span>
<span class="n">captured_values</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">.</span><span class="n">name_hint</span><span class="p">:</span> <span class="n">fw_and_cap_compiled_module</span><span class="p">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">num_regular_outputs</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">capture_vars</span><span class="p">)}</span>

<span class="n">gr_only_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="o">**</span><span class="n">drop_tvm</span><span class="p">)</span>
<span class="n">gr_only_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="o">**</span><span class="n">model_params_tvm</span><span class="p">)</span>
<span class="n">gr_only_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="o">**</span><span class="n">captured_values</span><span class="p">)</span>
<span class="n">gr_only_compiled_module</span><span class="p">.</span><span class="n">set_input</span><span class="p">(</span><span class="s">'gr:out:0'</span><span class="p">,</span> <span class="n">tensor_to_tvm</span><span class="p">(</span><span class="n">gr_out_c</span><span class="p">))</span>
<span class="n">gr_only_compiled_module</span><span class="p">.</span><span class="n">run</span><span class="p">()</span>
</code></pre></div></div>

<p>On the PyTorch side, it is easiest to re-run the forward (remembering to reset the random seed) and get the grads.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">12345</span><span class="p">)</span>
<span class="n">pytorch_model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
<span class="n">inp_c_rq</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">inp_c</span><span class="p">]</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">pytorch_model</span><span class="p">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">p</span><span class="p">.</span><span class="n">requires_grad_</span><span class="p">()</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">pytorch_model</span><span class="p">(</span><span class="o">*</span><span class="n">inp_c_rq</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">grads_pt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="n">grad</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">inp_c_rq</span> <span class="o">+</span> <span class="nb">list</span><span class="p">(</span><span class="n">pytorch_model</span><span class="p">.</span><span class="n">parameters</span><span class="p">()),</span> <span class="n">gr_out_c</span><span class="p">,</span> <span class="n">allow_unused</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p>Did it work? It seems so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">g_pt</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">grads_pt</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">numpy</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">gr_only_compiled_module</span><span class="p">.</span><span class="n">get_output</span><span class="p">(</span><span class="n">i</span><span class="p">).</span><span class="n">asnumpy</span><span class="p">()</span> <span class="o">-</span> <span class="n">g_pt</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()).</span><span class="nb">max</span><span class="p">())</span>
</code></pre></div></div>

<p>gives us a list of numbers in the 1e-5ish range.</p>

<p>But we wanted to get something running in PyTorch, right?</p>

<p>Keeping with how PyTorch works, we first define an <code class="language-plaintext highlighter-rouge">autograd.Function</code> that the things we just did manually:</p>

<p>In the <code class="language-plaintext highlighter-rouge">forward</code>:</p>

<ul>
  <li>Generate the dropout random values,</li>
  <li>Run the forward,</li>
  <li>Record the captures, inputs, and dropout values needed for backward.</li>
</ul>

<p>In the <code class="language-plaintext highlighter-rouge">backward</code>, run the backward and return the result (as PyTorch tensors).</p>

<p>With that, we get a PyTorch autograd.Function calling into TVM (we would want a small wrapper for that.</p>

<p>Now all we need to do to achive our goal of getting a method <code class="language-plaintext highlighter-rouge">add_tvm_dispatch(module, sample_inputs)</code> is
to trace the module, create the TVM-based autograd function from it and then replace the forward that calls
that (with the parameters) if applicable or falls back to the usual forward.
Python’s unlimited dynamism makes that kind of hackery relatively easy.
As all this it is not really TVM-related, we are sparing us that here (but you could check the
<a href="https://lernapparat.de/transformers-pytorch-tvm/">companion post</a>.</p>

<h2 id="performance">Performance</h2>

<p>As I said in the beginning, we aren’t quite where we want to eventually be in terms of performance.
After tuning the tasks (and on the not very realistic inference example from the HuggingFace BERT + PyTorch JIT tutorial)
we run 100 iterations of the TVM-enabled BertLayer forward and backward similar to how we did it for the inference.
One iteration takes 6.2ms going through TVM versus 1.3ms on PyTorch.</p>

<p>So ran our model through TVM all right. But it’s not as fast as the usual method yet. Here is to opportunity!</p>

<p>More seriously, we have two immediate paths to improve performance:</p>

<ul>
  <li>Find a better set of captured nodes.</li>
  <li>Find optimizations on the TVM graph.</li>
</ul>

<p>In terms of heuristics for the former (remember that it quite likely NP hard, i.e. I believe it is, but I didn’t work out a formal proof),
one would want to re-do cheap computation, most prominently point-wise computation (or maybe anything but matmul?). But that is for another day.</p>

<p>I hope you enjoyed the tutorial, I look forward to your comments at <a href="mailto:tv@lernapparat.de">tv@lernapparat.de</a>.</p>

<h1 id="acknowledgements">Acknowledgements</h1>

<p>I had many interesting discussions with HugingFace people and Morgan Funtowicz in particular. Also the TVM contributors had many good comments during the review of the patches TVM and on the forums. The creation of this tutorial was sponsored by AMD.</p>

<h1 id="author">Author</h1>

<p><a href="https://lernapparat.de/">Thomas Viehmann</a> is the founder of <a href="https://mathinf.eu/">MathInf GmbH</a>, Munich, Germany, a boutique training and consultancy firm focusing on Machine Learning and PyTorch.
He is a PyTorch core developer and co-authored <a href="https://www.manning.com/books/deep-learning-with-pytorch">Deep Learning with PyTorch</a>, which currently available as <a href="https://pytorch.org/deep-learning-with-pytorch">free download from the PyTorch website</a>.</p>]]></content><author><name>Thomas Viehmann, MathInf GmbH</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">TinyML - How TVM is Taming Tiny</title><link href="/2020/06/04/tinyml-how-tvm-is-taming-tiny" rel="alternate" type="text/html" title="TinyML - How TVM is Taming Tiny" /><published>2020-06-04T00:00:00+00:00</published><updated>2020-06-04T00:00:00+00:00</updated><id>/2020/06/04/tinyml-how-tvm-is-taming-tiny</id><content type="html" xml:base="/2020/06/04/tinyml-how-tvm-is-taming-tiny"><![CDATA[<p><img src="/images/microtvm/logo.png" alt="microTVM logo" width="30%" /><br /></p>

<p>The proliferation of low-cost, AI-powered consumer devices has led to widespread interest in “bare-metal” (low-power, often without an operating system) devices among ML researchers and practitioners.  While it is already possible for experts to run <em>some</em> models on <em>some</em> bare-metal devices, optimizing models for diverse sets of devices is challenging, often requiring manually optimized device-specific libraries.  And for those platforms without, say, Linux support, there exists no scalable solution for deploying models.  Because of this, in order to target new devices, developers must implement one-off custom software stacks for managing system resources and scheduling model execution.</p>

<p>The manual optimization of machine learning software is not unique to the domain of bare-metal devices.  In fact, this has been a common theme for developers working with other hardware backends (e.g., GPUs and FPGAs).  TVM has proven resilient to the onslaught of new hardware targets, but until now, it couldn’t grapple with the unique profile of microcontrollers.  To solve the problem in this domain, we’ve extended TVM to feature a microcontroller backend, called µTVM (footnote: pronounced “MicroTVM”).   µTVM facilitates host-driven execution of tensor programs on bare-metal devices and enables automatic optimization of these programs via AutoTVM, TVM’s built-in tensor program optimizer. In the figure below, a bird’s eye view of the µTVM + AutoTVM infrastructure is shown:</p>

<p style="text-align: center"><img src="/images/microtvm/autotvm-infrastructure.png" alt="/images/microtvm/autotvm-infrastructure.png" width="80%" /><br /></p>

<h1 id="lets-see-it-in-action">Let’s see it in action</h1>

<p>Before we talk about what TVM/MicroTVM is or how it works, let’s see a quick example of it in action.</p>

<p style="text-align: center"><img src="/images/microtvm/hardware-connection-diagram.png" alt="/images/microtvm/hardware-connection-diagram.png" width="80%" /><br />
A standard µTVM setup, where the host communicates with the device via JTAG.</p>

<p>Above, we have an <a href="https://www.st.com/en/microcontrollers-microprocessors/stm32f746zg.html">STM32F746ZG board</a>, housing an ARM Cortex-M7 processor, an ideal part for AI on the edge given it’s strong performance in a low power envelope. We use its USB-JTAG port to connect it to our desktop machine.  On the desktop, we run OpenOCD to open a JTAG connection with the device; in turn, OpenOCD allows µTVM to control the M7 processor using a device-agnostic TCP socket.  With this setup in place, we can run a CIFAR-10 classifier using TVM code that looks like this (full script <a href="https://github.com/areusch/microtvm-blogpost-eval/blob/master/python/micro_eval/bin/eval.py">here</a>):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">OPENOCD_SERVER_ADDR</span> <span class="o">=</span> <span class="s">'127.0.0.1'</span>
<span class="n">OPENOCD_SERVER_PORT</span> <span class="o">=</span> <span class="mi">6666</span>
<span class="n">TARGET</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">target</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="s">'c -device=micro_dev'</span><span class="p">)</span>
<span class="n">DEV_CONFIG</span> <span class="o">=</span> <span class="n">stm32f746xx</span><span class="p">.</span><span class="n">default_config</span><span class="p">(</span><span class="n">OPENOCD_SERVER_ADDR</span><span class="p">,</span> <span class="n">OPENOCD_SERVER_PORT</span><span class="p">)</span>

<span class="n">module</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">get_cifar10_cnn</span><span class="p">()</span>
<span class="k">with</span> <span class="n">micro</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">device_config</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
	<span class="n">graph</span><span class="p">,</span> <span class="n">c_module</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="s">'main'</span><span class="p">],</span> <span class="n">target</span><span class="o">=</span><span class="n">TARGET</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
  <span class="n">micro_mod</span> <span class="o">=</span> <span class="n">micro</span><span class="p">.</span><span class="n">create_micro_mod</span><span class="p">(</span><span class="n">c_module</span><span class="p">,</span> <span class="n">DEV_CONFIG</span><span class="p">)</span>
  <span class="n">graph_mod</span> <span class="o">=</span> <span class="n">graph_runtime</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">graph</span><span class="p">,</span> <span class="n">micro_mod</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">tvm</span><span class="p">.</span><span class="n">micro_dev</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
  <span class="n">graph_mod</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data_np</span><span class="p">)</span>
  <span class="n">prediction</span> <span class="o">=</span> <span class="n">CIFAR10_CLASSES</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">graph_mod</span><span class="p">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">asnumpy</span><span class="p">())]</span>
  <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'prediction was </span><span class="si">{</span><span class="n">prediction</span><span class="si">}</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<p>Below are the performance results of MicroTVM, compared with <a href="https://github.com/ARM-software/CMSIS_5/releases/tag/5.6.0">CMSIS-NN version 5.7.0</a> (commit <code class="language-plaintext highlighter-rouge">a65b7c9a</code>), a hand-optimized library of ML kernels.</p>

<p style="text-align: center"><img src="/images/microtvm/post-2020-05-28/cifar10-int-8-cnn.png" alt="/images/microtvm/post-2020-05-28/cifar10-int-8-cnn.png" width="60%" /><br /></p>

<p>As we can see, the out-of-the-box performance isn’t great, but this is where <a href="https://dl.acm.org/doi/10.5555/3327144.3327258">AutoTVM</a> comes to the rescue.  We can write a schedule template for our device, do a round of autotuning, then achieve significantly better results.  To plug in our autotuned results, we only need to replace this line:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">graph</span><span class="p">,</span> <span class="n">c_module</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="s">'main'</span><span class="p">],</span> <span class="n">target</span><span class="o">=</span><span class="n">TARGET</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div>

<p>with these lines:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">TARGET</span><span class="p">,</span> <span class="n">autotvm</span><span class="p">.</span><span class="n">apply_history_best</span><span class="p">(</span><span class="n">TUNING_RESULTS_FILE</span><span class="p">):</span>
  <span class="n">graph</span><span class="p">,</span> <span class="n">c_module</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="s">'main'</span><span class="p">],</span> <span class="n">target</span><span class="o">=</span><span class="n">TARGET</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div>

<p>And our results now look like this:</p>

<p style="text-align: center"><img src="/images/microtvm/post-2020-05-28/autotuned-cifar10-int-8-cnn.png" alt="/images/microtvm/post-2020-05-28/autotuned-cifar10-int-8-cnn.png" width="60%" /><br /></p>

<p>We’ve improved our performance by ~2x, and we’re now much closer to CMSIS-NN. Although the MicroTVM CIFAR10 implementation is competitive in with a similar TFLite/CMSIS-NN model, this work has just begun to take advantage of TVM’s optimization features. There’s room to optimize further by accelerating other operators such as dense/fully-connected and taking advantage of TVM’s model-specific quantization and operator fusion capabilities. TVM with µTVM enables you to play with the best of them.  So how does it work?  What’s going on behind the scenes?  Let’s dive in now.</p>

<h1 id="design">Design</h1>

<p style="text-align: center"><img src="/images/microtvm/post-2020-05-28/memory-layout.png" alt="/images/microtvm/post-2020-05-28/memory-layout.png" width="20%" /><br />
The µTVM Device Memory Layout in RAM</p>

<p>µTVM aims to support the lowest common denominator of devices by minimizing the set of requirements that must be satisfied.  In particular, users need only provide:</p>

<ol>
  <li>a C cross-compiler toolchain for their device</li>
  <li>a method for reading/writing to device memory and executing code on the device</li>
  <li>a specification containing the device’s memory layout and general architectural characteristics</li>
  <li>a code snippet that prepares the device for function execution</li>
</ol>

<p>Most bare-metal devices have support for C and JTAG (a debugging protocol), so (1) and (2) usually come for free!  Furthermore, (3) and (4) are often very small asks.  Below are examples of (3) and (4) for STM32F746-series boards.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">device_config</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">'device_id'</span><span class="p">:</span> <span class="s">'arm.stm32f746xx'</span><span class="p">,</span>        <span class="c1"># unique identifier for the device
</span>    <span class="s">'toolchain_prefix'</span><span class="p">:</span> <span class="s">'arm-none-eabi-'</span><span class="p">,</span>  <span class="c1"># prefix of each binary in the cross-compilation toolchain (e.g., arm-none-eabi-gcc)
</span>    <span class="s">'base_addr'</span><span class="p">:</span> <span class="mh">0x20000000</span><span class="p">,</span>               <span class="c1"># first address of RAM
</span>    <span class="s">'section_sizes'</span><span class="p">:</span> <span class="p">{</span>                     <span class="c1"># dictionary of desired section sizes in bytes
</span>         <span class="s">'text'</span><span class="p">:</span> <span class="mi">18000</span><span class="p">,</span>
         <span class="s">'rodata'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
         <span class="s">'data'</span><span class="p">:</span> <span class="mi">100</span><span class="p">,</span>
         <span class="p">...</span>
    <span class="p">},</span>
    <span class="s">'word_size'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>                        <span class="c1"># device word size
</span>    <span class="s">'thumb_mode'</span><span class="p">:</span> <span class="bp">True</span><span class="p">,</span>                    <span class="c1"># whether to use ARM's thumb ISA
</span>    <span class="s">'comms_method'</span><span class="p">:</span> <span class="s">'openocd'</span><span class="p">,</span>             <span class="c1"># method of communication with the device
</span>    <span class="s">'server_addr'</span><span class="p">:</span> <span class="s">'127.0.0.1'</span><span class="p">,</span>            <span class="c1"># OpenOCD server address (if 'comms_method' is 'openocd')
</span>    <span class="s">'server_port'</span><span class="p">:</span> <span class="mi">6666</span><span class="p">,</span>                   <span class="c1"># OpenOCD server port (if 'comms_method' is 'openocd')
</span><span class="p">}</span>
</code></pre></div></div>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">.</span><span class="n">syntax</span> <span class="n">unified</span>
<span class="p">.</span><span class="n">cpu</span> <span class="n">cortex</span><span class="o">-</span><span class="n">m7</span>
<span class="p">.</span><span class="n">fpu</span> <span class="n">softvfp</span>
<span class="p">.</span><span class="n">thumb</span>

<span class="p">.</span><span class="n">section</span> <span class="p">.</span><span class="n">text</span><span class="p">.</span><span class="n">UTVMInit</span>
<span class="p">.</span><span class="n">type</span> <span class="n">UTVMInit</span><span class="p">,</span> <span class="o">%</span><span class="n">function</span>
<span class="n">UTVMInit</span><span class="o">:</span>
  <span class="cm">/* enable fpu */</span>
  <span class="n">ldr</span> <span class="n">r0</span><span class="p">,</span> <span class="o">=</span><span class="mh">0xE000ED88</span>
  <span class="n">ldr</span> <span class="n">r1</span><span class="p">,</span> <span class="p">[</span><span class="n">r0</span><span class="p">]</span>
  <span class="n">ldr</span> <span class="n">r2</span><span class="p">,</span> <span class="o">=</span><span class="mh">0xF00000</span>
  <span class="n">orr</span> <span class="n">r1</span><span class="p">,</span> <span class="n">r2</span>
  <span class="n">str</span> <span class="n">r1</span><span class="p">,</span> <span class="p">[</span><span class="n">r0</span><span class="p">]</span>
  <span class="n">dsb</span>
  <span class="n">isb</span>
  <span class="cm">/* set stack pointer */</span>
  <span class="n">ldr</span> <span class="n">sp</span><span class="p">,</span> <span class="o">=</span><span class="n">_utvm_stack_pointer_init</span>
  <span class="n">bl</span> <span class="n">UTVMMain</span>
<span class="p">.</span><span class="n">size</span> <span class="n">UTVMInit</span><span class="p">,</span> <span class="p">.</span><span class="o">-</span><span class="n">UTVMInit</span>
</code></pre></div></div>

<p>The µTVM infrastructure and device runtime have been built to only make use of these requirements, and we’re working to lessen these requirements by supporting common open source runtime platforms such as mBED OS to handle the compilation and linking processes.</p>

<h2 id="device-sessions">Device Sessions</h2>

<p>Given the networked nature of microcontroller interaction, we slightly deviate from standard TVM code by introducing the concept of <code class="language-plaintext highlighter-rouge">MicroSession</code>.</p>

<p>Every piece of functionality in µTVM relies on having an open session with the target device.  If you’re familiar with TVM, you may have noticed a line of code that deviates from the norm in our first code snippet—-namely, this one:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">...</span>
<span class="k">with</span> <span class="n">micro</span><span class="p">.</span><span class="n">Session</span><span class="p">(</span><span class="n">device_config</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
	<span class="p">...</span>
</code></pre></div></div>

<p>Every line inside this <code class="language-plaintext highlighter-rouge">with</code> block can call functions in µTVM, with the context being the device specified by <code class="language-plaintext highlighter-rouge">device_config</code>.  This line is doing a number of things under the hood, so let’s unpack it.</p>

<p>First, it initializes a connection with your device, using whichever communication method you specified (usually OpenOCD).  The µTVM device runtime is then cross-compiled, using whichever cross-compiler you specified.  Finally, space for the compiled binary is allocated by the host, and the binary is loaded onto the device using the opened connection.</p>

<p>With the runtime now situated on the device, we’ll naturally want some functions to run through it.</p>

<h2 id="module-loading">Module Loading</h2>

<p>One of the core abstractions in TVM is that of a module.  A module stores a set of related functions for a particular device/runtime target.  Given that microcontrollers don’t normally have operating systems, µTVM needs to do a lot of extra work to maintain this high-level abstraction.  To see what’s going on, we’ll trace through the process of creating and loading a µTVM-compatible module.</p>

<p>Suppose we have a <code class="language-plaintext highlighter-rouge">micro.Session</code> open with our device and a TVM schedule that implements 2D convolution.  If we want to load it onto our microcontroller, we need it to emit C code.  To do so, we just need to set the <code class="language-plaintext highlighter-rouge">target</code> in either <code class="language-plaintext highlighter-rouge">tvm.build</code> or <code class="language-plaintext highlighter-rouge">relay.build</code>.  Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">graph</span><span class="p">,</span> <span class="n">c_module</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">module</span><span class="p">[</span><span class="s">'main'</span><span class="p">],</span> <span class="n">target</span><span class="o">=</span><span class="s">'c -device=micro_dev'</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div>

<p>By setting the target like so, the build process runs through our C code generation backend.  However, the resulting C module still resides on the host machine.  In order to load it onto the device, we run it through one of the core functions in the µTVM infrastructure: <code class="language-plaintext highlighter-rouge">create_micro_mod</code>.  Example:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">micro_mod</span> <span class="o">=</span> <span class="n">micro</span><span class="p">.</span><span class="n">create_micro_mod</span><span class="p">(</span><span class="n">c_module</span><span class="p">,</span> <span class="n">DEV_CONFIG</span><span class="p">)</span>
</code></pre></div></div>

<p>The line above cross-compiles the C source within the module, allocates room for the resulting binary (so it can coexist with the runtime in device memory), then sends each section of the binary to its allocated slot on the device.  Once the module binary is snug in device memory, function pointers within the binary are patched to give the module access to helper functions in the device runtime (e.g., for allocating scratchpads).</p>

<p>Now, with our kernel loaded on the device, we can grab a remote handle to the convolution function like so:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">micro_func</span> <span class="o">=</span> <span class="n">micro_mod</span><span class="p">[</span><span class="s">'conv2d'</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="tensor-loading">Tensor Loading</h2>

<p>If we want to call an operator, we first need some tensors as arguments:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_np</span><span class="p">,</span> <span class="n">kernel_np</span> <span class="o">=</span> <span class="n">get_conv_inputs</span><span class="p">()</span>
<span class="n">ctx</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">micro_dev</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">data_np</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">tvm</span><span class="p">.</span><span class="n">nd</span><span class="p">.</span><span class="n">array</span><span class="p">(</span><span class="n">kernel_np</span><span class="p">,</span> <span class="n">ctx</span><span class="o">=</span><span class="n">ctx</span><span class="p">)</span>
</code></pre></div></div>

<p>Based on its data type (e.g., <code class="language-plaintext highlighter-rouge">int8</code>, <code class="language-plaintext highlighter-rouge">float32</code>, etc.) and shape, each tensor’s size in bytes is calculated, and the host allocates a region of memory on the device’s heap.  The tensor’s data is then loaded into the allocated region.</p>

<h2 id="function-calls">Function Calls</h2>

<p>Operator execution is perhaps the trickiest part of this system.  To simplify its presentation, we’ll first cover strict execution (where operators are executed as soon as they’re called), then lazy execution (where operators are only executed once their results are needed)—-the latter is how the system actually works.</p>

<h3 id="strict-execution">Strict Execution</h3>

<p>When calling a function, both input and output tensors are passed as arguments, in what’s known as destination-passing style:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">conv2D</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">output</span><span class="p">)</span>
</code></pre></div></div>

<p>Given that these tensors are already allocated on the device, we only need to send <em>metadata</em> to the device (device address, shape, and data type), so it knows which of its resident tensors to use.  The runtime representation of a function call includes this metadata, as well as the address of the function being called (shown below).  Before constructing this representation, the metadata needs to be serialized into the arguments section on the device that exists expressly for this purpose.</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cm">/*
 * task struct for uTVM
 */</span>
<span class="k">typedef</span> <span class="k">struct</span> <span class="p">{</span>
  <span class="cm">/* pointer to function to call for this task */</span>
  <span class="kt">int32_t</span> <span class="p">(</span><span class="o">*</span><span class="n">func</span><span class="p">)(</span><span class="kt">void</span><span class="o">*</span><span class="p">,</span> <span class="kt">void</span><span class="o">*</span><span class="p">,</span> <span class="kt">int32_t</span><span class="p">);</span>
  <span class="cm">/* array of argument tensors */</span>
  <span class="n">TVMValue</span><span class="o">*</span> <span class="n">arg_values</span><span class="p">;</span>
  <span class="cm">/* array of datatype codes for each argument */</span>
  <span class="kt">int</span><span class="o">*</span> <span class="n">arg_type_codes</span><span class="p">;</span>
  <span class="cm">/* number of arguments */</span>
  <span class="kt">int32_t</span> <span class="n">num_args</span><span class="p">;</span>
<span class="p">}</span> <span class="n">UTVMTask</span><span class="p">;</span>
</code></pre></div></div>

<p>In the strict setting, there is a single global <code class="language-plaintext highlighter-rouge">UTVMTask</code> instance that we, from the host side, write into.  Once we have written to the task, the runtime has everything it needs to execute the function, and we can begin execution at the runtime’s entry point.  The runtime will perform some lightweight initialization, run our operator, then return control to the host.</p>

<h3 id="lazy-execution">Lazy Execution</h3>

<p>In practice, executing operators as soon as the user requests to becomes prohibitively expensive, as communication overhead begins to dominate.  We can improve the throughput of our system by delaying evaluation until the user wants the results of the call.</p>

<p>From an implementation standpoint, instead of eagerly serializing argument metadata and <code class="language-plaintext highlighter-rouge">UTVMTask</code> data, we now need to accumulate function call metadata on the host side, before flushing it to the device.  The device runtime also needs a few changes: (1) we must now have a global array of <code class="language-plaintext highlighter-rouge">UTVMTask</code> and (2) we need to loop through and execute each task in order.</p>

<h2 id="autotvm-with-microtvm">AutoTVM with MicroTVM</h2>

<p>So far, the runtime we’ve described doesn’t seem very useful for <em>model deployment</em>, since it relies so heavily on a host machine.  This is intentional, and the runtime has in fact been designed for a different goal: <strong>AutoTVM support</strong>.</p>

<p>In general, AutoTVM proposes candidate kernels, runs them on the target backend with random inputs, then uses the timing results to improve its search process.  Given that AutoTVM only cares about single operator executions, we have designed the runtime to be operator-oriented, as opposed to being model-oriented.  In the case of µTVM though, communication with the device will usually dominate the execution time.  Lazy execution allows us to run the same operator many times without returning control to the host, so the communication cost is amortized over each run, and we can get a better idea of the performance profile.</p>

<p>Because AutoTVM requires rapid iteration on large numbers of candidate kernels, µTVM infrastructure only makes use of RAM currently.  However, for a self-hosted runtime, we will surely need to make use of both flash memory and RAM.</p>

<h2 id="the-hosted-graph-runtime">The Hosted Graph Runtime</h2>

<p>Although the hosted runtime was designed for AutoTVM, we can still run full models (as long as they don’t have any control flow).  This functionality comes for free just by using TVM’s graph runtime, but with a µTVM context.  In fact, the only reliance on the host with the graph runtime is for tensor allocation and operator scheduling (which is just a topological sort of the dependence graph).</p>

<h1 id="evaluation">Evaluation</h1>

<p>With this infrastructure in place, we sought to answer the following questions:</p>

<ol>
  <li>Is µTVM truly device-agnostic?</li>
  <li>How much effort is required to experiment with optimizations using µTVM?</li>
</ol>

<p>To evaluate (1), we ran our experiments on two targets:</p>

<ul>
  <li>An <a href="https://www.st.com/en/microcontrollers-microprocessors/stm32f746ng.html">Arm STM32F746NG development board</a>, featuring a Cortex-M7 processor</li>
  <li>The µTVM host emulated device, which creates a memory arena on the host machine that is interfaced with as if it is a bare-metal device.</li>
</ul>

<p>To evaluate (2), we explore optimizations for the Arm board that give the biggest bang for your buck.</p>

<p>As a point of comparison, we pulled a quantized CIFAR-10 CNN from <a href="https://developer.arm.com/solutions/machine-learning-on-arm/developer-material/how-to-guides/image-recognition-on-arm-cortex-m-with-cmsis-nn/single-page">this tutorial by Arm</a>.  In the tutorial, <a href="https://arm-software.github.io/CMSIS_5/NN/html/index.html">CMSIS-NN</a> (a library of highly optimized kernels by Arm experts) is used as the operator library, making this CNN the perfect evaluation target, as we could now directly compare the results of µTVM with CMSIS-NN on the Arm board.</p>

<p style="text-align: center"><img src="/images/microtvm/post-2020-05-28/cifar10-graphical.png" alt="/images/microtvm/post-2020-05-28/cifar10-graphical.png" width="80%" /><br />
Diagram of CIFAR-10 CNN</p>

<h2 id="methodology">Methodology</h2>

<p>In our experiments, we use TVM from HEAD (commit <code class="language-plaintext highlighter-rouge">9fa8341</code>), version 5.7.0 of CMSIS-NN (commit <code class="language-plaintext highlighter-rouge">a65b7c9a</code>), version 1.16.0 of STM32CubeF7, and GCC from Arm’s GNU Tools for Arm Embedded Processors 9-2019-q4-major 9.2.1 toolchain (revision 277599).  The host machine used in our experiments runs Ubuntu Linux 18.04.4 LTS and sports an AMD Ryzen Threadripper 2990WX 32-Core Processor with 62GB of RAM.  All evaluation scripts for this blogpost are contained in <a href="https://github.com/areusch/microtvm-blogpost-eval">this repo</a>.</p>

<h3 id="arm-specific-optimizations">Arm-Specific Optimizations</h3>

<p>With CMSIS-NN, the first convolution maps to their <a href="https://github.com/ARM-software/CMSIS_5/blob/develop/CMSIS/NN/Source/ConvolutionFunctions/arm_convolve_HWC_q7_RGB.c">RGB convolution implementation</a> (specifically for usage in input layers) and the latter two map to their <a href="https://github.com/ARM-software/CMSIS_5/blob/develop/CMSIS/NN/Source/ConvolutionFunctions/arm_convolve_HWC_q7_fast.c">“fast” convolution implementation</a>.  We felt our performance was close enough for the RGB convolution after the earlier generic optimizations, but were left unsatisfied with our fast convolution results.  Luckily, Arm released a <a href="https://arxiv.org/abs/1801.06601">paper</a> describing optimizations used in CMSIS-NN, and we found they are getting massive speedups from SIMD intrinsics.  In the paper, they present a matrix multiplication microkernel that uses SIMD intrinsics (figure below).  While we could add first-class support for the intrinsics in TVM’s code generation facilities—and this is likely the best move in the long run—TVM offers <a href="https://tvm.apache.org/docs/tutorials/language/tensorize.html">tensorization</a> as a “quick-and-dirty” solution to supporting SIMD.</p>

<p style="text-align: center"><img src="/images/microtvm/post-2020-05-28/simd-diagram.png" alt="/images/microtvm/post-2020-05-28/simd-diagram.png" width="80%" /><br />
Diagram from CMSIS-NN paper showing a 2x2 matrix multiplication microkernel</p>

<p>Tensorization works by defining a microkernel that can be inserted into the innermost loop of a TVM operator.  Using this mechanism, adding SIMD support for the Arm board was as simple as defining a microkernel in C (found <a href="https://github.com/apache/incubator-tvm/blob/8d7249688771bb6806595931586d95648036f383/topi/python/topi/arm_cpu/cortex_m7/micro_kernel/gemm.py">here</a>) that mirrored the implementation in their paper.  We defined a schedule that used this microkernel (found <a href="https://github.com/apache/incubator-tvm/blob/8d7249688771bb6806595931586d95648036f383/topi/python/topi/arm_cpu/cortex_m7/conv2d/direct_simd.py">here</a>), autotuned it, then got the “µTVM SIMD tuned” results.</p>

<p>While we were able to use the SIMD microkernel for direct convolution, CMSIS-NN uses what they call “partial im2col” as their implementation strategy, which offers a tradeoff between performance and memory usage.  Instead of manifesting the entire im2col matrix at once, partial im2col generates only a few columns at a time.  Then, with each batch, they can send the matrix to their SIMD matmul function.</p>

<p>Our hypothesis was that, among other optimizations, we could find the optimal batch size via autotuning.  In practice, we found partial im2col to be significantly slower than our direct convolution implementation, so we don’t include it in the rest of our results.</p>

<p>There are certainly other optimizations we could pull from CMSIS-NN to close the gap even further:</p>

<ul>
  <li>Batch expansion of <code class="language-plaintext highlighter-rouge">int8</code> weights into <code class="language-plaintext highlighter-rouge">int16</code>, to cut down on duplicate expansion for SIMD</li>
  <li>Splitting convolution into 3x3 tiles to reduce padding checks</li>
</ul>

<p>But our goal in this blog post is to show the broad strokes of what can be done with µTVM.  Even so, it’s not a competition, because CMSIS-NN (and any other hand-optimized library) can plug directly into TVM using the <a href="https://tvm.apache.org/docs/dev/relay_bring_your_own_codegen.html">Bring Your Own Codegen framework</a>.</p>

<h2 id="end-to-end">End-To-End</h2>

<h3 id="cifar-10">CIFAR-10</h3>

<p>After exploring optimizations for convolution, we set out to measure their effects on end-to-end performance.  For the Arm board, we collected untuned results, results that were tuned <strong>without</strong> any use of SIMD, results that were tuned <strong>with</strong> SIMD, and results using CMSIS-NN.  For the emulated host device, we only collected untuned results and generic tuned results.</p>

<p><a href="https://github.com/areusch/microtvm-blogpost-eval">https://github.com/areusch/microtvm-blogpost-eval</a></p>

<p style="text-align: center"><img src="/images/microtvm/post-2020-05-28/autotuned-cifar10-int-8-cnn.png" alt="/images/microtvm/post-2020-05-28/autotuned-cifar10-int-8-cnn.png" width="60%" /><br />
<code class="language-plaintext highlighter-rouge">int8</code>-quantized CIFAR-10 CNN comparison on an Arm STM32F746NG (re-posted from above)</p>

<p style="text-align: center"><img src="/images/microtvm/post-2020-05-28/autotuned-cifar10-int-8-cnn-x86.png" alt="/images/microtvm/post-2020-05-28/autotuned-cifar10-int-8-cnn-x86.png" width="60%" /><br />
<code class="language-plaintext highlighter-rouge">int8</code>-quantized CIFAR-10 CNN comparison on µTVM’s emulated host device</p>

<p>On the Arm STM32-series board, we were able to improve performance by ~2x compared to the initial untuned operators, and we achieved results much closer to CMSIS-NN.  Additionally, we were able to significantly improve performance on the host emulated device.  Though the x86 <strong><em>numbers</em></strong> don’t mean much, they show we can use the same infrastructure (µTVM) to optimize performance on vastly different architectures.</p>

<p>Stay tuned in the future for more end-to-end benchmarks as we scale this approach out more broadly.</p>

<h1 id="self-hosted-runtime-the-final-frontier">Self-Hosted Runtime: The Final Frontier</h1>

<p style="text-align: center"><img src="/images/microtvm/self-hosted-runtime.png" alt="/images/microtvm/self-hosted-runtime.png" width="80%" /><br /></p>

<p>The envisioned µTVM optimization and deployment pipeline</p>

<p>While end-to-end benchmark results are already obtainable with the current runtime as we demonstrated above, deployment of these models in a standalone capacity is currently still on our roadmap. The gap being that the AutoTVM-oriented runtime currently relies on the host to allocate tensors and to schedule function execution. However, to be useful at the edge, we need a pipeline through µTVM that generates a <strong>single</strong> binary to be run on a bare-metal device. Users will then be able to easily integrate fast ML into their applications by including this binary in their edge application. Each stage of this pipeline is already in place, and now it’s just a matter of gluing it all together, so expect updates from us soon on this front.</p>

<h1 id="conclusion">Conclusion</h1>

<p>MicroTVM for single-kernel optimization is ready <strong>today</strong> and is <em>the</em> choice for that use case.  As we now build out self-hosted deployment support we hope you’re just as excited as we are to make µTVM <em>the</em> choice for model deployment as well. However, this isn’t just a spectator sport - remember: this is all open source!  µTVM is still in its early days, so every individual can have a great deal of impact on its trajectory. Check out the <a href="https://tvm.apache.org/docs/contribute/">TVM contributor’s guide</a> if you’re interested in building with us or jump straight into <a href="https://discuss.tvm.ai/">the TVM forums</a> to discuss ideas first.</p>

<h2 id="acknowledgements">Acknowledgements</h2>

<p>None of this work would have been possible, if not for the following people:</p>

<ul>
  <li><a href="https://tqchen.com/">Tianqi Chen</a>, for guiding the design and for being a fantastic mentor.</li>
  <li><a href="https://homes.cs.washington.edu/~patelp1/">Pratyush Patel</a>, for collaborating on early prototypes of MicroTVM.</li>
  <li><a href="https://octoml.ai/">OctoML</a>, for facilitating the internships where I have been able to go full steam on this project.</li>
  <li><a href="https://homes.cs.washington.edu/~moreau/">Thierry Moreau</a>, for mentoring me during my time at OctoML.</li>
  <li><a href="https://homes.cs.washington.edu/~vegaluis/">Luis Vega</a>, for teaching me the fundamentals of interacting with microcontrollers.</li>
  <li><a href="https://www.linkedin.com/in/themadrasi/?originalSubdomain=uk">Ramana Radhakrishnan</a>, for supplying the Arm hardware used in our experiments and for providing guidance on its usage.</li>
</ul>]]></content><author><name>Logan Weber and Andrew Reusch, OctoML</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Compiling Machine Learning to WASM and WebGPU with Apache TVM</title><link href="/2020/05/14/compiling-machine-learning-to-webassembly-and-webgpu" rel="alternate" type="text/html" title="Compiling Machine Learning to WASM and WebGPU with Apache TVM" /><published>2020-05-14T00:00:00+00:00</published><updated>2020-05-14T00:00:00+00:00</updated><id>/2020/05/14/compiling-machine-learning-to-webassembly-and-webgpu</id><content type="html" xml:base="/2020/05/14/compiling-machine-learning-to-webassembly-and-webgpu"><![CDATA[<p><strong>TLDR</strong></p>

<p>We introduced support for WASM and WebGPU to the Apache TVM deep learning compiler. Our experiments shows that  TVM’s WebGPU backend can get <strong>close to native</strong> <strong>GPU performance</strong> when deploying models to the web.</p>

<p style="text-align: center"><img src="/images/webgpu/webgpu-mobilenet-perf.png" alt="image" width="55%" /><br /></p>

<h2 id="introduction">Introduction</h2>

<p>Computing is one of the pillars of modern machine learning applications. The introduction of the GPU to accelerate deep learning workloads has increased the rate of progress dramatically. Given the growing requirement to deploy machine learning everywhere, the browser becomes a natural place to deploy intelligent applications.</p>

<p>While TensorFlow.js and ONNX.js are existing efforts to bring machine learning to the browser, there still exist non-trivial gaps in performance between the web versions and native ones. One of the many reasons is the lack of standard and performant access to the GPU on the web. WebGL lacks important features such as compute shaders and generic storage buffers that are necessary for high performance deep learning.</p>

<p>WebGPU is the upcoming standard for next generation web graphics which has the possibility to dramatically change this situation. Like the latest generation graphics APIs such as Vulkan and Metal, WebGPU offers first-class compute shader support.</p>

<p>To explore the potential of using WebGPU for machine learning deployment in the browser, we enhanced the deep learning compiler Apache(incubating) TVM to target WASM (for host code that computes the launching parameters and calls into the device launch) and WebGPU (for device execution). Our preliminary results are quite positive — for the first time, we can deploy machine learning applications on the web while still getting near native performance on the GPU.</p>

<h2 id="machine-learning-compiler">Machine Learning Compiler</h2>

<p style="text-align: center"><img src="/images/webgpu/ml-compiler-flow.png" alt="image" width="65%" /><br /></p>

<p>One natural reaction when trying out WebGPU is to write shaders for primitive operators in deep neural networks (matrix multiplication and convolution) and then directly optimize their performance. This is the traditional workflow used  by existing frameworks such as TensorFlow.js.</p>

<p>Instead, we apply a compilation based approach. TVM automatically ingests models from high-level frameworks such as TensorFlow, Keras, PyTorch, MXNet and ONNX and uses a machine learning driven approach to automatically generate low level code, in this case compute shaders in SPIR-V format. The generated code can then be packaged as a deployable module.</p>

<p>One important advantage of the compilation based approach is the reuse of infrastructure. We are able to effortlessly (relative to <a href="https://arxiv.org/abs/1901.05350">other approaches</a>) target the web by reusing the infrastructure for optimizing GPU kernels for native platforms such as CUDA, Metal and OpenCL. If the mapping of the WebGPU API to native APIs is efficient we can expect similar performance with very little work. More importantly, the <a href="https://tvm.apache.org/2018/10/03/auto-opt-all">AutoTVM</a> infrastructure allows us to specialize the compute shaders for specific models, enabling the generation of the best compute shaders for our specific model of interest.</p>

<h2 id="building-a-wasm-and-webgpu-compiler">Building a WASM and WebGPU Compiler</h2>

<p>In order to build a compiler that can target WASM and WebGPU, we need the following elements:</p>

<ul>
  <li>A SPIR-V generator for compute shaders.</li>
  <li>A WASM generator for the host program.</li>
  <li>A runtime to load and execute the generated program.</li>
</ul>

<p>Luckily, TVM already has a SPIR-V target for Vulkan, and uses LLVM for host code generation. So we can just repurpose the two to generate the device and host programs.</p>

<p>The main challenge is the runtime. We need a runtime to load the shader code, and to enable  the host code talk to communicate with the shader correctly. TVM has a minimum C++ based runtime. We build a minimum web runtime library and link it with the generated shader and host driving code, producing a single WASM file. However, this WASM module still contains two unknown dependencies:</p>

<ul>
  <li>The runtime needs to call into system library calls (malloc, stderr).</li>
  <li>The wasm runtime needs to interact with the WebGPU driver (in javascript where the WebGPU API is the first-class citizen).</li>
</ul>

<p>WASI is a standard solution to solve the first problem. While there is not yet a mature WASI on the web, we can use emscripten to generate a WASI-like library (see discussion <a href="https://github.com/emscripten-core/emscripten/issues/11075">here</a>) to provide these system libraries.</p>

<p>We solve the second problem by building a WebGPU runtime inside TVM’s JS runtime, and calling back to these functions from the WASM module when invoking GPU code. Using the <a href="https://tvm.apache.org/docs/dev/runtime.html#packedfunc">PackedFunc</a> mechanism in TVM’s runtime system, we can directly expose high-level runtime primitives by passing JavaScript closures to the WASM interface. This approach keeps most of the runtime code in JavaScript, we could bring more JS code into the WASM runtime as WASI and WASM support matures.</p>

<p style="text-align: center"><img src="/images/webgpu/tvm-wasm-stack.png" alt="image" width="65%" /></p>

<h2 id="performance">Performance</h2>

<p style="text-align: center"><img src="/images/webgpu/webgpu-mobilenet-perf.png" alt="image" width="65%" /></p>

<p>We ran a quick experiment comparing the execution of a full computational graph via TVM’s WebGPU backend and native targets that use native GPU runtimes (Metal and OpenCL). On the MobileNet model, we can find that the WebGPU can get close to matching the performance of Metal. Assuming Chrome WebGPU’s runtime targets Metal instead of OpenCL on the MacOS, we can safely assume there is little to no performance loss when targeting the GPU.</p>

<p>This benchmark excludes the CPU to GPU data copy cost and only benchmarks the GPU execution. Currently the data copy from CPU to GPU can still take 25% of the execution time; however, these costs can further be amortized via approaches like double buffering in a continuous execution setting.</p>

<p>Our reported end-to-end running time of mobilenet is by no means optimal, since we simply reused a tuned programs from GTX 1080 Ti, which is very different from the Intel graphics GPU. We expect further performance boost by using <a href="https://tvm.apache.org/2018/10/03/auto-opt-all">AutoTVM</a> on the target platform of interest.</p>

<h2 id="looking-to-the-future">Looking to the Future</h2>

<p>Our results suggest many interesting opportunities for machine learning on the web. Notably, WebGPU is an API that is still evolving and its implications could go beyond web applications. For example one could target native APIs of WebGPU as it matures and becomes standardized through WASI, enabling standalone WASM applications that make use of WebGPU.</p>

<p>The TVM community is also actively working on a <a href="https://github.com/apache/incubator-tvm/tree/master/rust">Rust based runtime</a> that would enable much more robust WASM support and enable easier interaction with projects like <a href="https://github.com/gfx-rs/wgpu-rs">wgpu</a>, and the <a href="https://rustwasm.github.io/docs/book/">Rust WASM</a> ecosystem. As an open source project, we are looking for contributors who can bring in new ideas and help push the project in these exciting directions.</p>

<p>The proposed approach provides effective machine learning support for most WASM’s application scenarios. The close to native performance could unlock better <a href="https://en.wikipedia.org/wiki/Federated_learning">federated learning</a> capabilities on the browser. The same compiled package should also be able to run on native WASM executors to provide sandbox for the applications.</p>

<h2 id="show-me-the-code">Show me the Code</h2>

<ul>
  <li><a href="https://github.com/tqchen/tvm-webgpu-example">Example project for image classification</a></li>
  <li><a href="https://github.com/apache/incubator-tvm/tree/master/web">Apache TVM on github</a></li>
</ul>

<h2 id="acknowledgement">Acknowledgement</h2>

<p>We would like to thank the emscripten project for providing the WASM compilation infrastructures as well as the JS library support on the web. We would also like to thank the WebGPU community for various helpful discussions. Thanks to Fletcher Haynes for valuable feedbacks to the post.</p>]]></content><author><name>Tianqi Chen and Jared Roesch, OctoML</name></author><summary type="html"><![CDATA[TLDR]]></summary></entry><entry><title type="html">Integrating TVM into PyTorch</title><link href="/2019/05/30/pytorch-frontend" rel="alternate" type="text/html" title="Integrating TVM into PyTorch" /><published>2019-05-30T00:00:00+00:00</published><updated>2019-05-30T00:00:00+00:00</updated><id>/2019/05/30/pytorch-frontend</id><content type="html" xml:base="/2019/05/30/pytorch-frontend"><![CDATA[<p>As TVM continuously demonstrates improvements to the efficiency of deep learning execution,
it has become clear that PyTorch stands to benefit from directly leveraging the compiler stack.
A major tenet of PyTorch is providing seamless and robust integrations that don’t get in the user’s way.
To that end, PyTorch now has an official TVM-based backend, <a href="https://github.com/pytorch/tvm">torch_tvm</a>.</p>

<p>Usage is simple:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch_tvm
torch_tvm.enable()
</code></pre></div></div>

<p>That’s it!  PyTorch will then attempt to convert all operators it can to known Relay operators during its JIT compilation process.</p>

<h3 id="background">Background</h3>

<p>Unlike many other ML frameworks, PyTorch exposes an eager-execution programming interface.  This style of programming avoids graph-based meta-programming and focuses on the direct manipulation of n-dimensional arrays (tensors) in a Pythonic way.  As such, the framework was initially well suited for the experimentation and development of models, but not for automatic performance optimization or deployment.  To leverage optimizing compiler techniques, some large changes were recently introduced to PyTorch to solve this problem.</p>

<p><img src="https://i.imgur.com/4XVHbJE.png" alt="TVM Integration" /></p>

<p>PyTorch 1.0 introduced PyTorch IR, a PyTorch-specific intermediate representation for models similar to Relay.  PyTorch programs can be converted into the IR via model tracing, which records the execution of a model or TorchScript, a subset of Python.  The new TVM backend lowers PyTorch IR to Relay, and is able to transparently improve PyTorch performance with little user involvement.</p>

<h3 id="integration-and-results">Integration and Results</h3>

<p>To support Relay, two features were added to the PyTorch JIT: custom transformation passes and custom subgraph interpreters.</p>

<p>When <code class="language-plaintext highlighter-rouge">torch_tvm</code> is enabled, subgraphs of PyTorch IR that can be converted to Relay <code class="language-plaintext highlighter-rouge">Expr</code>s will be marked as Relay-compatible.  Since PyTorch IR does not always contain shape information, none of the subgraphs can be compiled in a useful way before invocation.</p>

<p>During user invocation, the PyTorch JIT runtime will determine input shape information and compile the previously marked subgraphs with the new Relay C++ <a href="https://github.com/pytorch/tvm/blob/main/torch_tvm/compiler.cpp#L226-L246">build system</a>.  The compilation is cached based on input shapes for subsequent runs.  More details can be found in the <a href="https://github.com/pytorch/tvm/blob/main/README.md">README</a>.</p>

<p><code class="language-plaintext highlighter-rouge">torch_tvm</code> has a continuous benchmark system set up, which is monitoring the performance of ResNet18 on CPU.
Out of the box TVM provides over two times the performance of the default PyTorch JIT backend for various ResNet models.
Below is a graph that details the iterations per second achieved with 16 threads on an AWS c5n.4xlarge instance (larger is better):</p>

<p style="text-align: center"><img src="https://i.imgur.com/KfJ7oas.png" alt="bench" width="90%" /></p>

<p>These results are quite encouraging, and the project will continue to focus on improving CPU inference speed across more models.</p>

<h3 id="future-work">Future work</h3>

<p>Right now the PyTorch JIT does a lot of work to find pure functional subsets of its IR to feed to Relay.  This avoids the need to map aliasing and control flow information to Relay, but is not necessary.  Mapping more of the PyTorch IR to Relay may yield performance wins and is a goal of the project.  PyTorch IR is rapidly changing as it is being developed, so this must be done carefully.</p>

<p>More work will be done to ensure the hand off between PyTorch and TVM code is efficient.  This includes unifying the threading model, allocators and reducing the overhead associated with copying inputs into TVM.</p>

<h3 id="tutorial">Tutorial</h3>

<p>If you have an already written PyTorch model, the easiest way to get started comes from using <code class="language-plaintext highlighter-rouge">torch.jit.trace</code> as follows</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import torch_tvm
from your_model import model, inputs

torch_tvm.enable(opt_level=3)

iters = 100
warmup = 10

# Ensure your model is in eval mode and also turn off gradients.
with torch.no_grad():
  # Use tuned parameters for better performance.
  with autotvm.apply_history_best("test/autotvm_tuning.log"):
    # This is where all the compilation happens.
    trace_tvm = torch.jit.trace(model, inputs)
    
    # Warmup
    for _ in range(warmup):
      _ = trace_tvm(*inputs)

    # Benchmark
    start = time.time()
    for _ in range(iters):
      _ = trace_tvm(*inputs)
    tvm_time = time.time() - start
    
    print("Took {}s to run {} iters".format(tvm_time, iters))
</code></pre></div></div>

<p>Much of this code comes from <a href="https://github.com/pytorch/tvm/blob/main/test/benchmarks.py">benchmarks.py</a>.  Note that tuned parameters for AVX2 LLVM compilation is in the <code class="language-plaintext highlighter-rouge">test/</code> folder of the repo.</p>

<p>If you are more comfortable using Relay directly, it is possible to simply extract the expression directly from a
PyTorch function either via (implicit) tracing or TorchScript:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def add(a, b, c):
    return a + b + c

# via tracing
relay_graph = torch_tvm.to_relay(add, inputs)

@torch.jit.script
def mul(a, b, c):
    return a * b * c

# via script
relay_graph = torch_tvm.to_relay(mul, inputs)
</code></pre></div></div>]]></content><author><name>Bram Wasti</name></author><summary type="html"><![CDATA[As TVM continuously demonstrates improvements to the efficiency of deep learning execution, it has become clear that PyTorch stands to benefit from directly leveraging the compiler stack. A major tenet of PyTorch is providing seamless and robust integrations that don’t get in the user’s way. To that end, PyTorch now has an official TVM-based backend, torch_tvm.]]></summary></entry><entry><title type="html">Automating Optimization of Quantized Deep Learning Models on CUDA</title><link href="/2019/04/29/opt-cuda-quantized" rel="alternate" type="text/html" title="Automating Optimization of Quantized Deep Learning Models on CUDA" /><published>2019-04-29T16:00:00+00:00</published><updated>2019-04-29T16:00:00+00:00</updated><id>/2019/04/29/opt-cuda-quantized</id><content type="html" xml:base="/2019/04/29/opt-cuda-quantized"><![CDATA[<p>Deep learning has been successfully applied to a variety of tasks.
On real-time scenarios such as inference on autonomous vehicles, the inference speed of the model is critical.
Network quantization is an effective approach to accelerating deep learning models.
In quantized models, both data and model parameters are represented with low precision data types such as <code class="language-plaintext highlighter-rouge">int8</code> and <code class="language-plaintext highlighter-rouge">float16</code>.
The lowered data bandwidth reduces the inference time and memory/storage requirements, as well as the power consumption.
Meanwhile, under proper quantization schemes, we can minimize the accuracy drops of the quantized models.
Therefore, quantized models are of particular interests of researchers and developers as it makes large models suitable to deploy on diverse devices, such as GPU, CPU and mobile devices.</p>

<p>Previously, quantized operators are usually optimized with handcrafted microkernels for different workloads, or rely on blackbox proprietary solutions such as cuDNN and TensorRT.
Writing a high-performance microkernel in assembly can be very challenging and usually requires heavy engineering effort.
Besides, it is difficult to adapt these ad-hoc microkernels to emerging workloads and new devices.</p>

<p style="text-align: center"><img src="/images/cuda-quantized/benchmark.svg" alt="image" width="100%" /></p>
<center> Figure 1. Inference time of different models on TVM, TensorRT, and MXNet </center>
<p></p>

<p>TVM solves this challenge with a full stack compiler and a machine-learning-based optimizer to automatically generate computing kernels.
TVM can generate efficient kernels via automatic search in a human-designed search space.
In standard workloads such as VGG and ResNet, TVM achieves competitive performance compared with other state-of-the-art frameworks. 
In emerging models such as ResNeXt and Deformable ConvNets, the automatic optimization makes it easy for TVM to adapt to these new workloads and achieve a significant performance boost.</p>

<p>In this post, we show how to use TVM to automatically optimize of quantized deep learning models on CUDA.</p>

<h1 id="expressing-quantized-cuda-kernels-in-tvm">Expressing Quantized CUDA Kernels in TVM</h1>
<h2 id="leveraging-tensor-intrinsics-via-tensorization">Leveraging Tensor Intrinsics via Tensorization</h2>
<p>Many platforms provide architecture-specific instructions for special computation patterns, for example, the SIMD instructions on x86, and the <code class="language-plaintext highlighter-rouge">dp4a</code> and <code class="language-plaintext highlighter-rouge">hfma</code> instructions on CUDA.
These intrinsic instructions are highly optimized for specific devices.
By leveraging hardware intrinsics, we can achieve a significant performance boost for quantized operators.</p>

<p>Currently, <a href="https://devblogs.nvidia.com/mixed-precision-programming-cuda-8/">dp4a</a> has been extensively used in TVM int8 operators on CUDA.
<code class="language-plaintext highlighter-rouge">dp4a</code> is a CUDA intrinsic on Compute Capability 6.1 devices.
It is a mixed-precision instruction that provides the efficient computation of the dot product between two 4-element 8-bit integer vectors and accumulates the result in 32-bit format.
Using <code class="language-plaintext highlighter-rouge">dp4a</code>, we can implement a dot product between 8-bit integer vectors with number of elements evenly divisible by four.
With an efficient dot product operator, we can implement high-level operators such as 2d convolution and dense layers as these operators are commonly backed by dot products.</p>

<p>To illustrate, in 2d convolution we accumulate along the channel, the width, and the height axis of the kernel.
This is a typical use case of <code class="language-plaintext highlighter-rouge">dp4a</code>.
TVM uses tensorization to support calling external intrinsics.
We do not need to modify the original computation declaration; we use the schedule primitive <code class="language-plaintext highlighter-rouge">tensorize</code> to replace the accumulation with <code class="language-plaintext highlighter-rouge">dp4a</code> tensor intrinsic.
More details of tensorization can be found in the <a href="https://tvm.apache.org/docs//tutorials/language/tensorize.html">tutorial</a>.</p>

<h2 id="data-layout-rearrangement">Data Layout Rearrangement</h2>
<p>One of the challenges in tensorization is that we may need to design special computation logic to adapt to the requirement of tensor intrinsics.
Although it is natural to accumulate along the inner axis of the tensor in the dense operator, <code class="language-plaintext highlighter-rouge">conv2d</code> can be more challenging.
In <code class="language-plaintext highlighter-rouge">conv2d</code> we expect to take a slice in the channel dimension as the input of <code class="language-plaintext highlighter-rouge">dp4a</code> because the number of channels is typically multiple of 4 (otherwise we fall back to original <code class="language-plaintext highlighter-rouge">conv2d</code> in NCHW layout).
Meanwhile, to achieve memory locality, we would like to reduce along the innermost axis first.
Taking these factors into account, we use a custom data layout to address this challenge.</p>

<p>In CUDA int8 2d convolution, we empirically choose <code class="language-plaintext highlighter-rouge">NCHW4c</code> as data layout and <code class="language-plaintext highlighter-rouge">OIHW4o4i</code> as weight layout.
The templates can also be easily generalized to <code class="language-plaintext highlighter-rouge">NCHW[x]c</code> and <code class="language-plaintext highlighter-rouge">OIHW[x]o[x]i</code>, where x is an arbitrary positive integer divisible by four.
In the data layout we choose, slices of channels are in the packed innermost dimension.
Likewise, we pack slices in both the input and output channel dimensions of the weight so that the output has a consistent data layout with the input, which prevents redundant layout transformations between layers.</p>

<p>We show the computation of one element of the output of the 2d convolution in Figure 2.
The element in each position of the super dimension (the outer dimension of the blocked layout which contains packed elements) NCHW and OIHW is the packed input and kernel, respectively.
Each column of the packed kernel comes from a different filter.
We calculate the dot product between the packed input and each row in the packed kernel using <code class="language-plaintext highlighter-rouge">dp4a</code>, and accumulate the result to the output tensor.</p>

<p style="text-align: center"><img src="/images/cuda-quantized/conv2d.png" alt="image" width="60%" /></p>
<div>
Figure 2. 2D convolution with data layout in NCHW4c and weight layout in OIHW4o4i.
<b>Left</b>: The input tensor in NCHW4c layout. One moving filter of the kernel is colored in blue. One element of the input and kernel is colored in grey. 
<b>Mid</b>: The packed input and kernel in the grey block.
<b>Right</b>: The output in NCHW4c layout. Inside the one element depicted, there are four packed elements in channel sub-dimension.
</div>
<p></p>

<p>After we have specified the layout of convolution layers, other operators such as <code class="language-plaintext highlighter-rouge">add</code> and activations can automatically adapt to the chosen layout during the <a href="https://github.com/apache/incubator-tvm/blob/main/src/relay/pass/alter_op_layout.cc">AlterOpLayout</a> pass in Relay.
The layout transformation of the weight can be precomputed offline. Therefore, we can run the whole model in the same layout without extra overhead.</p>

<h2 id="designing-search-space-for-automatic-optimization">Designing Search Space for Automatic Optimization</h2>
<p>The key to achieving good performance in our quantized operators is to integrate with machine-learning-based automatic optimization. One question is how to design an effective schedule search space.
An effective schedule template means that we can obtain good performance in a reasonable number of iterations in automatic tuning.
Generally speaking, we strive to define a flexible template to cover different configurations in the search space.
On the other hand, we also take advantage of the prior knowledge in performance optimization.
For example, as caching data in the shared memory is a common practice in CUDA programming, we utilize shared memory, but we use machine learning to choose the best tile size.
We also do some manual tiling such as splitting axes by 4 or 16 to facilitate vectorized memory access.</p>

<p>In quantized 2d convolution, we design a search space that includes a set of tunable options, such as the tile size, the axes to fuse, configurations of loop unrolling and double buffering.
The templates of quantized <code class="language-plaintext highlighter-rouge">conv2d</code> and <code class="language-plaintext highlighter-rouge">dense</code> on CUDA are registered under template key <code class="language-plaintext highlighter-rouge">int8</code>.
During automatic tuning, we can create tuning tasks for these quantized operators by setting the <code class="language-plaintext highlighter-rouge">template_key</code> argument.
Details of how to launch automatic optimization can be found in the <a href="https://tvm.apache.org/docs//tutorials/autotvm/tune_relay_cuda.html">AutoTVM tutorial</a>.</p>

<h1 id="general-workflow">General Workflow</h1>

<p style="text-align: center"><img src="/images/cuda-quantized/workflow.png" alt="image" width="60%" /></p>
<center> Figure 3. Workflow of running quantized models </center>
<p></p>

<p>TVM provides an easy workflow to quantize trained models from other frameworks, automatically optimize operators (with AutoTVM), and deploy to different devices.</p>

<p>First, we use the Relay frontend to import existing models. Here we use an MXNet model with <code class="language-plaintext highlighter-rouge">(1, 3, 224, 224)</code> input shape as an example.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">sym</span><span class="p">,</span> <span class="n">arg_params</span><span class="p">,</span> <span class="n">aux_params</span> <span class="o">=</span> <span class="n">mxnet</span><span class="p">.</span><span class="n">model</span><span class="p">.</span><span class="n">load_checkpoint</span><span class="p">(</span><span class="n">model_path</span><span class="p">,</span> <span class="n">epoch</span><span class="p">)</span>
<span class="n">net</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">from_mxnet</span><span class="p">(</span><span class="n">sym</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">{</span><span class="s">'data'</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)},</span> <span class="n">arg_params</span><span class="o">=</span><span class="n">arg_params</span><span class="p">,</span> <span class="n">aux_params</span><span class="o">=</span><span class="n">aux_params</span><span class="p">)</span>
</code></pre></div></div>

<p>Next, we use the relay quantization API to convert it to a quantized model.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">quantize</span><span class="p">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</code></pre></div></div>

<p>Then, we use AutoTVM to extract tuning tasks for the operators in the model and perform automatic optimization. The <a href="https://tvm.apache.org/docs//tutorials/autotvm/tune_relay_cuda.html">AutoTVM tutorial</a> provides an example for this.</p>

<p>Finally, we build the model and run inference in the quantized mode.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">relay</span><span class="p">.</span><span class="n">build_config</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">graph</span><span class="p">,</span> <span class="n">lib</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">relay</span><span class="p">.</span><span class="n">build</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
</code></pre></div></div>
<p>The result of <code class="language-plaintext highlighter-rouge">relay.build</code> is a deployable library.
We can either run inference <a href="https://tvm.apache.org/docs//tutorials/frontend/from_mxnet.html#execute-the-portable-graph-on-tvm">on the GPU</a> directly or deploy <a href="https://tvm.apache.org/docs//tutorials/frontend/deploy_model_on_rasp.html#deploy-the-model-remotely-by-rpc">on the remote devices</a> via RPC.</p>

<h1 id="benchmark">Benchmark</h1>
<p>To verify the performance of the quantized operators in TVM, we benchmark the performance of several popular network models including VGG-19, ResNet-50 and Inception V3.
We also benchmark on DRN-C-26, ResNeXt-50, and DCN-ResNet-101 from <a href="https://github.com/msracver/Deformable-ConvNets">Deformable ConvNets</a> to show the performance of emerging models, which contains less conventional operators such as dilated convolutions, group convolutions and deformable convolutions.
We choose NVIDIA TensorRT as our baseline.
The result of MXNet 1.4 + cuDNN 7.3 in float32 mode is reported to show the speed up of quantization.
The experiments are conducted on NVIDIA GTX 1080.
We report the inference time per image when running in batch size = 1 and 16.</p>

<p>As shown in the Figure 1, TVM achieves up to 8x speedup using quantization.
In standard CNN models such as VGG and ResNet, TVM achieves parity with the state-of-the-art results from TensorRT.</p>

<p>When benchmarking emerging models, TVM achieves impressive results.
We obtain significant performance gains on ResNeXt and DCN-ResNet-101.
Results of DCN-ResNet-101 of TensorRT are not available because there is no official implementation of the deformable convolution.
We show that automatic optimization in TVM makes it easy and flexible to support and optimize emerging workloads.</p>

<h1 id="show-me-the-code">Show Me the Code</h1>
<ul>
  <li><a href="https://github.com/vinx13/tvm-cuda-int8-benchmark">Benchmark</a></li>
  <li><a href="https://github.com/apache/incubator-tvm/blob/main/topi/python/topi/cuda/conv2d_int8.py">CUDA int8 conv2d</a></li>
  <li><a href="https://github.com/apache/incubator-tvm/blob/main/topi/python/topi/cuda/group_conv2d_nchw.py">CUDA int8 group conv2d</a></li>
  <li><a href="https://github.com/apache/incubator-tvm/blob/main/topi/python/topi/cuda/dense.py">CUDA int8 dense</a></li>
  <li><a href="https://github.com/apache/incubator-tvm/blob/main/topi/python/topi/cuda/tensor_intrin.py">Tensor intrinsics declaration</a></li>
</ul>

<h1 id="bio--acknowledgement">Bio &amp; Acknowledgement</h1>
<p><a href="https://wuwei.io/">Wuwei Lin</a> is an undergraduate student at SJTU. He is currently an intern at TuSimple. The author has many thanks to <a href="https://homes.cs.washington.edu/~tqchen/">Tianqi Chen</a> and <a href="https://homes.cs.washington.edu/~eqy/">Eddie Yan</a> for their reviews.</p>]]></content><author><name>Wuwei Lin</name></author><summary type="html"><![CDATA[Deep learning has been successfully applied to a variety of tasks. On real-time scenarios such as inference on autonomous vehicles, the inference speed of the model is critical. Network quantization is an effective approach to accelerating deep learning models. In quantized models, both data and model parameters are represented with low precision data types such as int8 and float16. The lowered data bandwidth reduces the inference time and memory/storage requirements, as well as the power consumption. Meanwhile, under proper quantization schemes, we can minimize the accuracy drops of the quantized models. Therefore, quantized models are of particular interests of researchers and developers as it makes large models suitable to deploy on diverse devices, such as GPU, CPU and mobile devices.]]></summary></entry><entry><title type="html">TVM Deep Learning Compiler Joins Apache Software Foundation</title><link href="/2019/03/18/tvm-apache-announcement" rel="alternate" type="text/html" title="TVM Deep Learning Compiler Joins Apache Software Foundation" /><published>2019-03-18T00:00:00+00:00</published><updated>2019-03-18T00:00:00+00:00</updated><id>/2019/03/18/tvm-apache-announcement</id><content type="html" xml:base="/2019/03/18/tvm-apache-announcement"><![CDATA[<p>There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms – such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) – requires significant manual effort.</p>

<p>TVM is an open source deep learning compiler stack that closes the gap between the productivity-focused deep learning frameworks, and the performance- or efficiency-oriented hardware backends. Today, we are glad to announce that the TVM community has decided to move on to Apache incubator, and becomes an Apache(incubating) project.</p>

<p style="text-align: center"><img src="/images/main/tvm-stack.png" alt="image" width="70%" /></p>

<p>TVM stack began as a research project at the <a href="https://sampl.cs.washington.edu/">SAMPL group</a> of Paul G. Allen School of Computer Science &amp; Engineering, University of Washington. The project uses the loop-level IR and several optimizations from the <a href="http://halide-lang.org/">Halide project</a>, in addition to <a href="https://tvm.apache.org/about">a full deep learning compiler stack</a> to support machine learning workloads for diverse hardware backends.</p>

<p>Since its introduction, the project was driven by an open source community involving multiple industry and academic institutions. Currently, the TVM stack includes a high-level differentiable programming IR for high-level optimization, a machine learning driven program optimizer and VTA – a fully open sourced deep learning accelerator. The community brings innovations from machine learning, compiler systems, programming languages, and computer architecture to build a full-stack open source deep learning compiler system. The project has been used in production in <a href="https://sampl.cs.washington.edu/tvmconf/#about-tvmconf">several major companies</a>.</p>

<p>Besides the technical innovations, the community adopts an open, welcoming and neutral policy. The project is run by committers who are elected purely based on their merit of the contributions to the project. Besides the contributors from UW SAMPL, the community now has nearly 200 contributors that come from Amazon Web Services (AWS), Qualcomm, Facebook, Google, Huawei, AMD, Microsoft, Cornell University, University of California, Berkeley, and more.        The community successfully organized the first developer conference last December which attracted more than 180 attendees from all around the world. Moving forward to the Apache, we will continue to exercise this principle in an effort to bring deep learning compilation to everyone.</p>

<p>We would like to take this chance to thank the Allen School for supporting the SAMPL team that gave birth to the TVM project. We would also like to thank the Halide project which provided the basis for TVM’s loop-level IR and initial code generation. We would like to thank our Apache incubator mentors for introducing the project to Apache and providing useful guidance. Finally, we would like to thank the TVM community and all of the organizations, as listed above, that supported the developers of TVM.</p>

<p>See also the <a href="https://news.cs.washington.edu/2019/03/18/allen-schools-tvm-deep-learning-compiler-framework-transitions-to-apache/">Allen School news about the transition here</a>, <a href="https://sampl.cs.washington.edu/tvmconf/#about-tvmconf">TVM conference program slides and recordings</a>, and <a href="https://tvm.apache.org/docs//contribute/community.html">our community guideline here</a>. Follow us on Twitter: <a href="https://twitter.com/ApacheTVM">@ApacheTVM</a>.</p>]]></content><author><name>TVM Community</name></author><summary type="html"><![CDATA[There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms – such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) – requires significant manual effort.]]></summary></entry></feed>