
.. DO NOT EDIT. THIS FILE WAS AUTOMATICALLY GENERATED BY
.. TVM'S MONKEY-PATCHED VERSION OF SPHINX-GALLERY. TO MAKE
.. CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "how_to/work_with_schedules/schedule_primitives.py"

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        This tutorial can be used interactively with Google Colab! You can also click
        :ref:`here <sphx_glr_download_how_to_work_with_schedules_schedule_primitives.py>` to run the Jupyter notebook locally.

        .. image:: https://raw.githubusercontent.com/tlc-pack/web-data/main/images/utilities/colab_button.svg
            :align: center
            :target: https://colab.research.google.com/github/apache/tvm-site/blob/asf-site/docs/_downloads/b78f1a6e1b2c2fb073a791dc258a1d7d/schedule_primitives.ipynb
            :width: 300px

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_how_to_work_with_schedules_schedule_primitives.py:


.. _schedule_primitives:

Schedule Primitives in TVM
==========================
**Author**: `Ziheng Jiang <https://github.com/ZihengJiang>`_

TVM is a domain specific language for efficient kernel construction.

In this tutorial, we will show you how to schedule the computation by
various primitives provided by TVM.

.. GENERATED FROM PYTHON SOURCE LINES 29-36

.. code-block:: default

    from __future__ import absolute_import, print_function


    import tvm
    from tvm import te
    import numpy as np








.. GENERATED FROM PYTHON SOURCE LINES 37-45

There often exist several methods to compute the same result,
however, different methods will result in different locality and
performance. So TVM asks user to provide how to execute the
computation called **Schedule**.

A **Schedule** is a set of transformation of computation that
transforms the loop of computations in the program.


.. GENERATED FROM PYTHON SOURCE LINES 46-51

.. code-block:: default


    # declare some variables for use later
    n = te.var("n")
    m = te.var("m")








.. GENERATED FROM PYTHON SOURCE LINES 52-54

A schedule can be created from a list of ops, by default the
schedule computes tensor in a serial manner in a row-major order.

.. GENERATED FROM PYTHON SOURCE LINES 54-67

.. code-block:: default


    # declare a matrix element-wise multiply
    A = te.placeholder((m, n), name="A")
    B = te.placeholder((m, n), name="B")
    C = te.compute((m, n), lambda i, j: A[i, j] * B[i, j], name="C")

    s = te.create_schedule([C.op])
    # lower will transform the computation from definition to the real
    # callable function. With argument `simple_mode=True`, it will
    # return you a readable C like statement, we use it here to print the
    # schedule result.
    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle, C: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m, n = T.int32(), T.int32()
            A_1 = T.match_buffer(A, (m, n), strides=("stride", "stride"), buffer_type="auto")
            B_1 = T.match_buffer(B, (m, n), strides=("stride", "stride"), buffer_type="auto")
            C_1 = T.match_buffer(C, (m, n), strides=("stride", "stride"), buffer_type="auto")
            for i, j in T.grid(m, n):
                C_2 = T.Buffer((C_1.strides[0] * m,), data=C_1.data, buffer_type="auto")
                A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
                C_2[i * C_1.strides[0] + j * C_1.strides[1]] = A_2[i * A_1.strides[0] + j * A_1.strides[1]] * B_2[i * B_1.strides[0] + j * B_1.strides[1]]




.. GENERATED FROM PYTHON SOURCE LINES 68-71

One schedule is composed by multiple stages, and one
**Stage** represents schedule for one operation. We provide various
methods to schedule every stage.

.. GENERATED FROM PYTHON SOURCE LINES 73-77

split
-----
:code:`split` can split a specified axis into two axes by
:code:`factor`.

.. GENERATED FROM PYTHON SOURCE LINES 77-84

.. code-block:: default

    A = te.placeholder((m,), name="A")
    B = te.compute((m,), lambda i: A[i] * 2, name="B")

    s = te.create_schedule(B.op)
    xo, xi = s[B].split(B.op.axis[0], factor=32)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m = T.int32()
            A_1 = T.match_buffer(A, (m,), strides=("stride",), buffer_type="auto")
            B_1 = T.match_buffer(B, (m,), strides=("stride",), buffer_type="auto")
            for i_outer, i_inner in T.grid((m + 31) // 32, 32):
                if T.likely(i_outer * 32 + i_inner < m):
                    B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
                    A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                    cse_var_1: T.int32 = i_outer * 32 + i_inner
                    B_2[cse_var_1 * B_1.strides[0]] = A_2[cse_var_1 * A_1.strides[0]] * T.float32(2)




.. GENERATED FROM PYTHON SOURCE LINES 85-87

You can also split a axis by :code:`nparts`, which splits the axis
contrary with :code:`factor`.

.. GENERATED FROM PYTHON SOURCE LINES 87-94

.. code-block:: default

    A = te.placeholder((m,), name="A")
    B = te.compute((m,), lambda i: A[i], name="B")

    s = te.create_schedule(B.op)
    bx, tx = s[B].split(B.op.axis[0], nparts=32)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m = T.int32()
            A_1 = T.match_buffer(A, (m,), strides=("stride",), buffer_type="auto")
            B_1 = T.match_buffer(B, (m,), strides=("stride",), buffer_type="auto")
            for i_outer, i_inner in T.grid(32, (m + 31) // 32):
                if T.likely(i_inner + i_outer * ((m + 31) // 32) < m):
                    B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
                    A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                    B_2[(i_inner + i_outer * ((m + 31) // 32)) * B_1.strides[0]] = A_2[(i_inner + i_outer * ((m + 31) // 32)) * A_1.strides[0]]




.. GENERATED FROM PYTHON SOURCE LINES 95-99

tile
----
:code:`tile` help you execute the computation tile by tile over two
axes.

.. GENERATED FROM PYTHON SOURCE LINES 99-106

.. code-block:: default

    A = te.placeholder((m, n), name="A")
    B = te.compute((m, n), lambda i, j: A[i, j], name="B")

    s = te.create_schedule(B.op)
    xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m, n = T.int32(), T.int32()
            A_1 = T.match_buffer(A, (m, n), strides=("stride", "stride"), buffer_type="auto")
            B_1 = T.match_buffer(B, (m, n), strides=("stride", "stride"), buffer_type="auto")
            for i_outer, j_outer, i_inner in T.grid((m + 9) // 10, (n + 4) // 5, 10):
                if T.likely(i_outer * 10 + i_inner < m):
                    for j_inner in range(5):
                        if T.likely(j_outer * 5 + j_inner < n):
                            cse_var_2: T.int32 = j_outer * 5 + j_inner
                            cse_var_1: T.int32 = i_outer * 10 + i_inner
                            B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
                            A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                            B_2[cse_var_1 * B_1.strides[0] + cse_var_2 * B_1.strides[1]] = A_2[cse_var_1 * A_1.strides[0] + cse_var_2 * A_1.strides[1]]




.. GENERATED FROM PYTHON SOURCE LINES 107-110

fuse
----
:code:`fuse` can fuse two consecutive axes of one computation.

.. GENERATED FROM PYTHON SOURCE LINES 110-120

.. code-block:: default

    A = te.placeholder((m, n), name="A")
    B = te.compute((m, n), lambda i, j: A[i, j], name="B")

    s = te.create_schedule(B.op)
    # tile to four axes first: (i.outer, j.outer, i.inner, j.inner)
    xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)
    # then fuse (i.inner, j.inner) into one axis: (i.inner.j.inner.fused)
    fused = s[B].fuse(xi, yi)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m, n = T.int32(), T.int32()
            A_1 = T.match_buffer(A, (m, n), strides=("stride", "stride"), buffer_type="auto")
            B_1 = T.match_buffer(B, (m, n), strides=("stride", "stride"), buffer_type="auto")
            for i_outer, j_outer, i_inner_j_inner_fused in T.grid((m + 9) // 10, (n + 4) // 5, 50):
                if T.likely(i_outer * 10 + i_inner_j_inner_fused // 5 < m):
                    if T.likely(j_outer * 5 + i_inner_j_inner_fused % 5 < n):
                        cse_var_2: T.int32 = j_outer * 5 + i_inner_j_inner_fused % 5
                        cse_var_1: T.int32 = i_outer * 10 + i_inner_j_inner_fused // 5
                        B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
                        A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                        B_2[cse_var_1 * B_1.strides[0] + cse_var_2 * B_1.strides[1]] = A_2[cse_var_1 * A_1.strides[0] + cse_var_2 * A_1.strides[1]]




.. GENERATED FROM PYTHON SOURCE LINES 121-124

reorder
-------
:code:`reorder` can reorder the axes in the specified order.

.. GENERATED FROM PYTHON SOURCE LINES 124-134

.. code-block:: default

    A = te.placeholder((m, n), name="A")
    B = te.compute((m, n), lambda i, j: A[i, j], name="B")

    s = te.create_schedule(B.op)
    # tile to four axes first: (i.outer, j.outer, i.inner, j.inner)
    xo, yo, xi, yi = s[B].tile(B.op.axis[0], B.op.axis[1], x_factor=10, y_factor=5)
    # then reorder the axes: (i.inner, j.outer, i.outer, j.inner)
    s[B].reorder(xi, yo, xo, yi)
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m, n = T.int32(), T.int32()
            A_1 = T.match_buffer(A, (m, n), strides=("stride", "stride"), buffer_type="auto")
            B_1 = T.match_buffer(B, (m, n), strides=("stride", "stride"), buffer_type="auto")
            for i_inner, j_outer, i_outer in T.grid(10, (n + 4) // 5, (m + 9) // 10):
                if T.likely(i_outer * 10 + i_inner < m):
                    for j_inner in range(5):
                        if T.likely(j_outer * 5 + j_inner < n):
                            cse_var_2: T.int32 = j_outer * 5 + j_inner
                            cse_var_1: T.int32 = i_outer * 10 + i_inner
                            B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
                            A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                            B_2[cse_var_1 * B_1.strides[0] + cse_var_2 * B_1.strides[1]] = A_2[cse_var_1 * A_1.strides[0] + cse_var_2 * A_1.strides[1]]




.. GENERATED FROM PYTHON SOURCE LINES 135-139

bind
----
:code:`bind` can bind a specified axis with a thread axis, often used
in gpu programming.

.. GENERATED FROM PYTHON SOURCE LINES 139-148

.. code-block:: default

    A = te.placeholder((n,), name="A")
    B = te.compute(A.shape, lambda i: A[i] * 2, name="B")

    s = te.create_schedule(B.op)
    bx, tx = s[B].split(B.op.axis[0], factor=64)
    s[B].bind(bx, te.thread_axis("blockIdx.x"))
    s[B].bind(tx, te.thread_axis("threadIdx.x"))
    print(tvm.lower(s, [A, B], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            n = T.int32()
            A_1 = T.match_buffer(A, (n,), strides=("stride",), buffer_type="auto")
            B_1 = T.match_buffer(B, (n,), strides=("stride",), buffer_type="auto")
            blockIdx_x = T.launch_thread("blockIdx.x", (n + 63) // 64)
            threadIdx_x = T.launch_thread("threadIdx.x", 64)
            if T.likely(blockIdx_x * 64 + threadIdx_x < n):
                B_2 = T.Buffer((B_1.strides[0] * n,), data=B_1.data, buffer_type="auto")
                A_2 = T.Buffer((A_1.strides[0] * n,), data=A_1.data, buffer_type="auto")
                B_2[(blockIdx_x * 64 + threadIdx_x) * B_1.strides[0]] = A_2[(blockIdx_x * 64 + threadIdx_x) * A_1.strides[0]] * T.float32(2)




.. GENERATED FROM PYTHON SOURCE LINES 149-153

compute_at
----------
For a schedule that consists of multiple operators, TVM will compute
tensors at the root separately by default.

.. GENERATED FROM PYTHON SOURCE LINES 153-160

.. code-block:: default

    A = te.placeholder((m,), name="A")
    B = te.compute((m,), lambda i: A[i] + 1, name="B")
    C = te.compute((m,), lambda i: B[i] * 2, name="C")

    s = te.create_schedule(C.op)
    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle, C: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m = T.int32()
            A_1 = T.match_buffer(A, (m,), strides=("stride",), buffer_type="auto")
            B_1 = T.match_buffer(B, (m,), strides=("stride",), buffer_type="auto")
            C_1 = T.match_buffer(C, (m,), strides=("stride",), buffer_type="auto")
            B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
            for i in range(m):
                A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                B_2[i * B_1.strides[0]] = A_2[i * A_1.strides[0]] + T.float32(1)
            for i in range(m):
                C_2 = T.Buffer((C_1.strides[0] * m,), data=C_1.data, buffer_type="auto")
                C_2[i * C_1.strides[0]] = B_2[i * B_1.strides[0]] * T.float32(2)




.. GENERATED FROM PYTHON SOURCE LINES 161-163

:code:`compute_at` can move computation of `B` into the first axis
of computation of `C`.

.. GENERATED FROM PYTHON SOURCE LINES 163-171

.. code-block:: default

    A = te.placeholder((m,), name="A")
    B = te.compute((m,), lambda i: A[i] + 1, name="B")
    C = te.compute((m,), lambda i: B[i] * 2, name="C")

    s = te.create_schedule(C.op)
    s[B].compute_at(s[C], C.op.axis[0])
    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle, C: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m = T.int32()
            A_1 = T.match_buffer(A, (m,), strides=("stride",), buffer_type="auto")
            B_1 = T.match_buffer(B, (m,), strides=("stride",), buffer_type="auto")
            C_1 = T.match_buffer(C, (m,), strides=("stride",), buffer_type="auto")
            for i in range(m):
                B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
                A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                B_2[i * B_1.strides[0]] = A_2[i * A_1.strides[0]] + T.float32(1)
                C_2 = T.Buffer((C_1.strides[0] * m,), data=C_1.data, buffer_type="auto")
                C_2[i * C_1.strides[0]] = B_2[i * B_1.strides[0]] * T.float32(2)




.. GENERATED FROM PYTHON SOURCE LINES 172-177

compute_inline
--------------
:code:`compute_inline` can mark one stage as inline, then the body of
computation will be expanded and inserted at the address where the
tensor is required.

.. GENERATED FROM PYTHON SOURCE LINES 177-185

.. code-block:: default

    A = te.placeholder((m,), name="A")
    B = te.compute((m,), lambda i: A[i] + 1, name="B")
    C = te.compute((m,), lambda i: B[i] * 2, name="C")

    s = te.create_schedule(C.op)
    s[B].compute_inline()
    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle, C: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m = T.int32()
            A_1 = T.match_buffer(A, (m,), strides=("stride",), buffer_type="auto")
            B_1 = T.match_buffer(B, (m,), strides=("stride",), buffer_type="auto")
            C_1 = T.match_buffer(C, (m,), strides=("stride",), buffer_type="auto")
            for i in range(m):
                C_2 = T.Buffer((C_1.strides[0] * m,), data=C_1.data, buffer_type="auto")
                A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                C_2[i * C_1.strides[0]] = (A_2[i * A_1.strides[0]] + T.float32(1)) * T.float32(2)




.. GENERATED FROM PYTHON SOURCE LINES 186-189

compute_root
------------
:code:`compute_root` can move computation of one stage to the root.

.. GENERATED FROM PYTHON SOURCE LINES 189-198

.. code-block:: default

    A = te.placeholder((m,), name="A")
    B = te.compute((m,), lambda i: A[i] + 1, name="B")
    C = te.compute((m,), lambda i: B[i] * 2, name="C")

    s = te.create_schedule(C.op)
    s[B].compute_at(s[C], C.op.axis[0])
    s[B].compute_root()
    print(tvm.lower(s, [A, B, C], simple_mode=True))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    # from tvm.script import ir as I
    # from tvm.script import tir as T

    @I.ir_module
    class Module:
        @T.prim_func
        def main(A: T.handle, B: T.handle, C: T.handle):
            T.func_attr({"from_legacy_te_schedule": True, "global_symbol": "main", "tir.noalias": True})
            m = T.int32()
            A_1 = T.match_buffer(A, (m,), strides=("stride",), buffer_type="auto")
            B_1 = T.match_buffer(B, (m,), strides=("stride",), buffer_type="auto")
            C_1 = T.match_buffer(C, (m,), strides=("stride",), buffer_type="auto")
            B_2 = T.Buffer((B_1.strides[0] * m,), data=B_1.data, buffer_type="auto")
            for i in range(m):
                A_2 = T.Buffer((A_1.strides[0] * m,), data=A_1.data, buffer_type="auto")
                B_2[i * B_1.strides[0]] = A_2[i * A_1.strides[0]] + T.float32(1)
            for i in range(m):
                C_2 = T.Buffer((C_1.strides[0] * m,), data=C_1.data, buffer_type="auto")
                C_2[i * C_1.strides[0]] = B_2[i * B_1.strides[0]] * T.float32(2)




.. GENERATED FROM PYTHON SOURCE LINES 199-212

Summary
-------
This tutorial provides an introduction to schedule primitives in
tvm, which permits users schedule the computation easily and
flexibly.

In order to get a good performance kernel implementation, the
general workflow often is:

- Describe your computation via series of operations.
- Try to schedule the computation with primitives.
- Compile and run to see the performance difference.
- Adjust your schedule according the running result.


.. _sphx_glr_download_how_to_work_with_schedules_schedule_primitives.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: schedule_primitives.py <schedule_primitives.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: schedule_primitives.ipynb <schedule_primitives.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
