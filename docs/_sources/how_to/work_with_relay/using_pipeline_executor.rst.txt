
.. DO NOT EDIT. THIS FILE WAS AUTOMATICALLY GENERATED BY
.. TVM'S MONKEY-PATCHED VERSION OF SPHINX-GALLERY. TO MAKE
.. CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "how_to/work_with_relay/using_pipeline_executor.py"

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        This tutorial can be used interactively with Google Colab! You can also click
        :ref:`here <sphx_glr_download_how_to_work_with_relay_using_pipeline_executor.py>` to run the Jupyter notebook locally.

        .. image:: https://raw.githubusercontent.com/tlc-pack/web-data/main/images/utilities/colab_button.svg
            :align: center
            :target: https://colab.research.google.com/github/apache/tvm-site/blob/asf-site/docs/_downloads/f407f66fb8174d0d4ec37407af1128d6/using_pipeline_executor.ipynb
            :width: 300px

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_how_to_work_with_relay_using_pipeline_executor.py:


Using Pipeline Executor in Relay
=================================
**Author**: `Hua Jiang <https://github.com/huajsj>`_

This is a short tutorial on how to use "Pipeline Executor" with Relay.

.. GENERATED FROM PYTHON SOURCE LINES 24-35

.. code-block:: default

    import tvm
    from tvm import te
    import numpy as np
    from tvm.contrib import graph_executor as runtime
    from tvm.relay.op.contrib.cutlass import partition_for_cutlass
    from tvm import relay
    from tvm.relay import testing
    import tvm.testing
    from tvm.contrib.cutlass import finalize_modules

    img_size = 8







.. GENERATED FROM PYTHON SOURCE LINES 36-40

Create a simple network, this network can be a pre-trained model too.
---------------------------------------------------------------------
Let's create a very simple network for demonstration.
It consists of convolution, batch normalization, dense, and ReLU activation.

.. GENERATED FROM PYTHON SOURCE LINES 40-66

.. code-block:: default

    def get_network():
        out_channels = 16
        batch_size = 1
        data = relay.var("data", relay.TensorType((batch_size, 3, img_size, img_size), "float16"))
        dense_weight = relay.var(
            "dweight", relay.TensorType((batch_size, 16 * img_size * img_size), "float16")
        )
        weight = relay.var("weight")
        bn_gamma = relay.var("bn_gamma")
        bn_beta = relay.var("bn_beta")
        bn_mmean = relay.var("bn_mean")
        bn_mvar = relay.var("bn_var")
        simple_net = relay.nn.conv2d(
            data=data, weight=weight, kernel_size=(3, 3), channels=out_channels, padding=(1, 1)
        )
        simple_net = relay.nn.batch_norm(simple_net, bn_gamma, bn_beta, bn_mmean, bn_mvar)[0]
        simple_net = relay.nn.relu(simple_net)
        simple_net = relay.nn.batch_flatten(simple_net)
        simple_net = relay.nn.dense(simple_net, dense_weight)
        simple_net = relay.Function(relay.analysis.free_vars(simple_net), simple_net)
        data_shape = (batch_size, 3, img_size, img_size)
        net, params = testing.create_workload(simple_net)
        return net, params, data_shape


    net, params, data_shape = get_network()







.. GENERATED FROM PYTHON SOURCE LINES 67-71

Splitting the network into two subgraphs.
-----------------------------------------
This function called 'graph_split' from a unit test is just an example. User can create a customized logic
to split the graph.

.. GENERATED FROM PYTHON SOURCE LINES 71-78

.. code-block:: default

    import inspect
    import os

    tutorial_dir = os.path.dirname(inspect.getfile(lambda: None))
    os.sys.path.append(os.path.join(tutorial_dir, "../../../tests/python/relay"))
    from test_pipeline_executor import graph_split








.. GENERATED FROM PYTHON SOURCE LINES 79-80

Splitting the network into two subgraphs.

.. GENERATED FROM PYTHON SOURCE LINES 80-82

.. code-block:: default

    split_config = [{"op_name": "nn.relu", "op_index": 0}]
    subgraphs = graph_split(net["main"], split_config, params)







.. GENERATED FROM PYTHON SOURCE LINES 83-84

The generated subgraphs should look something like below.

.. GENERATED FROM PYTHON SOURCE LINES 84-105

.. code-block:: default


    """
    #subgraphs[0])

     def @main(%data: Tensor[(1, 3, img_size, img_size), float16]) {
      %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float16] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, img_size, img_size), float16] */;
      %1 = nn.batch_norm(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float16] */, meta[relay.Constant][2] /* ty=Tensor[(16), float16]*/, meta[relay.Constant][3] /* ty=Tensor[(16), float16] */, meta[relay.Constant][4] /* ty=Tensor[(16), float16] */) /* ty=(Tensor[(1,16, img_size, img_size), float16], Tensor[(16), float16], Tensor[(16), float16]) */;
      %2 = %1.0;
      nn.relu(%2) /* ty=Tensor[(1, 16, img_size, img_size), float16] */
     }

    #subgraphs[1]

     def @main(%data_n_0: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */) {
      %0 = nn.batch_flatten(%data_n_0) /* ty=Tensor[(1, 1024), float16] */;
      nn.dense(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 1024), float16] */, units=None) /* ty=Tensor[(1, 1), float16] */
     }

    """






.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    '\n#subgraphs[0])\n\n def @main(%data: Tensor[(1, 3, img_size, img_size), float16]) {\n  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float16] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, img_size, img_size), float16] */;\n  %1 = nn.batch_norm(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float16] */, meta[relay.Constant][2] /* ty=Tensor[(16), float16]*/, meta[relay.Constant][3] /* ty=Tensor[(16), float16] */, meta[relay.Constant][4] /* ty=Tensor[(16), float16] */) /* ty=(Tensor[(1,16, img_size, img_size), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n  %2 = %1.0;\n  nn.relu(%2) /* ty=Tensor[(1, 16, img_size, img_size), float16] */\n }\n\n#subgraphs[1]\n\n def @main(%data_n_0: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */) {\n  %0 = nn.batch_flatten(%data_n_0) /* ty=Tensor[(1, 1024), float16] */;\n  nn.dense(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 1024), float16] */, units=None) /* ty=Tensor[(1, 1), float16] */\n }\n\n'



.. GENERATED FROM PYTHON SOURCE LINES 106-108

Build the subgraph with cutlass target.
---------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 108-133

.. code-block:: default


    cutlass = tvm.target.Target(
        {
            "kind": "cutlass",
            "sm": int(tvm.target.Target("cuda").arch.split("_")[1]),
            "use_3xtf32": True,
            "split_k_slices": [1],
            "profile_all_alignments": False,
            "find_first_valid": True,
            "use_multiprocessing": True,
            "use_fast_math": False,
            "tmp_dir": "./tmp",
        },
        host=tvm.target.Target("llvm"),
    )


    def cutlass_build(mod, target, params=None, target_host=None, mod_name="default"):
        target = [target, cutlass]
        lib = relay.build_module.build(
            mod, target=target, params=params, target_host=target_host, mod_name=mod_name
        )
        return lib









.. GENERATED FROM PYTHON SOURCE LINES 134-137

Run the two subgraphs in pipeline with pipeline executor.
---------------------------------------------------------
Set 'USE_PIPELINE_EXECUTOR' as ON, and set USE_CUTLASS' as ON  in cmake.

.. GENERATED FROM PYTHON SOURCE LINES 137-139

.. code-block:: default

    from tvm.contrib import graph_executor, pipeline_executor, pipeline_executor_build








.. GENERATED FROM PYTHON SOURCE LINES 140-143

Create subgraph pipeline configuration.
Associate a subgraph module with a target.
Use CUTLASS BYOC to build the second subgraph module.

.. GENERATED FROM PYTHON SOURCE LINES 143-146

.. code-block:: default

    mod0, mod1 = subgraphs[0], subgraphs[1]
    # Use cutlass as the codegen.
    mod1 = partition_for_cutlass(mod1)







.. GENERATED FROM PYTHON SOURCE LINES 147-148

Get the pipeline executor configuration object.

.. GENERATED FROM PYTHON SOURCE LINES 148-149

.. code-block:: default

    pipe_config = pipeline_executor_build.PipelineConfig()







.. GENERATED FROM PYTHON SOURCE LINES 150-151

Set the compile target of the subgraph module.

.. GENERATED FROM PYTHON SOURCE LINES 151-153

.. code-block:: default

    pipe_config[mod0].target = "llvm"
    pipe_config[mod0].dev = tvm.cpu(0)







.. GENERATED FROM PYTHON SOURCE LINES 154-155

Set the compile target of the second subgraph module as cuda.

.. GENERATED FROM PYTHON SOURCE LINES 155-166

.. code-block:: default

    pipe_config[mod1].target = "cuda"
    pipe_config[mod1].dev = tvm.device("cuda", 0)
    pipe_config[mod1].build_func = cutlass_build
    pipe_config[mod1].export_cc = "nvcc"
    # Create the pipeline by connecting the subgraph modules.
    # The global input will be forwarded to the input interface of the first module named mod0
    pipe_config["input"]["data"].connect(pipe_config[mod0]["input"]["data"])
    # The first output of mod0 will be forwarded to the input interface of mod1
    pipe_config[mod0]["output"][0].connect(pipe_config[mod1]["input"]["data_n_0"])
    # The first output of mod1 will be the first global output.
    pipe_config[mod1]["output"][0].connect(pipe_config["output"][0])







.. GENERATED FROM PYTHON SOURCE LINES 167-168

The pipeline configuration as below.

.. GENERATED FROM PYTHON SOURCE LINES 168-180

.. code-block:: default

    """
    print(pipe_config)
     Inputs
      |data: mod0:data

     output
      |output(0) : mod1.output(0)

     connections
      |mod0.output(0)-> mod1.data_n_0
    """





.. rst-class:: sphx-glr-script-out

 .. code-block:: none


    '\nprint(pipe_config)\n Inputs\n  |data: mod0:data\n\n output\n  |output(0) : mod1.output(0)\n\n connections\n  |mod0.output(0)-> mod1.data_n_0\n'



.. GENERATED FROM PYTHON SOURCE LINES 181-183

Build the pipeline executor.
----------------------------

.. GENERATED FROM PYTHON SOURCE LINES 183-185

.. code-block:: default

    with tvm.transform.PassContext(opt_level=3):
        pipeline_mod_factory = pipeline_executor_build.build(pipe_config)







.. GENERATED FROM PYTHON SOURCE LINES 186-187

Export the parameter configuration to a file.

.. GENERATED FROM PYTHON SOURCE LINES 187-190

.. code-block:: default

    directory_path = tvm.contrib.utils.tempdir().temp_dir
    os.makedirs(directory_path, exist_ok=True)
    config_file_name = pipeline_mod_factory.export_library(directory_path)







.. GENERATED FROM PYTHON SOURCE LINES 191-193

Use the load function to create and initialize PipelineModule.
--------------------------------------------------------------

.. GENERATED FROM PYTHON SOURCE LINES 193-195

.. code-block:: default

    pipeline_module = pipeline_executor.PipelineModule.load_library(config_file_name)








.. GENERATED FROM PYTHON SOURCE LINES 196-199

Run the pipeline executor.
--------------------------
Allocate input data.

.. GENERATED FROM PYTHON SOURCE LINES 199-201

.. code-block:: default

    data = np.random.uniform(-1, 1, size=data_shape).astype("float16")
    pipeline_module.set_input("data", tvm.nd.array(data))







.. GENERATED FROM PYTHON SOURCE LINES 202-204

Run the two subgraph in the pipeline mode to get the output asynchronously
or synchronously. In the following example, it is synchronous.

.. GENERATED FROM PYTHON SOURCE LINES 204-206

.. code-block:: default

    pipeline_module.run()
    outputs = pipeline_module.get_output()







.. GENERATED FROM PYTHON SOURCE LINES 207-210

Use graph_executor for verification.
------------------------------------
Run these two subgraphs in sequence with graph_executor to get the output.

.. GENERATED FROM PYTHON SOURCE LINES 210-230

.. code-block:: default

    target = "llvm"
    dev0 = tvm.device(target, 0)
    lib0 = relay.build_module.build(mod0, target, params=params)
    module0 = runtime.GraphModule(lib0["default"](dev0))
    cuda = tvm.target.Target("cuda", host=tvm.target.Target("llvm"))
    lib1 = relay.build_module.build(mod1, [cuda, cutlass], params=params)
    lib1 = finalize_modules(lib1, "compile.so", "./tmp")

    dev1 = tvm.device("cuda", 0)

    module1 = runtime.GraphModule(lib1["default"](dev1))

    module0.set_input("data", data)
    module0.run()
    out_shape = (1, 16, img_size, img_size)
    out = module0.get_output(0, tvm.nd.empty(out_shape, "float16"))
    module1.set_input("data_n_0", out)
    module1.run()
    out_shape = (1, 1)
    out = module1.get_output(0, tvm.nd.empty(out_shape, "float16"))







.. GENERATED FROM PYTHON SOURCE LINES 231-232

Verify the result.

.. GENERATED FROM PYTHON SOURCE LINES 232-233

.. code-block:: default

    tvm.testing.assert_allclose(outputs[0].numpy(), out.numpy())








.. _sphx_glr_download_how_to_work_with_relay_using_pipeline_executor.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example


    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: using_pipeline_executor.py <using_pipeline_executor.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: using_pipeline_executor.ipynb <using_pipeline_executor.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
