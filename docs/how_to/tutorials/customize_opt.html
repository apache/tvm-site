



<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Customize Optimization &mdash; tvm 0.23.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/downloads/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=61e891cc"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Optimize Large Language Model" href="optimize_llm.html" />
    <link rel="prev" title="End-to-End Optimize Model" href="e2e_opt_model.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="sidetitle" alt="Documentation Home"> tvm
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/ir_module.html">IRModule</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How To</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="e2e_opt_model.html">End-to-End Optimize Model</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Customize Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#review-overall-flow">Review Overall Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#composable-irmodule-optimization">Composable IRModule Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#prepare-a-relax-module">Prepare a Relax Module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#library-dispatch">Library Dispatch</a></li>
<li class="toctree-l3"><a class="reference internal" href="#auto-tuning">Auto Tuning</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dlight-rules">DLight Rules</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-the-optimized-model">Deploy the Optimized Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="optimize_llm.html">Optimize Large Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="export_and_load_executable.html">Export and Load Relax Executables</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/index.html">Development Guides</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../arch/index.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/tensor_ir/index.html">TensorIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/relax/index.html">Relax</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/links.html">Other APIs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/security.html">Security Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Customize Optimization</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/tvm/edit/main/docs/how_to/tutorials/customize_opt.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>You can click <a class="reference internal" href="#sphx-glr-download-how-to-tutorials-customize-opt-py"><span class="std std-ref">here</span></a> to run the Jupyter notebook locally.</p>
</div>
<section class="sphx-glr-example-title" id="customize-optimization">
<span id="customize-opt"></span><span id="sphx-glr-how-to-tutorials-customize-opt-py"></span><h1>Customize Optimization<a class="headerlink" href="#customize-optimization" title="Link to this heading"></a></h1>
<p>One main design goal of Apache TVM is to enable easy customization of the optimization pipeline
for both research or development purposes and iterate the engineering optimizations. In this
tutorial we will</p>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#review-overall-flow" id="id1">Review Overall Flow</a></p></li>
<li><p><a class="reference internal" href="#composable-irmodule-optimization" id="id2">Composable IRModule Optimization</a></p></li>
<li><p><a class="reference internal" href="#deploy-the-optimized-model" id="id3">Deploy the Optimized Model</a></p></li>
<li><p><a class="reference internal" href="#summary" id="id4">Summary</a></p></li>
</ul>
</nav>
<section id="review-overall-flow">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Review Overall Flow</a><a class="headerlink" href="#review-overall-flow" title="Link to this heading"></a></h2>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_static/downloads/tvm_overall_flow.svg"><img alt="../../_static/downloads/tvm_overall_flow.svg" src="../../_static/downloads/tvm_overall_flow.svg" style="width: 80%;" />
</a>
</figure>
<p>The overall flow consists of the following steps:</p>
<ul class="simple">
<li><p><strong>Construct or Import a Model</strong>: Construct a neural network model or import a pre-trained
model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains
all the information needed for compilation, including high-level Relax functions for
computational graph, and low-level TensorIR functions for tensor program.</p></li>
<li><p><strong>Perform Composable Optimizations</strong>: Perform a series of optimization transformations,
such as graph optimizations, tensor program optimizations, and library dispatching.</p></li>
<li><p><strong>Build and Universal Deployment</strong>: Build the optimized model to a deployable module to the
universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tempfile</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tvm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm</span><span class="w"> </span><span class="kn">import</span> <a href="../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">IRModule</span></a><span class="p">,</span> <span class="n">relax</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax.frontend</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
</pre></div>
</div>
</section>
<section id="composable-irmodule-optimization">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Composable IRModule Optimization</a><a class="headerlink" href="#composable-irmodule-optimization" title="Link to this heading"></a></h2>
<p>Apache TVM Unity provides a flexible way to optimize the IRModule. Everything centered
around IRModule optimization can be composed with existing pipelines. Note that each optimization
can focus on <strong>part of the computation graph</strong>, enabling partial lowering or partial optimization.</p>
<p>In this tutorial, we will demonstrate how to optimize a model with Apache TVM Unity.</p>
<section id="prepare-a-relax-module">
<h3>Prepare a Relax Module<a class="headerlink" href="#prepare-a-relax-module" title="Link to this heading"></a></h3>
<p>We first prepare a Relax module. The module can be imported from other frameworks, constructed
with NN module frontend or TVMScript. Here we use a simple neural network model as an example.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RelaxModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RelaxModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_shape</span></a> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>
<span class="n">mod</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">params</span></a> <span class="o">=</span> <span class="n">RelaxModel</span><span class="p">()</span><span class="o">.</span><span class="n">export_tvm</span><span class="p">({</span><span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_shape</span></a><span class="p">,</span> <span class="s2">&quot;float32&quot;</span><span class="p">)}})</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import relax as R

@I.ir_module
class Module:
    @R.function
    def forward(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        with R.dataflow():
            permute_dims: R.Tensor((784, 256), dtype=&quot;float32&quot;) = R.permute_dims(fc1_weight, axes=None)
            matmul: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.matmul(x, permute_dims, out_dtype=&quot;void&quot;)
            add: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.add(matmul, fc1_bias)
            relu: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.nn.relu(add)
            permute_dims1: R.Tensor((256, 10), dtype=&quot;float32&quot;) = R.permute_dims(fc2_weight, axes=None)
            matmul1: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.matmul(relu, permute_dims1, out_dtype=&quot;void&quot;)
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = matmul1
            R.output(gv)
        return gv
</pre></div>
</div>
</section>
<section id="library-dispatch">
<h3>Library Dispatch<a class="headerlink" href="#library-dispatch" title="Link to this heading"></a></h3>
<p>We would like to quickly try out a variant of library optimization for certain platforms
(e.g., GPU). We can write a certain dispatching pass for the specific platform and
operator. Here we demonstrate how to dispatch the CUBLAS library for certain patterns.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial only demonstrates a single operator dispatching for CUBLAS, highlighting
the flexibility of the optimization pipeline. In real-world cases, we can import multiple
patterns and dispatch them to different kernels.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import cublas pattern</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tvm.relax.backend.cuda.cublas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">_cublas</span>


<span class="c1"># Define a new pass for CUBLAS dispatch</span>
<span class="nd">@tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">module_pass</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;CublasDispatch&quot;</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">CublasDispatch</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">transform_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">:</span> <a href="../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">IRModule</span></a><span class="p">,</span> <span class="n">_ctx</span><span class="p">:</span> <a href="../../reference/api/python/transform.html#tvm.transform.PassContext" title="tvm.transform.PassContext" class="sphx-glr-backref-module-tvm-transform sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a href="../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">IRModule</span></a><span class="p">:</span>
        <span class="c1"># Check if CUBLAS is enabled</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">tvm</span><span class="o">.</span><span class="n">get_global_func</span><span class="p">(</span><span class="s2">&quot;relax.ext.cublas&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span><span class="s2">&quot;CUBLAS is not enabled.&quot;</span><span class="p">)</span>

        <span class="c1"># Get interested patterns</span>
        <span class="n">patterns</span> <span class="o">=</span> <span class="p">[</span><span class="n">relax</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">get_pattern</span><span class="p">(</span><span class="s2">&quot;cublas.matmul_transposed_bias_relu&quot;</span><span class="p">)]</span>
        <span class="c1"># Note in real-world cases, we usually get all patterns</span>
        <span class="c1"># patterns = relax.backend.get_patterns_with_prefix(&quot;cublas&quot;)</span>

        <span class="c1"># Fuse ops by patterns and then run codegen</span>
        <span class="n">mod</span> <span class="o">=</span> <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.FuseOpsByPattern" title="tvm.relax.transform.FuseOpsByPattern" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FuseOpsByPattern</span></a><span class="p">(</span><span class="n">patterns</span><span class="p">,</span> <span class="n">annotate_codegen</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">mod</span><span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.RunCodegen" title="tvm.relax.transform.RunCodegen" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">RunCodegen</span></a><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mod</span>


<span class="n">mod</span> <span class="o">=</span> <span class="n">CublasDispatch</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import relax as R

@I.ir_module
class Module:
    I.module_attrs({&quot;external_mods&quot;: [metadata[&quot;ffi.Module&quot;][0]]})
    @R.function
    def forward(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        with R.dataflow():
            lv = R.call_dps_packed(&quot;fused_relax_permute_dims_relax_matmul_relax_add_relax_nn_relu_cublas&quot;, (fc1_weight, x, fc1_bias), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            permute_dims1: R.Tensor((256, 10), dtype=&quot;float32&quot;) = R.permute_dims(fc2_weight, axes=None)
            matmul1: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.matmul(lv, permute_dims1, out_dtype=&quot;void&quot;)
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = matmul1
            R.output(gv)
        return gv

# Metadata omitted. Use show_meta=True in script() method to show it.
</pre></div>
</div>
<p>After the dispatching pass, we can see that the first <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.ReLU</span></code> are fused
and rewritten to a <code class="docutils literal notranslate"><span class="pre">call_dps_packed</span></code> function which call the CUBLAS library. Notably, the
other part is not changed, which means we can selectively dispatch the optimization for
certain computation.</p>
</section>
<section id="auto-tuning">
<h3>Auto Tuning<a class="headerlink" href="#auto-tuning" title="Link to this heading"></a></h3>
<p>Continuing from the previous example, we can further optimize the model with auto-tuning for
the <strong>rest part of the computation</strong>. Here we demonstrate how to use the meta-schedule to auto-tune
the model.</p>
<p>We can use <code class="docutils literal notranslate"><span class="pre">MetaScheduleTuneTIR</span></code> pass to simply tuning the model, while <code class="docutils literal notranslate"><span class="pre">MetaScheduleApplyDatabase</span></code>
pass to apply the best configuration to the model. The tuning process will generate search space,
tune the model and the following steps will apply the best configuration to the model. Before
running the passes, we need to lowering relax operator into TensorIR functions via <code class="docutils literal notranslate"><span class="pre">LegalizeOps</span></code></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To save CI time and avoid flakiness, we skip the tuning process in CI environment.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cuda</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a> <span class="o">=</span> <a href="../../reference/api/python/target.html#tvm.target.Target.from_device" title="tvm.target.Target.from_device" class="sphx-glr-backref-module-tvm-target-Target sphx-glr-backref-type-py-method"><span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span><span class="o">.</span><span class="n">from_device</span></a><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="k">if</span> <a href="https://docs.python.org/3/library/os.html#os.getenv" title="os.getenv" class="sphx-glr-backref-module-os sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">getenv</span></a><span class="p">(</span><span class="s2">&quot;CI&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;true&quot;</span><span class="p">:</span>
    <span class="n">trials</span> <span class="o">=</span> <span class="mi">2000</span>
    <span class="k">with</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryDirectory" title="tempfile.TemporaryDirectory" class="sphx-glr-backref-module-tempfile sphx-glr-backref-type-py-class"><span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span></a><span class="p">()</span> <span class="k">as</span> <span class="n">tmp_dir</span><span class="p">:</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="p">[</span>
                <a href="../../reference/api/python/relax/relax.html#tvm.relax.get_pipeline" title="tvm.relax.get_pipeline" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">get_pipeline</span></a><span class="p">(</span><span class="s2">&quot;zero&quot;</span><span class="p">),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.MetaScheduleTuneTIR" title="tvm.relax.transform.MetaScheduleTuneTIR" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">MetaScheduleTuneTIR</span></a><span class="p">(</span><span class="n">work_dir</span><span class="o">=</span><span class="n">tmp_dir</span><span class="p">,</span> <span class="n">max_trials_global</span><span class="o">=</span><span class="n">trials</span><span class="p">),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.MetaScheduleApplyDatabase" title="tvm.relax.transform.MetaScheduleApplyDatabase" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">MetaScheduleApplyDatabase</span></a><span class="p">(</span><span class="n">work_dir</span><span class="o">=</span><span class="n">tmp_dir</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)(</span><span class="n">mod</span><span class="p">)</span>

    <span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="dlight-rules">
<h3>DLight Rules<a class="headerlink" href="#dlight-rules" title="Link to this heading"></a></h3>
<p>DLight rules are a set of default rules for scheduling and optimization the kernel.
DLight rules are designed for fast compilation and <strong>fair</strong> performance. In some cases,
e.g. language model, DLight provides excellent performance, while for generic models,
it achieves a balance between performance and compilation time.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tvm</span><span class="w"> </span><span class="kn">import</span> <span class="n">dlight</span> <span class="k">as</span> <span class="n">dl</span>

<span class="c1"># Apply DLight rules</span>
<span class="k">with</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a><span class="p">:</span>
    <span class="n">mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="p">[</span>
            <a href="../../reference/api/python/relax/relax.html#tvm.relax.get_pipeline" title="tvm.relax.get_pipeline" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">get_pipeline</span></a><span class="p">(</span><span class="s2">&quot;zero&quot;</span><span class="p">),</span>
            <a href="../../reference/api/python/dlight.html#tvm.dlight.ApplyDefaultSchedule" title="tvm.dlight.ApplyDefaultSchedule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dl</span><span class="o">.</span><span class="n">ApplyDefaultSchedule</span></a><span class="p">(</span>  <span class="c1"># pylint: disable=not-callable</span>
                <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Matmul</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">GEMV</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Reduction</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">GeneralReduction</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Fallback</span></a><span class="p">(),</span>
            <span class="p">),</span>
        <span class="p">]</span>
    <span class="p">)(</span><span class="n">mod</span><span class="p">)</span>

<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    I.module_attrs({&quot;external_mods&quot;: [metadata[&quot;ffi.Module&quot;][0]]})
    @T.prim_func(private=True)
    def matmul(lv: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;), permute_dims1: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;), matmul: T.Buffer((T.int64(1), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;op_pattern&quot;: 4, &quot;tir.is_scheduled&quot;: True, &quot;tir.noalias&quot;: True})
        # with T.block(&quot;root&quot;):
        matmul_rf_local = T.alloc_buffer((T.int64(16), T.int64(1), T.int64(10)), scope=&quot;local&quot;)
        for ax0_fused_0 in T.thread_binding(T.int64(1), thread=&quot;blockIdx.x&quot;):
            for ax0_fused_1 in T.thread_binding(T.int64(10), thread=&quot;threadIdx.x&quot;):
                for ax1_fused_1 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.y&quot;):
                    with T.block(&quot;matmul_rf_init&quot;):
                        vax1_fused_1 = T.axis.spatial(T.int64(16), ax1_fused_1)
                        v0 = T.axis.spatial(T.int64(10), ax0_fused_0 * T.int64(10) + ax0_fused_1)
                        T.reads()
                        T.writes(matmul_rf_local[vax1_fused_1, T.int64(0), v0])
                        matmul_rf_local[vax1_fused_1, T.int64(0), v0] = T.float32(0.0)
                    for ax1_fused_0, u in T.grid(T.int64(16), 1):
                        with T.block(&quot;matmul_rf_update&quot;):
                            vax1_fused_1 = T.axis.spatial(T.int64(16), ax1_fused_1)
                            v0 = T.axis.spatial(T.int64(10), ax0_fused_0 * T.int64(10) + ax0_fused_1)
                            vax1_fused_0 = T.axis.reduce(T.int64(16), ax1_fused_0)
                            T.reads(matmul_rf_local[vax1_fused_1, T.int64(0), v0], lv[T.int64(0), vax1_fused_0 * T.int64(16) + vax1_fused_1], permute_dims1[vax1_fused_0 * T.int64(16) + vax1_fused_1, v0])
                            T.writes(matmul_rf_local[vax1_fused_1, T.int64(0), v0])
                            matmul_rf_local[vax1_fused_1, T.int64(0), v0] = matmul_rf_local[vax1_fused_1, T.int64(0), v0] + lv[T.int64(0), vax1_fused_0 * T.int64(16) + vax1_fused_1] * permute_dims1[vax1_fused_0 * T.int64(16) + vax1_fused_1, v0]
            for ax1_fused in T.thread_binding(T.int64(10), thread=&quot;threadIdx.x&quot;):
                for ax0 in T.thread_binding(T.int64(16), thread=&quot;threadIdx.y&quot;):
                    with T.block(&quot;matmul&quot;):
                        vax1_fused_1, v0 = T.axis.remap(&quot;RS&quot;, [ax0, ax1_fused])
                        T.reads(matmul_rf_local[vax1_fused_1, T.int64(0), v0])
                        T.writes(matmul[T.int64(0), v0])
                        with T.init():
                            matmul[T.int64(0), v0] = T.float32(0.0)
                        matmul[T.int64(0), v0] = matmul[T.int64(0), v0] + matmul_rf_local[vax1_fused_1, T.int64(0), v0]

    @T.prim_func(private=True)
    def transpose(fc2_weight: T.Buffer((T.int64(10), T.int64(256)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;op_pattern&quot;: 2, &quot;tir.is_scheduled&quot;: True, &quot;tir.noalias&quot;: True})
        # with T.block(&quot;root&quot;):
        for ax0_ax1_fused_0 in T.thread_binding(T.int64(3), thread=&quot;blockIdx.x&quot;):
            for ax0_ax1_fused_1 in T.thread_binding(T.int64(1024), thread=&quot;threadIdx.x&quot;):
                with T.block(&quot;T_transpose&quot;):
                    v0 = T.axis.spatial(T.int64(256), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) // T.int64(10))
                    v1 = T.axis.spatial(T.int64(10), (ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1) % T.int64(10))
                    T.where(ax0_ax1_fused_0 * T.int64(1024) + ax0_ax1_fused_1 &lt; T.int64(2560))
                    T.reads(fc2_weight[v1, v0])
                    T.writes(T_transpose[v0, v1])
                    T_transpose[v0, v1] = fc2_weight[v1, v0]

    @R.function
    def forward(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        cls = Module
        with R.dataflow():
            lv = R.call_dps_packed(&quot;fused_relax_permute_dims_relax_matmul_relax_add_relax_nn_relu_cublas&quot;, (fc1_weight, x, fc1_bias), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            permute_dims1 = R.call_tir(cls.transpose, (fc2_weight,), out_sinfo=R.Tensor((256, 10), dtype=&quot;float32&quot;))
            gv = R.call_tir(cls.matmul, (lv, permute_dims1), out_sinfo=R.Tensor((1, 10), dtype=&quot;float32&quot;))
            R.output(gv)
        return gv

# Metadata omitted. Use show_meta=True in script() method to show it.
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial focuses on the demonstration of the optimization pipeline, instead of
pushing the performance to the limit. The current optimization may not be the best.</p>
</div>
</section>
</section>
<section id="deploy-the-optimized-model">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Deploy the Optimized Model</a><a class="headerlink" href="#deploy-the-optimized-model" title="Link to this heading"></a></h2>
<p>We can build and deploy the optimized model to the TVM runtime.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="../../reference/api/python/relax/relax.html#tvm.relax.VMExecutable" title="tvm.relax.VMExecutable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ex</span></a> <span class="o">=</span> <a href="../../reference/api/python/driver.html#tvm.compile" title="tvm.compile" class="sphx-glr-backref-module-tvm sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">vm</span> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.VirtualMachine" title="tvm.relax.VirtualMachine" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span></a><span class="p">(</span><a href="../../reference/api/python/relax/relax.html#tvm.relax.VMExecutable" title="tvm.relax.VMExecutable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ex</span></a><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
<span class="c1"># Need to allocate data and params on GPU device</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">input_shape</span></a><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span> <span class="n">dev</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gpu_params</span></a> <span class="o">=</span> <span class="p">[</span><span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="o">*</span><span class="n">p</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span> <span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">params</span></a><span class="p">]</span>
<span class="n">gpu_out</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&quot;forward&quot;</span><span class="p">](</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gpu_params</span></a><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gpu_out</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[25288.63  24984.18  22165.008 24781.422 26253.805 25068.621 24589.043
  24149.98  27014.742 24772.598]]
</pre></div>
</div>
</section>
<section id="summary">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Summary</a><a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p>This tutorial demonstrates how to customize the optimization pipeline for ML models in Apache TVM.
We can easily compose the optimization passes and customize the optimization for different parts
of the computation graph. The flexibility of the optimization pipeline enables us to quickly
iterate the optimization and improve the performance of the model.</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-how-to-tutorials-customize-opt-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/d64d105c8921b2ab908ef001ab382b45/customize_opt.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">customize_opt.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/42304b518561422fdab8c7d8ee640a55/customize_opt.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">customize_opt.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/f69433a4a80715725df90d1386679956/customize_opt.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">customize_opt.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="optimize_llm.html" class="btn btn-neutral float-right" title="Optimize Large Language Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="e2e_opt_model.html" class="btn btn-neutral float-left" title="End-to-End Optimize Model" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2024 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2024 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="../../_static/downloads/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="../../_static/downloads/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>