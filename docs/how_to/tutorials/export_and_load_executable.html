



<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Export and Load Relax Executables &mdash; tvm 0.23.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/downloads/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=61e891cc"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Development Guides" href="../dev/index.html" />
    <link rel="prev" title="Cross Compilation and RPC" href="cross_compilation_and_rpc.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="sidetitle" alt="Documentation Home"> tvm
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/ir_module.html">IRModule</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How To</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="e2e_opt_model.html">End-to-End Optimize Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="customize_opt.html">Customize Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimize_llm.html">Optimize Large Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Export and Load Relax Executables</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-a-torch-mlp-and-convert-to-relax">Prepare a Torch MLP and Convert to Relax</a></li>
<li class="toctree-l2"><a class="reference internal" href="#build-and-export-with-export-library">Build and Export with <code class="docutils literal notranslate"><span class="pre">export_library</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="#load-the-exported-library-and-run-it">Load the Exported Library and Run It</a></li>
<li class="toctree-l2"><a class="reference internal" href="#save-parameters-for-deployment">Save Parameters for Deployment</a></li>
<li class="toctree-l2"><a class="reference internal" href="#loading-and-running-the-exported-model">Loading and Running the Exported Model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploying-to-remote-devices">Deploying to Remote Devices</a></li>
<li class="toctree-l2"><a class="reference internal" href="#faq">FAQ</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dev/index.html">Development Guides</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../arch/index.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/tensor_ir/index.html">TensorIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/relax/index.html">Relax</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/links.html">Other APIs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/security.html">Security Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Export and Load Relax Executables</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/tvm/edit/main/docs/how_to/tutorials/export_and_load_executable.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial can be used interactively with Google Colab! You can also click
<a class="reference internal" href="#sphx-glr-download-how-to-tutorials-export-and-load-executable-py"><span class="std std-ref">here</span></a> to run the Jupyter notebook locally.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/apache/tvm-site/blob/asf-site/docs/_downloads/9b7f75c66e796871ed816f1eddbac0ca/export_and_load_executable.ipynb"><img alt="../../_static/downloads/colab_button.svg" class="align-center" src="../../_static/downloads/colab_button.svg" style="width: 300px;" />
</a>
</div>
<section class="sphx-glr-example-title" id="export-and-load-relax-executables">
<span id="deploy-export-and-load-executable"></span><span id="sphx-glr-how-to-tutorials-export-and-load-executable-py"></span><h1>Export and Load Relax Executables<a class="headerlink" href="#export-and-load-relax-executables" title="Link to this heading"></a></h1>
<p>This tutorial walks through exporting a compiled Relax module to a shared
object, loading it back into the TVM runtime, and running the result either
interactively or from a standalone script. This tutorial demonstrates how
to turn Relax (or imported PyTorch / ONNX) programs into deployable artifacts
using <code class="docutils literal notranslate"><span class="pre">tvm.relax</span></code> APIs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial uses PyTorch as the source format, but the export/load workflow
is the same for ONNX models. For ONNX, use <code class="docutils literal notranslate"><span class="pre">from_onnx(model,</span> <span class="pre">keep_params_in_input=True)</span></code>
instead of <code class="docutils literal notranslate"><span class="pre">from_exported_program()</span></code>, then follow the same steps for building,
exporting, and loading.</p>
</div>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>TVM builds Relax programs into <code class="docutils literal notranslate"><span class="pre">tvm.runtime.Executable</span></code> objects. These
contain VM bytecode, compiled kernels, and constants. By exporting the
executable with <code class="xref py py-meth docutils literal notranslate"><span class="pre">export_library()</span></code>, you obtain a shared library (for
example <code class="docutils literal notranslate"><span class="pre">.so</span></code> on Linux) that can be shipped to another machine, uploaded
via RPC, or loaded back later with the TVM runtime. This tutorial shows the
exact steps end-to-end and explains what files are produced along the way.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="pathlib.Path" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class"><span class="n">Path</span></a>

<span class="k">try</span><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">torch.export</span><span class="w"> </span><span class="kn">import</span> <span class="n">export</span>
<span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
    <span class="n">torch</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore</span>
</pre></div>
</div>
</section>
<section id="prepare-a-torch-mlp-and-convert-to-relax">
<h2>Prepare a Torch MLP and Convert to Relax<a class="headerlink" href="#prepare-a-torch-mlp-and-convert-to-relax" title="Link to this heading"></a></h2>
<p>We start with a small PyTorch MLP so the example remains lightweight. The
model is exported to a <code class="xref py py-class docutils literal notranslate"><span class="pre">torch.export.ExportedProgram</span></code> and then
translated into a Relax <code class="docutils literal notranslate"><span class="pre">IRModule</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tvm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm</span><span class="w"> </span><span class="kn">import</span> <span class="n">relax</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax.frontend.torch</span><span class="w"> </span><span class="kn">import</span> <a href="../../reference/api/python/relax/frontend.html#tvm.relax.frontend.torch.from_exported_program" title="tvm.relax.frontend.torch.from_exported_program" class="sphx-glr-backref-module-tvm-relax-frontend-torch sphx-glr-backref-type-py-function"><span class="n">from_exported_program</span></a>

<span class="c1"># Check dependencies first</span>
<a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/os.html#os.getenv" title="os.getenv" class="sphx-glr-backref-module-os sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">getenv</span></a><span class="p">(</span><span class="s2">&quot;CI&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span>
<a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">HAS_TORCH</span></a> <span class="o">=</span> <span class="n">torch</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">RUN_EXAMPLE</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">HAS_TORCH</span></a> <span class="ow">and</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a>


<span class="k">if</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">HAS_TORCH</span></a><span class="p">:</span>

    <span class="k">class</span><span class="w"> </span><span class="nc">TorchMLP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Flatten</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">28</span> <span class="o">*</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
                <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
            <span class="p">)</span>

        <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>  <span class="c1"># type: ignore[override]</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">net</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="k">else</span><span class="p">:</span>  <span class="c1"># pragma: no cover</span>
    <span class="n">TorchMLP</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore[misc, assignment]</span>

<span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">RUN_EXAMPLE</span></a><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Skip model conversion because PyTorch is unavailable or we are in CI.&quot;</span><span class="p">)</span>

<span class="k">if</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">RUN_EXAMPLE</span></a><span class="p">:</span>
    <span class="n">torch_model</span> <span class="o">=</span> <span class="n">TorchMLP</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">example_args</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">exported_program</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">torch_model</span><span class="p">,</span> <span class="n">example_args</span><span class="p">)</span>

    <span class="n">mod</span> <span class="o">=</span> <a href="../../reference/api/python/relax/frontend.html#tvm.relax.frontend.torch.from_exported_program" title="tvm.relax.frontend.torch.from_exported_program" class="sphx-glr-backref-module-tvm-relax-frontend-torch sphx-glr-backref-type-py-function"><span class="n">from_exported_program</span></a><span class="p">(</span><span class="n">exported_program</span><span class="p">,</span> <span class="n">keep_params_as_input</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># Separate model parameters so they can be bound later (or stored on disk).</span>
    <span class="n">mod</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <a href="../../reference/api/python/relax/frontend.html#tvm.relax.frontend.detach_params" title="tvm.relax.frontend.detach_params" class="sphx-glr-backref-module-tvm-relax-frontend sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">frontend</span><span class="o">.</span><span class="n">detach_params</span></a><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Imported Relax module:&quot;</span><span class="p">)</span>
    <span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>Skip model conversion because PyTorch is unavailable or we are in CI.
</pre></div>
</div>
</section>
<section id="build-and-export-with-export-library">
<h2>Build and Export with <code class="docutils literal notranslate"><span class="pre">export_library</span></code><a class="headerlink" href="#build-and-export-with-export-library" title="Link to this heading"></a></h2>
<p>We build for <code class="docutils literal notranslate"><span class="pre">llvm</span></code> to generate CPU code and then export the resulting
executable. Passing <code class="docutils literal notranslate"><span class="pre">workspace_dir</span></code> keeps the intermediate packaging files,
which is useful to inspect what was produced.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">TARGET</span></a> <span class="o">=</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class"><span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span></a><span class="p">(</span><span class="s2">&quot;llvm&quot;</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/pathlib.html#pathlib.PosixPath" title="pathlib.PosixPath" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ARTIFACT_DIR</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="pathlib.Path" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class"><span class="n">Path</span></a><span class="p">(</span><span class="s2">&quot;relax_export_artifacts&quot;</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir" title="pathlib.Path.mkdir" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-method"><span class="n">ARTIFACT_DIR</span><span class="o">.</span><span class="n">mkdir</span></a><span class="p">(</span><span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="k">if</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">RUN_EXAMPLE</span></a><span class="p">:</span>
    <span class="c1"># Apply the default Relax compilation pipeline before building.</span>
    <span class="n">pipeline</span> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.get_pipeline" title="tvm.relax.get_pipeline" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">get_pipeline</span></a><span class="p">()</span>
    <span class="k">with</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">TARGET</span></a><span class="p">:</span>
        <span class="n">built_mod</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="c1"># Build without params - we&#39;ll pass them at runtime</span>
    <span class="n">executable</span> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.build" title="tvm.relax.build" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">build</span></a><span class="p">(</span><span class="n">built_mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">TARGET</span></a><span class="p">)</span>

    <span class="n">library_path</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/pathlib.html#pathlib.PosixPath" title="pathlib.PosixPath" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ARTIFACT_DIR</span></a> <span class="o">/</span> <span class="s2">&quot;mlp_cpu.so&quot;</span>
    <span class="n">executable</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">library_path</span><span class="p">),</span> <span class="n">workspace_dir</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><a href="https://docs.python.org/3/library/pathlib.html#pathlib.PosixPath" title="pathlib.PosixPath" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ARTIFACT_DIR</span></a><span class="p">))</span>

    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Exported runtime library to: </span><span class="si">{</span><span class="n">library_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># The workspace directory now contains the shared object and supporting files.</span>
    <span class="n">produced_files</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">name</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path.iterdir" title="pathlib.Path.iterdir" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-method"><span class="n">ARTIFACT_DIR</span><span class="o">.</span><span class="n">iterdir</span></a><span class="p">())</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Artifacts saved:&quot;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">name</span> <span class="ow">in</span> <span class="n">produced_files</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  - </span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="c1"># Generated files:</span>
    <span class="c1">#   - ``mlp_cpu.so``: The main deployable shared library containing VM bytecode,</span>
    <span class="c1">#     compiled kernels, and constants. Note: Since parameters are passed at runtime,</span>
    <span class="c1">#     you will also need to save a separate parameters file (see next section).</span>
    <span class="c1">#   - Intermediate object files (``devc.o``, ``lib0.o``, etc.) are kept in the</span>
    <span class="c1">#     workspace for inspection but are not required for deployment.</span>
    <span class="c1">#</span>
    <span class="c1">#   Note: Additional files like ``*.params``, ``*.metadata.json``, or ``*.imports``</span>
    <span class="c1">#   may appear in specific configurations but are typically embedded into the</span>
    <span class="c1">#   shared library or only generated when needed.</span>
</pre></div>
</div>
</section>
<section id="load-the-exported-library-and-run-it">
<h2>Load the Exported Library and Run It<a class="headerlink" href="#load-the-exported-library-and-run-it" title="Link to this heading"></a></h2>
<p>Once the shared object is produced, we can reload it back into the TVM runtime
on any machine with a compatible instruction set. The Relax VM consumes the
runtime module directly.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">RUN_EXAMPLE</span></a><span class="p">:</span>
    <span class="n">loaded_rt_mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">library_path</span><span class="p">))</span>
    <span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">vm</span> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span></a><span class="p">(</span><span class="n">loaded_rt_mod</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>

    <span class="c1"># Prepare input data</span>
    <span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">vm_input</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">input_tensor</span><span class="o">.</span><span class="n">numpy</span><span class="p">(),</span> <span class="n">dev</span><span class="p">)</span>

    <span class="c1"># Prepare parameters (allocate on target device)</span>
    <span class="n">vm_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">params</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">]]</span>

    <span class="c1"># Run inference: pass input data followed by all parameters</span>
    <span class="n">tvm_output</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">](</span><span class="n">vm_input</span><span class="p">,</span> <span class="o">*</span><span class="n">vm_params</span><span class="p">)</span>

    <span class="c1"># TVM returns Array objects for tuple outputs, access via indexing.</span>
    <span class="c1"># For models imported from PyTorch, outputs are typically tuples (even for single outputs).</span>
    <span class="c1"># For ONNX models, outputs may be a single Tensor directly.</span>
    <span class="n">result_tensor</span> <span class="o">=</span> <span class="n">tvm_output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tvm_output</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="k">else</span> <span class="n">tvm_output</span>

    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;VM output shape:&quot;</span><span class="p">,</span> <span class="n">result_tensor</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;VM output type:&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">tvm_output</span><span class="p">),</span> <span class="s2">&quot;-&gt;&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="p">(</span><span class="n">result_tensor</span><span class="p">))</span>

    <span class="c1"># You can still inspect the executable after reloading.</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Executable stats:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">loaded_rt_mod</span><span class="p">[</span><span class="s2">&quot;stats&quot;</span><span class="p">]())</span>
</pre></div>
</div>
</section>
<section id="save-parameters-for-deployment">
<h2>Save Parameters for Deployment<a class="headerlink" href="#save-parameters-for-deployment" title="Link to this heading"></a></h2>
<p>Since parameters are passed at runtime (not embedded in the <code class="docutils literal notranslate"><span class="pre">.so</span></code>), we must
save them separately for deployment. This is a required step to use the model
on other machines or in standalone scripts.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="k">if</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">RUN_EXAMPLE</span></a><span class="p">:</span>
    <span class="c1"># Save parameters to disk</span>
    <span class="n">params_path</span> <span class="o">=</span> <a href="https://docs.python.org/3/library/pathlib.html#pathlib.PosixPath" title="pathlib.PosixPath" class="sphx-glr-backref-module-pathlib sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ARTIFACT_DIR</span></a> <span class="o">/</span> <span class="s2">&quot;model_params.npz&quot;</span>
    <span class="n">param_arrays</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s2">&quot;p_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">:</span> <span class="n">p</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">])}</span>
    <span class="n">np</span><span class="o">.</span><span class="n">savez</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">params_path</span><span class="p">),</span> <span class="o">**</span><span class="n">param_arrays</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Saved parameters to: </span><span class="si">{</span><span class="n">params_path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Note: Alternatively, you can embed parameters directly into the ``.so`` to</span>
<span class="c1"># create a single-file deployment. Use ``keep_params_as_input=False`` when</span>
<span class="c1"># importing from PyTorch:</span>
<span class="c1">#</span>
<span class="c1"># .. code-block:: python</span>
<span class="c1">#</span>
<span class="c1">#    mod = from_exported_program(exported_program, keep_params_as_input=False)</span>
<span class="c1">#    # Parameters are now embedded as constants in the module</span>
<span class="c1">#    executable = relax.build(built_mod, target=TARGET)</span>
<span class="c1">#    # Runtime: vm[&quot;main&quot;](input)  # No need to pass params!</span>
<span class="c1">#</span>
<span class="c1"># This creates a single-file deployment (only the ``.so`` is needed), but you</span>
<span class="c1"># lose the flexibility to swap parameters without recompiling. For most</span>
<span class="c1"># production workflows, separating code and parameters (as shown above) is</span>
<span class="c1"># preferred for flexibility.</span>
</pre></div>
</div>
</section>
<section id="loading-and-running-the-exported-model">
<h2>Loading and Running the Exported Model<a class="headerlink" href="#loading-and-running-the-exported-model" title="Link to this heading"></a></h2>
<p>To use the exported model on another machine or in a standalone script, you need
to load both the <code class="docutils literal notranslate"><span class="pre">.so</span></code> library and the parameters file. Here’s a complete example
of how to reload and run the model. Save this as <code class="docutils literal notranslate"><span class="pre">run_mlp.py</span></code>:</p>
<p>To make it executable from the command line:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>chmod<span class="w"> </span>+x<span class="w"> </span>run_mlp.py
./run_mlp.py<span class="w">  </span><span class="c1"># Run it like a regular program</span>
</pre></div>
</div>
<p>Complete script:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python3</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">tvm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm</span><span class="w"> </span><span class="kn">import</span> <span class="n">relax</span>

<span class="c1"># Step 1: Load the compiled library</span>
<span class="n">lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="s2">&quot;relax_export_artifacts/mlp_cpu.so&quot;</span><span class="p">)</span>

<span class="c1"># Step 2: Create Virtual Machine</span>
<span class="n">device</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">vm</span> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span></a><span class="p">(</span><span class="n">lib</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="c1"># Step 3: Load parameters from the .npz file</span>
<span class="n">params_npz</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;relax_export_artifacts/model_params.npz&quot;</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">params_npz</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;p_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">],</span> <span class="n">device</span><span class="p">)</span>
          <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params_npz</span><span class="p">))]</span>

<span class="c1"># Step 4: Prepare input data</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">device</span><span class="p">)</span>

<span class="c1"># Step 5: Run inference (pass input followed by all parameters)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">](</span><span class="n">input_tensor</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">)</span>

<span class="c1"># Step 6: Extract result (output may be tuple or single Tensor)</span>
<span class="c1"># PyTorch models typically return tuples, ONNX models may return a single Tensor</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="p">(</span><span class="nb">tuple</span><span class="p">,</span> <span class="nb">list</span><span class="p">))</span> <span class="k">else</span> <span class="n">output</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction shape:&quot;</span><span class="p">,</span> <span class="n">result</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted class:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">result</span><span class="o">.</span><span class="n">numpy</span><span class="p">()))</span>
</pre></div>
</div>
<p><strong>Running on GPU:</strong>
To run on GPU instead of CPU, make the following changes:</p>
<ol class="arabic">
<li><p><strong>Compile for GPU</strong> (earlier in the tutorial, around line 112):
.. code-block:: python</p>
<blockquote>
<div><p>TARGET = tvm.target.Target(“cuda”)  # Change from “llvm” to “cuda”</p>
</div></blockquote>
</li>
<li><p><strong>Use GPU device in the script</strong>:
.. code-block:: python</p>
<blockquote>
<div><p>device = tvm.cuda(0)  # Use CUDA device instead of CPU
vm = relax.VirtualMachine(lib, device)</p>
<p># Load parameters to GPU
params = [tvm.runtime.tensor(params_npz[f”p_{i}”], device)  # Note: device parameter</p>
<blockquote>
<div><p>for i in range(len(params_npz))]</p>
</div></blockquote>
<p># Prepare input on GPU
input_tensor = tvm.runtime.tensor(data, device)  # Note: device parameter</p>
</div></blockquote>
<p>The rest of the script remains the same. All tensors (parameters and inputs)
must be allocated on the same device (GPU) as the compiled model.</p>
</li>
</ol>
<p><strong>Deployment Checklist:</strong>
When moving to another host (via RPC or SCP), you must copy <strong>both</strong> files:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mlp_cpu.so</span></code> (or <code class="docutils literal notranslate"><span class="pre">mlp_cuda.so</span></code> for GPU) - The compiled model code</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model_params.npz</span></code> - The model parameters (serialized as NumPy arrays)</p></li>
</ol>
</div></blockquote>
<p>The remote machine needs both files in the same directory. The script above
assumes they are in <code class="docutils literal notranslate"><span class="pre">relax_export_artifacts/</span></code> relative to the script location.
Adjust the paths as needed for your deployment. For GPU deployment, ensure the
target machine has compatible CUDA drivers and the model was compiled for the
same GPU architecture.</p>
</section>
<section id="deploying-to-remote-devices">
<h2>Deploying to Remote Devices<a class="headerlink" href="#deploying-to-remote-devices" title="Link to this heading"></a></h2>
<p>To deploy the exported model to a remote ARM Linux device (e.g., Raspberry Pi),
you can use TVM’s RPC mechanism to cross-compile, upload, and run the model
remotely. This workflow is useful when:</p>
<ul class="simple">
<li><p>The target device has limited resources for compilation</p></li>
<li><p>You want to fine-tune performance by running on the actual hardware</p></li>
<li><p>You need to deploy to embedded devices</p></li>
</ul>
<p>See <a class="reference internal" href="cross_compilation_and_rpc.html"><span class="doc">cross_compilation_and_rpc</span></a>
for a comprehensive guide on:</p>
<ul class="simple">
<li><p>Setting up TVM runtime on the remote device</p></li>
<li><p>Starting an RPC server on the device</p></li>
<li><p>Cross-compiling for ARM targets (e.g., <code class="docutils literal notranslate"><span class="pre">llvm</span> <span class="pre">-mtriple=aarch64-linux-gnu</span></code>)</p></li>
<li><p>Uploading exported libraries via RPC</p></li>
<li><p>Running inference remotely</p></li>
</ul>
<p>Quick example for ARM deployment workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tvm.rpc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">rpc</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm</span><span class="w"> </span><span class="kn">import</span> <span class="n">relax</span>

<span class="c1"># Step 1: Cross-compile for ARM target (on local machine)</span>
<a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">TARGET</span></a> <span class="o">=</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class"><span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span></a><span class="p">(</span><span class="s2">&quot;llvm -mtriple=aarch64-linux-gnu&quot;</span><span class="p">)</span>
<span class="n">executable</span> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.build" title="tvm.relax.build" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">build</span></a><span class="p">(</span><span class="n">built_mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">TARGET</span></a><span class="p">)</span>
<span class="n">executable</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="s2">&quot;mlp_arm.so&quot;</span><span class="p">)</span>

<span class="c1"># Step 2: Connect to remote device RPC server</span>
<span class="n">remote</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">connect</span><span class="p">(</span><span class="s2">&quot;192.168.1.100&quot;</span><span class="p">,</span> <span class="mi">9090</span><span class="p">)</span>  <span class="c1"># Device IP and RPC port</span>

<span class="c1"># Step 3: Upload the compiled library and parameters</span>
<span class="n">remote</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span><span class="s2">&quot;mlp_arm.so&quot;</span><span class="p">)</span>
<span class="n">remote</span><span class="o">.</span><span class="n">upload</span><span class="p">(</span><span class="s2">&quot;model_params.npz&quot;</span><span class="p">)</span>

<span class="c1"># Step 4: Load and run on remote device</span>
<span class="n">lib</span> <span class="o">=</span> <span class="n">remote</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="s2">&quot;mlp_arm.so&quot;</span><span class="p">)</span>
<span class="n">vm</span> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span></a><span class="p">(</span><span class="n">lib</span><span class="p">,</span> <span class="n">remote</span><span class="o">.</span><span class="n">cpu</span><span class="p">())</span>
<span class="c1"># ... prepare input and params, then run inference</span>
</pre></div>
</div>
<p>The key difference is using an ARM target triple during compilation and
uploading files via RPC instead of copying them directly.</p>
</section>
<section id="faq">
<h2>FAQ<a class="headerlink" href="#faq" title="Link to this heading"></a></h2>
<dl class="simple">
<dt><strong>Can I run the ``.so`` as a standalone executable (like ``./mlp_cpu.so``)?</strong></dt><dd><p>No. The <code class="docutils literal notranslate"><span class="pre">.so</span></code> file is a shared library, not a standalone executable binary.
You cannot run it directly from the terminal. It must be loaded through a TVM
runtime program (as shown in the “Loading and Running” section above). The
<code class="docutils literal notranslate"><span class="pre">.so</span></code> bundles VM bytecode and compiled kernels, but still requires the TVM
runtime to execute.</p>
</dd>
<dt><strong>Which devices can run the exported library?</strong></dt><dd><p>The target must match the ISA you compiled for (<code class="docutils literal notranslate"><span class="pre">llvm</span></code> in this example).
As long as the target triple, runtime ABI, and available devices line up,
you can move the artifact between machines. For heterogeneous builds (CPU
plus GPU), ship the extra device libraries as well.</p>
</dd>
<dt><strong>What about the ``.params`` and ``metadata.json`` files?</strong></dt><dd><p>These auxiliary files are only generated in specific configurations. In this
tutorial, since we pass parameters at runtime, they are not generated. When
they do appear, they may be kept alongside the <code class="docutils literal notranslate"><span class="pre">.so</span></code> for inspection, but
the essential content is typically embedded in the shared object itself, so
deploying the <code class="docutils literal notranslate"><span class="pre">.so</span></code> alone is usually sufficient.</p>
</dd>
</dl>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-how-to-tutorials-export-and-load-executable-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/9b7f75c66e796871ed816f1eddbac0ca/export_and_load_executable.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">export_and_load_executable.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/0f8c78d23feac867e3c6c4f11ff760d4/export_and_load_executable.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">export_and_load_executable.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/bc875d02d5382abc9ea5fb9eb2c1de2c/export_and_load_executable.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">export_and_load_executable.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../dev/index.html" class="btn btn-neutral float-right" title="Development Guides" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="cross_compilation_and_rpc.html" class="btn btn-neutral float-left" title="Cross Compilation and RPC" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2024 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2024 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="../../_static/downloads/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="../../_static/downloads/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>