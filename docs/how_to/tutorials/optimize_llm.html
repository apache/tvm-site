



<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Optimize Large Language Model &mdash; tvm 0.22.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/downloads/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=feaa0556"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Cross Compilation and RPC" href="cross_compilation_and_rpc.html" />
    <link rel="prev" title="Customize Optimization" href="customize_opt.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="sidetitle" alt="Documentation Home"> tvm
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/ir_module.html">IRModule</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How To</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="e2e_opt_model.html">End-to-End Optimize Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="customize_opt.html">Customize Optimization</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimize Large Language Model</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#review-overall-flow">Review Overall Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="#construct-the-model-architecture">Construct the model architecture</a></li>
<li class="toctree-l2"><a class="reference internal" href="#export-the-model-to-relax-irmodule">Export the model to Relax IRModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="#define-optimization-pipeline">Define Optimization Pipeline</a></li>
<li class="toctree-l2"><a class="reference internal" href="#prepare-the-model-weights">Prepare the model weights</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-the-compiled-model">Deploy the compiled model</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tokenization">Tokenization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-the-kvcache">Create the KVCache</a></li>
<li class="toctree-l3"><a class="reference internal" href="#embedding">Embedding</a></li>
<li class="toctree-l3"><a class="reference internal" href="#prefill">Prefill</a></li>
<li class="toctree-l3"><a class="reference internal" href="#decode">Decode</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dev/index.html">Development Guides</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../arch/index.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/tensor_ir/index.html">TensorIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/relax/index.html">Relax</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/links.html">Other APIs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/security.html">Security Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>Optimize Large Language Model</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/tvm/edit/main/docs/how_to/tutorials/optimize_llm.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial can be used interactively with Google Colab! You can also click
<a class="reference internal" href="#sphx-glr-download-how-to-tutorials-optimize-llm-py"><span class="std std-ref">here</span></a> to run the Jupyter notebook locally.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/apache/tvm-site/blob/asf-site/docs/_downloads/ab03cb35cd7f92b5425a8974fcab921a/optimize_llm.ipynb"><img alt="../../_static/downloads/colab_button.svg" class="align-center" src="../../_static/downloads/colab_button.svg" style="width: 300px;" />
</a>
</div>
<section class="sphx-glr-example-title" id="optimize-large-language-model">
<span id="opt-llm"></span><span id="sphx-glr-how-to-tutorials-optimize-llm-py"></span><h1>Optimize Large Language Model<a class="headerlink" href="#optimize-large-language-model" title="Link to this heading"></a></h1>
<p>As large language models (LLMs) have become a popular research topic in many different fields,
deploying them on cloud and edge devices has become a challenging task. In this tutorial, we will
demonstrate how to optimize a large language model using Apache TVM. We will use a pre-trained
TinyLlama model from Hugging Face and deploy it on various devices.</p>
<section id="review-overall-flow">
<h2>Review Overall Flow<a class="headerlink" href="#review-overall-flow" title="Link to this heading"></a></h2>
<figure class="align-center">
<a class="reference internal image-reference" href="../../_static/downloads/tvm_overall_flow.svg"><img alt="../../_static/downloads/tvm_overall_flow.svg" src="../../_static/downloads/tvm_overall_flow.svg" style="width: 80%;" />
</a>
</figure>
<p>The overall flow consists of the following steps:</p>
<ul class="simple">
<li><p><strong>Construct or Import a Model</strong>: Construct a neural network model or import a pre-trained
model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains
all the information needed for compilation, including high-level Relax functions for
computational graph, and low-level TensorIR functions for tensor program.</p></li>
<li><p><strong>Perform Composable Optimizations</strong>: Perform a series of optimization transformations,
such as graph optimizations, tensor program optimizations, and library dispatching.</p></li>
<li><p><strong>Build and Universal Deployment</strong>: Build the optimized model to a deployable module to the
universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.</p></li>
</ul>
</section>
<section id="construct-the-model-architecture">
<h2>Construct the model architecture<a class="headerlink" href="#construct-the-model-architecture" title="Link to this heading"></a></h2>
<p>We will use a pre-trained TinyLlama model from Hugging Face. However, usually we only load the
pre-trained weight from Hugging Face but not the model architecture. We need to construct the
model architecture by ourselves. Apache TVM prepares a PyTorch-liked API to construct the model
architecture. We can use the API to construct the model architecture.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">dataclasses</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">enum</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pathlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">pprint</span><span class="w"> </span><span class="kn">import</span> <a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <a href="https://docs.python.org/3/library/typing.html#typing.List" title="typing.List" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">List</span></a><span class="p">,</span> <a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="typing.Optional" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><span class="n">Optional</span></a>

<span class="kn">import</span><span class="w"> </span><span class="nn">tvm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm</span><span class="w"> </span><span class="kn">import</span> <span class="n">dlight</span><span class="p">,</span> <span class="n">relax</span><span class="p">,</span> <span class="n">te</span><span class="p">,</span> <span class="n">tir</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax</span><span class="w"> </span><span class="kn">import</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.register_pipeline" title="tvm.relax.register_pipeline" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">register_pipeline</span></a>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax.frontend</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax.frontend.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">op</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax.frontend.nn.llm.kv_cache</span><span class="w"> </span><span class="kn">import</span> <span class="n">PagedKVCache</span><span class="p">,</span> <span class="n">TIRPagedKVCache</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.runtime</span><span class="w"> </span><span class="kn">import</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a>
</pre></div>
</div>
<p>First, we need to define the model configuration. The configuration includes the key parameters
of the model, such as hidden size, intermediate size, etc. Here for convenience, we define a
constant config specially for the TinyLlama model.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nd">@dataclasses</span><span class="o">.</span><span class="n">dataclass</span>
<span class="k">class</span><span class="w"> </span><span class="nc">LlamaConfig</span><span class="p">:</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>
    <span class="n">intermediate_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5632</span>
    <span class="n">num_attention_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="n">num_hidden_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">22</span>
    <span class="n">rms_norm_eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-05</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32000</span>
    <span class="n">rope_theta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span>
    <span class="n">context_window_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>
    <span class="n">prefill_chunk_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span>
    <span class="n">num_key_value_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span>  <span class="c1"># hidden_size // num_attention_heads</span>


<span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a> <span class="o">=</span> <a href="../../reference/api/python/target.html#tvm.target.Target.from_device" title="tvm.target.Target.from_device" class="sphx-glr-backref-module-tvm-target-Target sphx-glr-backref-type-py-method"><span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span><span class="o">.</span><span class="n">from_device</span></a><span class="p">(</span><span class="n">dev</span><span class="p">)</span>
</pre></div>
</div>
<p>Next, we define the RoPE mode of the Paged KV cache. The RoPE mode is used to apply the
Relative Positional Encoding (RoPE) to the query and key tensors. The RoPE mode can be set to
<cite>NONE</cite>, <cite>NORMAL</cite>, or <cite>INLINE</cite>. If the RoPE mode is <cite>NONE</cite>, the KV cache will not apply RoPE to
the query and key tensors. If the RoPE mode is <cite>NORMAL</cite>, RoPE will be applied to the key tensor
before adding the key tensor to the cache. If the RoPE mode is <cite>INLINE</cite>, RoPE will be applied to
the query and key tensors in the attention kernel on-the-fly.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RopeMode</span><span class="p">(</span><a href="https://docs.python.org/3/library/enum.html#enum.IntEnum" title="enum.IntEnum" class="sphx-glr-backref-module-enum sphx-glr-backref-type-py-class"><span class="n">enum</span><span class="o">.</span><span class="n">IntEnum</span></a><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;The RoPE mode of the Paged KV cache.</span>
<span class="sd">    If it is none, the KV cache will not apply RoPE to q and k.</span>
<span class="sd">    If it is normal, RoPE will be applied to k before adding k to cache.</span>
<span class="sd">    Otherwise, RoPE will be applied to q/k in attention kernel on-the-fly.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">NONE</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">NORMAL</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">INLINE</span> <span class="o">=</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Secondly, we define the model architecture. The model architecture consists of three parts:</p>
<ul class="simple">
<li><p>Embedding layer: The embedding layer converts the input token IDs to the hidden states.</p></li>
<li><p>Decoder layers: The decoder layers are the core of the model. Each decoder layer consists of
a self-attention layer and a feed-forward network (FFN) layer.</p></li>
<li><p>Output layer: The output layer converts the hidden states to the logits.</p></li>
</ul>
<p>First we define the FFN layer. Note that the following FFN layer is optimized implementation
where we fuse the gate and up projection into one kernel.
The naive implementation of FFN layer is: <code class="docutils literal notranslate"><span class="pre">FFN(x)</span> <span class="pre">=</span> <span class="pre">down_proj(silu(gate(x))</span> <span class="pre">*</span> <span class="pre">up(x))</span></code>
We could combine the <code class="docutils literal notranslate"><span class="pre">gate</span></code> and <code class="docutils literal notranslate"><span class="pre">up</span></code> projection into one kernel for better performance.
The optimized implementation is:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">concat_x</span> <span class="o">=</span> <span class="n">gate_up</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">gate_x</span><span class="p">,</span> <span class="n">up_x</span> <span class="o">=</span> <span class="n">split</span><span class="p">(</span><span class="n">concat_x</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">FFN</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">=</span> <span class="n">down_proj</span><span class="p">(</span><span class="n">silu</span><span class="p">(</span><span class="n">gate_x</span><span class="p">)</span> <span class="o">*</span> <span class="n">up_x</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LlamaFFN</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LlamaConfig</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gate_up_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">out_features</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">concat_x1_x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">gate_up_proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">concat_x1_x2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">down_proj</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x2</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we define the self-attention layer. The self-attention layer consists of three parts:</p>
<ul class="simple">
<li><p>QKV projection: The QKV projection converts the input hidden states to the query, key, and
value tensors.</p></li>
<li><p>Attention: The attention layer computes the attention scores and applies the softmax
operation.</p></li>
<li><p>Output projection: The output projection converts the attention output to the hidden states.</p></li>
</ul>
<p>We perform optimizations on the different parts of the self-attention layer:</p>
<ul class="simple">
<li><p>QKV projection: We leverage the horizontal fusion on QKV projection and fuse them into one
kernel.</p></li>
<li><p>Attention: We leverage the horizontal fusion on attention and fuse the QKV projection and</p></li>
</ul>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LlamaAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>  <span class="c1"># pylint: disable=too-many-instance-attributes</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LlamaConfig</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_q_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="c1"># horizontal fusion on QKV projection</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span>
            <span class="n">in_features</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
            <span class="n">out_features</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_q_heads</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_q_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">:</span> <span class="n">PagedKVCache</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">d</span><span class="p">,</span> <span class="n">h_q</span><span class="p">,</span> <span class="n">h_kv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_q_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_kv_heads</span>
        <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">hidden_states</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># QKV Projection</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qkv_proj</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">qkv</span><span class="p">,</span> <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">h_q</span> <span class="o">+</span> <span class="n">h_kv</span> <span class="o">+</span> <span class="n">h_kv</span><span class="p">,</span> <span class="n">d</span><span class="p">))</span>
        <span class="c1"># Attention</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
            <span class="n">paged_kv_cache</span><span class="o">.</span><span class="n">attention_with_fused_qkv</span><span class="p">(</span>
                <span class="n">layer_id</span><span class="p">,</span> <span class="n">qkv</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_q_heads</span><span class="p">,</span> <span class="n">sm_scale</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="o">**-</span><span class="mf">0.5</span>
            <span class="p">),</span>
            <span class="p">(</span><span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">h_q</span> <span class="o">*</span> <span class="n">d</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="c1"># Output Projection</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">o_proj</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
<p>Finally, we define the model architecture with FFN and self-attention layers.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">LlamaDecoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LlamaConfig</span><span class="p">):</span>
        <span class="n">rms_norm_eps</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span> <span class="o">=</span> <span class="n">LlamaAttention</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span> <span class="o">=</span> <span class="n">LlamaFFN</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">:</span> <span class="n">PagedKVCache</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">self_attn</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">input_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">),</span> <span class="n">paged_kv_cache</span><span class="p">,</span> <span class="n">layer_id</span>
        <span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mlp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">post_attention_layernorm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LlamaModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LlamaConfig</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">%</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">==</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_tokens</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
            <span class="p">[</span><span class="n">LlamaDecoderLayer</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">norm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">RMSNorm</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">rms_norm_eps</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embed</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">:</span> <span class="n">PagedKVCache</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">input_embed</span>
        <span class="k">for</span> <span class="n">layer_id</span><span class="p">,</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">):</span>
            <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">,</span> <span class="n">layer_id</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>


<span class="k">class</span><span class="w"> </span><span class="nc">LlamaForCasualLM</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">LlamaConfig</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">LlamaModel</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_hidden_layers</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_attention_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">num_key_value_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">head_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">hidden_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">rope_theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="typing.Optional" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-data"><span class="n">Optional</span></a><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">dtype</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">embed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_ids</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">embed_tokens</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_logits</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lm_head</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">logits</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="s2">&quot;float32&quot;</span><span class="p">:</span>
            <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">prefill</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embed</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">:</span> <span class="n">PagedKVCache</span><span class="p">):</span>
        <span class="k">def</span><span class="w"> </span><span class="nf">_index</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <a href="../../reference/api/python/te.html#tvm.te.Tensor" title="tvm.te.Tensor" class="sphx-glr-backref-module-tvm-te sphx-glr-backref-type-py-class"><span class="n">te</span><span class="o">.</span><span class="n">Tensor</span></a><span class="p">):</span>  <span class="c1"># x[:-1,:]</span>
            <span class="n">b</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span>
            <span class="k">return</span> <a href="../../reference/api/python/te.html#tvm.te.compute" title="tvm.te.compute" class="sphx-glr-backref-module-tvm-te sphx-glr-backref-type-py-function"><span class="n">te</span><span class="o">.</span><span class="n">compute</span></a><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">k</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">)</span>

        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_embed</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">op</span><span class="o">.</span><span class="n">tensor_expr_op</span><span class="p">(</span><span class="n">_index</span><span class="p">,</span> <span class="n">name_hint</span><span class="o">=</span><span class="s2">&quot;index&quot;</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="n">hidden_states</span><span class="p">])</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_logits</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">paged_kv_cache</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_embed</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">:</span> <span class="n">PagedKVCache</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">input_embed</span><span class="p">,</span> <span class="n">paged_kv_cache</span><span class="p">)</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_logits</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">logits</span><span class="p">,</span> <span class="n">paged_kv_cache</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">create_tir_paged_kv_cache</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">max_batch_size</span><span class="p">:</span> <a href="../../reference/api/python/tir/tir.html#tvm.tir.Var" title="tvm.tir.Var" class="sphx-glr-backref-module-tvm-tir sphx-glr-backref-type-py-class"><span class="n">tir</span><span class="o">.</span><span class="n">Var</span></a><span class="p">,</span>
        <span class="n">max_total_seq_len</span><span class="p">:</span> <a href="../../reference/api/python/tir/tir.html#tvm.tir.Var" title="tvm.tir.Var" class="sphx-glr-backref-module-tvm-tir sphx-glr-backref-type-py-class"><span class="n">tir</span><span class="o">.</span><span class="n">Var</span></a><span class="p">,</span>
        <span class="n">prefill_chunk_size</span><span class="p">:</span> <a href="../../reference/api/python/tir/tir.html#tvm.tir.Var" title="tvm.tir.Var" class="sphx-glr-backref-module-tvm-tir sphx-glr-backref-type-py-class"><span class="n">tir</span><span class="o">.</span><span class="n">Var</span></a><span class="p">,</span>
        <span class="n">page_size</span><span class="p">:</span> <a href="../../reference/api/python/tir/tir.html#tvm.tir.Var" title="tvm.tir.Var" class="sphx-glr-backref-module-tvm-tir sphx-glr-backref-type-py-class"><span class="n">tir</span><span class="o">.</span><span class="n">Var</span></a><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">PagedKVCache</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">TIRPagedKVCache</span><span class="p">(</span>
            <span class="n">attn_kind</span><span class="o">=</span><span class="s2">&quot;mha&quot;</span><span class="p">,</span>
            <span class="n">max_batch_size</span><span class="o">=</span><span class="n">max_batch_size</span><span class="p">,</span>
            <span class="n">max_total_seq_len</span><span class="o">=</span><span class="n">max_total_seq_len</span><span class="p">,</span>
            <span class="n">prefill_chunk_size</span><span class="o">=</span><span class="n">prefill_chunk_size</span><span class="p">,</span>
            <span class="n">page_size</span><span class="o">=</span><span class="n">page_size</span><span class="p">,</span>
            <span class="n">support_sliding_window</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">layer_partition</span><span class="o">=</span><a href="../../reference/api/python/relax/relax.html#tvm.relax.ShapeExpr" title="tvm.relax.ShapeExpr" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">ShapeExpr</span></a><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">]),</span>
            <span class="n">num_hidden_layers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_hidden_layers</span><span class="p">,</span>
            <span class="n">num_attention_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_attention_heads</span><span class="p">,</span>
            <span class="n">num_key_value_heads</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_key_value_heads</span><span class="p">,</span>
            <span class="n">qk_head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="n">v_head_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="n">mla_original_qk_head_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">mla_original_v_head_dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
            <span class="n">rope_mode</span><span class="o">=</span><span class="n">RopeMode</span><span class="o">.</span><span class="n">NORMAL</span><span class="p">,</span>
            <span class="n">rope_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
            <span class="n">rope_theta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">rope_theta</span><span class="p">,</span>
            <span class="n">rope_scaling</span><span class="o">=</span><span class="p">{},</span>
            <span class="n">rope_ext_factors</span><span class="o">=</span><a href="../../reference/api/python/relax/relax.html#tvm.relax.PrimValue" title="tvm.relax.PrimValue" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">PrimValue</span></a><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="n">rotary_dim</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span>
            <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span>
            <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a><span class="o">=</span><a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a><span class="p">,</span>
            <span class="n">enable_disaggregation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">get_default_spec</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">mod_spec</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;embed&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;input_ids&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="s2">&quot;seq_len&quot;</span><span class="p">],</span> <span class="s2">&quot;int32&quot;</span><span class="p">),</span>
                <span class="s2">&quot;$&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;param_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;packed&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;effect_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">},</span>
            <span class="s2">&quot;prefill&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;input_embed&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;seq_len&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                <span class="s2">&quot;paged_kv_cache&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Object</span><span class="p">(</span><span class="n">object_type</span><span class="o">=</span><span class="n">PagedKVCache</span><span class="p">),</span>
                <span class="s2">&quot;$&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;param_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;packed&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;effect_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">},</span>
            <span class="s2">&quot;decode&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;input_embed&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
                <span class="s2">&quot;paged_kv_cache&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Object</span><span class="p">(</span><span class="n">object_type</span><span class="o">=</span><span class="n">PagedKVCache</span><span class="p">),</span>
                <span class="s2">&quot;$&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;param_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;packed&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;effect_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">},</span>
            <span class="s2">&quot;create_tir_paged_kv_cache&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;max_batch_size&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="s2">&quot;max_total_seq_len&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="s2">&quot;prefill_chunk_size&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="s2">&quot;page_size&quot;</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
                <span class="s2">&quot;$&quot;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s2">&quot;param_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
                    <span class="s2">&quot;effect_mode&quot;</span><span class="p">:</span> <span class="s2">&quot;none&quot;</span><span class="p">,</span>
                <span class="p">},</span>
            <span class="p">},</span>
        <span class="p">}</span>
        <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">ModuleSpec</span><span class="o">.</span><span class="n">from_raw</span><span class="p">(</span><span class="n">mod_spec</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="export-the-model-to-relax-irmodule">
<h2>Export the model to Relax IRModule<a class="headerlink" href="#export-the-model-to-relax-irmodule" title="Link to this heading"></a></h2>
<p>After defining the model architecture, we can export the model to the Relax IRModule.
For demonstration, we only show the part of the model architecture. and parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">model_config</span> <span class="o">=</span> <span class="n">LlamaConfig</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LlamaForCasualLM</span><span class="p">(</span><span class="n">model_config</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">)</span>
<span class="n">mod</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">named_params</span></a> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">export_tvm</span><span class="p">(</span><span class="n">spec</span><span class="o">=</span><span class="n">model</span><span class="o">.</span><span class="n">get_default_spec</span><span class="p">())</span>
<span class="n">prefill_str</span> <span class="o">=</span> <span class="n">mod</span><span class="p">[</span><span class="s2">&quot;prefill&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">script</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">prefill_str</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)[</span><span class="mi">3</span><span class="p">:</span><span class="mi">20</span><span class="p">],</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Only show the first 10 lines for demonstration</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;        ...&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Parameters:&quot;</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/pprint.html#pprint.pprint" title="pprint.pprint" class="sphx-glr-backref-module-pprint sphx-glr-backref-type-py-function"><span class="n">pprint</span></a><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">named_params</span></a><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>  <span class="c1"># Only show the first 5 parameters for demonstration</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>@R.function
def prefill(input_embed: R.Tensor((1, &quot;seq_len&quot;, 2048), dtype=&quot;float16&quot;), paged_kv_cache: R.Object, packed_params: R.Tuple(R.Tensor((32000, 2048), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2560, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 2048), dtype=&quot;float16&quot;), R.Tensor((11264, 2048), dtype=&quot;float16&quot;), R.Tensor((2048, 5632), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((2048,), dtype=&quot;float16&quot;), R.Tensor((32000, 2048), dtype=&quot;float16&quot;))) -&gt; R.Tuple(R.Tensor((1, 1, 32000), dtype=&quot;float32&quot;), R.Object):
    seq_len = T.int64()
    R.func_attr({&quot;num_input&quot;: 2})
    with R.dataflow():
        model_embed_tokens_weight1: R.Tensor((32000, 2048), dtype=&quot;float16&quot;) = packed_params[0]
        model_layers_0_self_attn_qkv_proj_weight1: R.Tensor((2560, 2048), dtype=&quot;float16&quot;) = packed_params[1]
        model_layers_0_self_attn_o_proj_weight1: R.Tensor((2048, 2048), dtype=&quot;float16&quot;) = packed_params[2]
        model_layers_0_mlp_gate_up_proj_weight1: R.Tensor((11264, 2048), dtype=&quot;float16&quot;) = packed_params[3]
        model_layers_0_mlp_down_proj_weight1: R.Tensor((2048, 5632), dtype=&quot;float16&quot;) = packed_params[4]
        model_layers_0_input_layernorm_weight1: R.Tensor((2048,), dtype=&quot;float16&quot;) = packed_params[5]
        model_layers_0_post_attention_layernorm_weight1: R.Tensor((2048,), dtype=&quot;float16&quot;) = packed_params[6]
        model_layers_1_self_attn_qkv_proj_weight1: R.Tensor((2560, 2048), dtype=&quot;float16&quot;) = packed_params[7]
        model_layers_1_self_attn_o_proj_weight1: R.Tensor((2048, 2048), dtype=&quot;float16&quot;) = packed_params[8]
        model_layers_1_mlp_gate_up_proj_weight1: R.Tensor((11264, 2048), dtype=&quot;float16&quot;) = packed_params[9]
        model_layers_1_mlp_down_proj_weight1: R.Tensor((2048, 5632), dtype=&quot;float16&quot;) = packed_params[10]
        model_layers_1_input_layernorm_weight1: R.Tensor((2048,), dtype=&quot;float16&quot;) = packed_params[11]
        ...

Parameters:
[(&#39;model.embed_tokens.weight&#39;, Tensor([32000, 2048], &quot;float16&quot;)),
 (&#39;model.layers.0.self_attn.qkv_proj.weight&#39;, Tensor([2560, 2048], &quot;float16&quot;)),
 (&#39;model.layers.0.self_attn.o_proj.weight&#39;, Tensor([2048, 2048], &quot;float16&quot;)),
 (&#39;model.layers.0.mlp.gate_up_proj.weight&#39;, Tensor([11264, 2048], &quot;float16&quot;)),
 (&#39;model.layers.0.mlp.down_proj.weight&#39;, Tensor([2048, 5632], &quot;float16&quot;))]
</pre></div>
</div>
</section>
<section id="define-optimization-pipeline">
<h2>Define Optimization Pipeline<a class="headerlink" href="#define-optimization-pipeline" title="Link to this heading"></a></h2>
<p>We define a series of optimization passes to optimize the model. The optimization pipeline
is designed specifically for the LLMs.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nd">@register_pipeline</span><span class="p">(</span><span class="s2">&quot;opt_llm&quot;</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">_pipeline</span><span class="p">(</span>  <span class="c1"># pylint: disable=too-many-arguments</span>
    <span class="n">ext_mods</span><span class="p">:</span> <a href="https://docs.python.org/3/library/typing.html#typing.List" title="typing.List" class="sphx-glr-backref-module-typing sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">List</span></a><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">ExternModule</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">ext_mods</span> <span class="o">=</span> <span class="n">ext_mods</span> <span class="ow">or</span> <span class="p">[]</span>

    <span class="nd">@tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">module_pass</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">_pipeline</span><span class="p">(</span><span class="n">mod</span><span class="p">:</span> <a href="../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">IRModule</span></a><span class="p">,</span> <span class="n">_ctx</span><span class="p">:</span> <a href="../../reference/api/python/transform.html#tvm.transform.PassContext" title="tvm.transform.PassContext" class="sphx-glr-backref-module-tvm-transform sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a href="../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">IRModule</span></a><span class="p">:</span>
        <span class="n">seq</span> <span class="o">=</span> <a href="../../reference/api/python/transform.html#tvm.transform.Sequential" title="tvm.transform.Sequential" class="sphx-glr-backref-module-tvm-transform sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">Sequential</span></a><span class="p">(</span>
            <span class="p">[</span>
                <span class="c1"># Phase 1. Passes on high-level operator graph</span>
                <span class="c1"># We can enable cublas for further optimization</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.FuseTransposeMatmul" title="tvm.relax.transform.FuseTransposeMatmul" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FuseTransposeMatmul</span></a><span class="p">(),</span>
                <span class="c1"># Phase 2. Lowering to TIR, inherited TVM Relax&#39;s official &quot;zero&quot; pipeline</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.LegalizeOps" title="tvm.relax.transform.LegalizeOps" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">LegalizeOps</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.AnnotateTIROpPattern" title="tvm.relax.transform.AnnotateTIROpPattern" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">AnnotateTIROpPattern</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.FoldConstant" title="tvm.relax.transform.FoldConstant" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FoldConstant</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.FuseOps" title="tvm.relax.transform.FuseOps" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FuseOps</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.FuseTIR" title="tvm.relax.transform.FuseTIR" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FuseTIR</span></a><span class="p">(),</span>
                <span class="c1"># Phase 3. Passes on TIR</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.DeadCodeElimination" title="tvm.relax.transform.DeadCodeElimination" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">DeadCodeElimination</span></a><span class="p">(),</span>
                <span class="c1"># Phase 4. Low-level Optimizations</span>
                <a href="../../reference/api/python/dlight.html#tvm.dlight.ApplyDefaultSchedule" title="tvm.dlight.ApplyDefaultSchedule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dlight</span><span class="o">.</span><span class="n">ApplyDefaultSchedule</span></a><span class="p">(</span>
                    <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dlight</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Matmul</span></a><span class="p">(),</span>
                    <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dlight</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">GEMV</span></a><span class="p">(),</span>
                    <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dlight</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Reduction</span></a><span class="p">(),</span>
                    <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dlight</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">GeneralReduction</span></a><span class="p">(),</span>
                    <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dlight</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Fallback</span></a><span class="p">(),</span>
                <span class="p">),</span>
                <span class="c1"># Phase 5. Lowering to VM bytecode</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.RewriteDataflowReshape" title="tvm.relax.transform.RewriteDataflowReshape" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">RewriteDataflowReshape</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.ToNonDataflow" title="tvm.relax.transform.ToNonDataflow" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">ToNonDataflow</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.RemovePurityChecking" title="tvm.relax.transform.RemovePurityChecking" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">RemovePurityChecking</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.CallTIRRewrite" title="tvm.relax.transform.CallTIRRewrite" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">CallTIRRewrite</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.StaticPlanBlockMemory" title="tvm.relax.transform.StaticPlanBlockMemory" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">StaticPlanBlockMemory</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.RewriteCUDAGraph" title="tvm.relax.transform.RewriteCUDAGraph" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">RewriteCUDAGraph</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.LowerAllocTensor" title="tvm.relax.transform.LowerAllocTensor" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">LowerAllocTensor</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.KillAfterLastUse" title="tvm.relax.transform.KillAfterLastUse" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">KillAfterLastUse</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.LowerRuntimeBuiltin" title="tvm.relax.transform.LowerRuntimeBuiltin" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">LowerRuntimeBuiltin</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.VMShapeLower" title="tvm.relax.transform.VMShapeLower" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">VMShapeLower</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.AttachGlobalSymbol" title="tvm.relax.transform.AttachGlobalSymbol" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">AttachGlobalSymbol</span></a><span class="p">(),</span>
                <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.AttachExternModules" title="tvm.relax.transform.AttachExternModules" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">AttachExternModules</span></a><span class="p">(</span><span class="n">ext_mods</span><span class="p">),</span>
            <span class="p">]</span>
        <span class="p">)</span>
        <span class="n">mod</span> <span class="o">=</span> <span class="n">seq</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mod</span>

    <span class="k">return</span> <span class="n">_pipeline</span>


<span class="k">with</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a><span class="p">:</span>
    <a href="../../reference/api/python/relax/relax.html#tvm.relax.VMExecutable" title="tvm.relax.VMExecutable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ex</span></a> <span class="o">=</span> <a href="../../reference/api/python/driver.html#tvm.compile" title="tvm.compile" class="sphx-glr-backref-module-tvm sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">compile</span></a><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">target</span></a><span class="p">,</span> <span class="n">relax_pipeline</span><span class="o">=</span><a href="../../reference/api/python/relax/relax.html#tvm.relax.get_pipeline" title="tvm.relax.get_pipeline" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">get_pipeline</span></a><span class="p">(</span><span class="s2">&quot;opt_llm&quot;</span><span class="p">))</span>
    <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vm</span></a> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span></a><span class="p">(</span><a href="../../reference/api/python/relax/relax.html#tvm.relax.VMExecutable" title="tvm.relax.VMExecutable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">ex</span></a><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="prepare-the-model-weights">
<h2>Prepare the model weights<a class="headerlink" href="#prepare-the-model-weights" title="Link to this heading"></a></h2>
<p>We load the pre-trained weights from Hugging Face and prepare the model weights.
The pre-trained weights are stored in the Hugging Face format. We need to load the weights
and prepare the model parameters.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that we won’t execute the following code in this tutorial because the pre-trained weights
are not available in the CI environment.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a> <span class="o">=</span> <a href="https://docs.python.org/3/library/os.html#os.getenv" title="os.getenv" class="sphx-glr-backref-module-os sphx-glr-backref-type-py-function"><span class="n">os</span><span class="o">.</span><span class="n">getenv</span></a><span class="p">(</span><span class="s2">&quot;CI&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;true&quot;</span>

<span class="n">HF_WEIGHT_PATH</span> <span class="o">=</span> <span class="kc">None</span>
<span class="c1"># HF_WEIGHT_PATH = Path(&quot;/path/to/TinyLlama-1.1B-Chat-v1.0/&quot;)</span>

<span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a><span class="p">:</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">safetensors.torch</span>
    <span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

    <span class="k">if</span> <span class="n">HF_WEIGHT_PATH</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">HF_WEIGHT_PATH</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Please set the HF_WEIGHT_PATH to the path of the pre-trained weights.&quot;</span><span class="p">)</span>

    <span class="c1"># Torch format weights</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="n">safetensors</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">load_file</span><span class="p">(</span><span class="n">HF_WEIGHT_PATH</span> <span class="o">/</span> <span class="s2">&quot;model.safetensors&quot;</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
    <span class="c1"># Numpy format weights</span>
    <span class="n">param_dict</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">if</span> <span class="n">v</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span> <span class="k">else</span> <span class="n">v</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">param_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
    <span class="p">}</span>

    <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">named_params</span></a> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">named_params</span></a><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><a href="https://docs.python.org/3/library/functions.html#int" title="builtins.int" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">model_config</span><span class="o">.</span><span class="n">num_hidden_layers</span></a><span class="p">):</span>
        <span class="c1"># Add QKV in self attention</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.self_attn&quot;</span>
        <span class="n">param_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attn</span><span class="si">}</span><span class="s2">.qkv_proj.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attn</span><span class="si">}</span><span class="s2">.q_proj.weight&quot;</span><span class="p">),</span>  <span class="c1"># Pop the old parameters to save memory</span>
                <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attn</span><span class="si">}</span><span class="s2">.k_proj.weight&quot;</span><span class="p">),</span>
                <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">attn</span><span class="si">}</span><span class="s2">.v_proj.weight&quot;</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="c1"># Add gates in MLP</span>
        <span class="n">mlp</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;model.layers.</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">.mlp&quot;</span>
        <span class="n">param_dict</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mlp</span><span class="si">}</span><span class="s2">.gate_up_proj.weight&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mlp</span><span class="si">}</span><span class="s2">.gate_proj.weight&quot;</span><span class="p">),</span>
                <span class="n">param_dict</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">mlp</span><span class="si">}</span><span class="s2">.up_proj.weight&quot;</span><span class="p">),</span>
            <span class="p">],</span>
            <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="c1"># Convert params into ndarray</span>
    <span class="n">params</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">param_dict</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float16&quot;</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">named_params</span><span class="o">.</span><span class="n">keys</span></a><span class="p">()</span>
    <span class="p">]</span>
</pre></div>
</div>
</section>
<section id="deploy-the-compiled-model">
<h2>Deploy the compiled model<a class="headerlink" href="#deploy-the-compiled-model" title="Link to this heading"></a></h2>
<p>After the model and weights are ready, we can deploy the compiled model on the target device.
The language models inference includes two steps: prefill and decode. The prefill step is
used to process the input tokens and store the KVCache. The decode step is used to generate
the token until the end token is generated.</p>
<section id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading"></a></h3>
<p>The first step is to tokenize the input prompt and embed the tokens into the hidden states.
The tokenization and embedding are the same as the original model. We use the HF tokenizer
to tokenize the input prompt and embed the tokens into the hidden states.
Note that different models require different tokenization and prompt format, please refer to
the model documentation for the correct tokenization and prompt format.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a><span class="p">:</span>
    <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoTokenizer</span>

    <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">HF_WEIGHT_PATH</span><span class="p">)</span>
    <span class="n">messages</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s your name?&quot;</span><span class="p">},</span>
    <span class="p">]</span>
    <span class="n">prompt</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">apply_chat_template</span><span class="p">(</span><span class="n">messages</span><span class="p">)</span>
    <span class="n">input_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span>

    <span class="c1"># Load prompt tokens into TVM ndarray on the target device</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">prompt</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="create-the-kvcache">
<h3>Create the KVCache<a class="headerlink" href="#create-the-kvcache" title="Link to this heading"></a></h3>
<p>Before starting the inference, we need to create the KVCache. The KVCache is used to store the
key and value tensors for the attention layer. Apache TVM provides a PagedKVCache to store the
key and value tensors. We create the PagedKVCache with the specified parameters.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a><span class="p">:</span>
    <span class="n">kv_cache</span> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vm</span></a><span class="p">[</span><span class="s2">&quot;create_tir_paged_kv_cache&quot;</span><span class="p">](</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="mi">1</span><span class="p">]),</span>  <span class="c1"># max_batch_size=1</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="mi">2048</span><span class="p">]),</span>  <span class="c1"># max_total_seq_len=2048</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="mi">2048</span><span class="p">]),</span>  <span class="c1"># prefill_chunk_size=2048</span>
        <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="mi">16</span><span class="p">]),</span>  <span class="c1"># page_size=16</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="embedding">
<h3>Embedding<a class="headerlink" href="#embedding" title="Link to this heading"></a></h3>
<p>The next step is to embed the tokens into the hidden states. We use the <cite>embed</cite> function
compiled in the Relax IRModule to embed the tokens into the hidden states.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">nd_view_func</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">get_global_func</span><span class="p">(</span><span class="s2">&quot;vm.builtin.reshape&quot;</span><span class="p">)</span>


<span class="k">def</span><span class="w"> </span><span class="nf">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">params</span><span class="p">):</span>
    <span class="n">_embed</span> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vm</span></a><span class="p">[</span><span class="s2">&quot;embed&quot;</span><span class="p">](</span><span class="n">tokens</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="c1"># Reshape hidden from [seq_len, hidden_size] to [1, seq_len, hidden_size]</span>
    <span class="n">_embed</span> <span class="o">=</span> <span class="n">nd_view_func</span><span class="p">(</span><span class="n">_embed</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="n">_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">_embed</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
    <span class="k">return</span> <span class="n">_embed</span>
</pre></div>
</div>
</section>
<section id="prefill">
<h3>Prefill<a class="headerlink" href="#prefill" title="Link to this heading"></a></h3>
<p>Before running the forward pass, we first get some help functions for preparation.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">add_sequence_func</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">get_global_func</span><span class="p">(</span><span class="s2">&quot;vm.builtin.kv_state_add_sequence&quot;</span><span class="p">)</span>
<span class="n">begin_forward_func</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">get_global_func</span><span class="p">(</span><span class="s2">&quot;vm.builtin.kv_state_begin_forward&quot;</span><span class="p">)</span>
<span class="n">end_forward_func</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">get_global_func</span><span class="p">(</span><span class="s2">&quot;vm.builtin.kv_state_end_forward&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>As we are creating a new sequence, we need to call <cite>add_sequence_func</cite> to initialize
the request. Additionally, we need to call <cite>begin_forward_func</cite> to start the forward pass,
and <cite>end_forward_func</cite> to end the forward pass.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a><span class="p">:</span>
    <span class="n">seq_id</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">add_sequence_func</span><span class="p">(</span><span class="n">kv_cache</span><span class="p">,</span> <span class="n">seq_id</span><span class="p">)</span>
    <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">begin_forward_func</span><span class="p">(</span><span class="n">kv_cache</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="n">seq_id</span><span class="p">]),</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="n">input_len</span><span class="p">]))</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">kv_cache</span> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vm</span></a><span class="p">[</span><span class="s2">&quot;prefill&quot;</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">kv_cache</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
    <span class="n">end_forward_func</span><span class="p">(</span><span class="n">kv_cache</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we have the output logits from the prefill step. The logits are used to generate the token
via sampling. Let’s sample the token from the logits.</p>
<p>In this tutorial, we simplify the sampling process and pick the token with the highest
probability. In practice, we should sample the token based on the probability distribution.
Also, to make the tutorial concise, we execute the sample process on CPU.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">sample_token</span><span class="p">(</span><span class="n">logits</span><span class="p">):</span>
    <span class="n">logits_np</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits_np</span><span class="p">)</span>


<span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a><span class="p">:</span>
    <span class="n">last_token</span> <span class="o">=</span> <span class="n">sample_token</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
    <span class="n">output_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">last_token</span><span class="p">]</span>
</pre></div>
</div>
</section>
<section id="decode">
<h3>Decode<a class="headerlink" href="#decode" title="Link to this heading"></a></h3>
<p>After the prefill step, we can start the decode step. The decode step is used to generate the
token until the end token is generated. We use the <cite>decode</cite> function compiled in the Relax
IRModule to generate the token.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="ow">not</span> <a href="https://docs.python.org/3/library/functions.html#bool" title="builtins.bool" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">IS_IN_CI</span></a><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;The generated token:&quot;</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">last_token</span> <span class="o">!=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">eos_token_id</span><span class="p">:</span>
        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">last_token</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">dev</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">embed</span><span class="p">(</span><span class="n">tokens</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
        <span class="n">begin_forward_func</span><span class="p">(</span><span class="n">kv_cache</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="n">seq_id</span><span class="p">]),</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class"><span class="n">ShapeTuple</span></a><span class="p">([</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">kv_cache</span> <span class="o">=</span> <a href="../../reference/api/python/runtime/vm.html#tvm.runtime.vm.VirtualMachine" title="tvm.runtime.vm.VirtualMachine" class="sphx-glr-backref-module-tvm-runtime-vm sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">vm</span></a><span class="p">[</span><span class="s2">&quot;decode&quot;</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">,</span> <span class="n">kv_cache</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

        <span class="n">end_forward_func</span><span class="p">(</span><span class="n">kv_cache</span><span class="p">)</span>
        <span class="n">last_token</span> <span class="o">=</span> <span class="n">sample_token</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">output_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">last_token</span><span class="p">)</span>

    <span class="nb">print</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">output_tokens</span><span class="p">))</span>
</pre></div>
</div>
<p class="sphx-glr-timing"><strong>Total running time of the script:</strong> (1 minutes 0.079 seconds)</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-how-to-tutorials-optimize-llm-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/ab03cb35cd7f92b5425a8974fcab921a/optimize_llm.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">optimize_llm.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/fb98884d1e98b239c77e2b86f43d4ac2/optimize_llm.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">optimize_llm.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/83e85f38cf16f1d926d06615fd54095c/optimize_llm.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">optimize_llm.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="cross_compilation_and_rpc.html" class="btn btn-neutral float-right" title="Cross Compilation and RPC" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="customize_opt.html" class="btn btn-neutral float-left" title="Customize Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2024 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2024 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="../../_static/downloads/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="../../_static/downloads/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>