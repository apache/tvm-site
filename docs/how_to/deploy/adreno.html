





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Deploy to Adreno™ GPU &mdash; tvm 0.18.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/tvm-logo-square.png"/>
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Integrate TVM into Your Project" href="integrate.html" />
    <link rel="prev" title="Deploy to Android" href="android.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/vta>VTA</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/blog>Blog</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmconf.org>Conference</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <input type="checkbox" class="version-toggle-box" hidden id="version-toggle">
              <label for="version-toggle" class="version-toggle-label">
                  <div tabindex="0" class="version version-selector version-selector-show">
                    0.18.dev0 <span class="chevron versions-hidden"><svg fill="none" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="m8 4 8 8-8 8" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"/></svg></span><span class="chevron versions-shown"><svg fill="none" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="m4 8 8 8 8-8" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"/></svg></span>
                  </div>
                </label>
                <div class="version-details wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                  <p class="caption" role="heading"><span class="caption-text">Versions</span></p>
                  <ol style="text-align: left">
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="/">0.18.dev0 (main)</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.8.0/">v0.8.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.9.0/">v0.9.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.10.0/">v0.10.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.11.0/">v0.11.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.12.0/">v0.12.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.13.0/">v0.13.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.14.0/">v0.14.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.15.0/">v0.15.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.16.0/">v0.16.0</a></div></li>
                    
                  </ol>
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../get_started/tutorials/ir_module.html">IRModule</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contributor Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../tutorial/index.html">User Tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">How To Guides</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../compile_models/index.html">Compile Deep Learning Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Deploy Models and Integrate TVM</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="index.html#build-the-tvm-runtime-library">Build the TVM runtime library</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#cross-compile-the-tvm-runtime-for-other-architectures">Cross compile the TVM runtime for other architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#optimize-and-tune-models-for-target-devices">Optimize and tune models for target devices</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html#deploy-optimized-model-on-target-devices">Deploy optimized model on target devices</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="cpp_deploy.html">Deploy TVM Module using C++ API</a></li>
<li class="toctree-l4"><a class="reference internal" href="android.html">Deploy to Android</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Deploy to Adreno™ GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="integrate.html">Integrate TVM into Your Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="hls.html">HLS Backend Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="arm_compute_lib.html">Relay Arm<sup>®</sup> Compute Library Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="tensorrt.html">Relay TensorRT Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="vitis_ai.html">Vitis AI Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="bnns.html">Relay BNNS Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="mrvl.html">Marvell Machine Learning Integration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#additional-deployment-how-tos">Additional Deployment How-Tos</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../work_with_relay/index.html">Work With Relay</a></li>
<li class="toctree-l2"><a class="reference internal" href="../work_with_schedules/index.html">Work With Tensor Expression and Schedules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../optimize_operators/index.html">Optimize Tensor Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tune_with_autotvm/index.html">Auto-Tune with Templates and AutoTVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tune_with_autoscheduler/index.html">Use AutoScheduler for Template-Free Scheduling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../work_with_microtvm/index.html">Work With microTVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../extend_tvm/index.html">Extend TVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../profile/index.html">Profile Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../errors.html">Handle TVM Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/tutorial/index.html">Developer Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/how_to/how_to.html">Developer How-To Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/links.html">Other APIs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Legacy</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../arch/index.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../topic/microtvm/index.html">microTVM: TVM on bare-metal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../topic/vta/index.html">VTA: Versatile Tensor Accelerator</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/security.html">Security Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="../index.html">How To Guides</a> <span class="br-arrow">></span></li>
        
          <li><a href="index.html">Deploy Models and Integrate TVM</a> <span class="br-arrow">></span></li>
        
      <li>Deploy to Adreno™ GPU</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/tvm/edit/main/docs/how_to/deploy/adreno.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="deploy-to-adreno-gpu">
<h1>Deploy to Adreno™ GPU<a class="headerlink" href="#deploy-to-adreno-gpu" title="Permalink to this headline">¶</a></h1>
<p><strong>Authors</strong>: Daniil Barinov, Egor Churaev, Andrey Malyshev, Siva Rama Krishna</p>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Adreno™ is a series of graphics processing unit (GPU) semiconductor
intellectual property cores developed by Qualcomm and used in many of
their SoCs.</p>
<p>The Adreno™ GPU accelerates the rendering of complex geometries to
deliver high-performance graphics and a rich user experience with low
power consumption.</p>
<p>TVM supports deep learning acceleration on Adreno™ GPU by native OpenCL backend of TVM and
also through OpenCLML backend. Native OpenCL backend of TVM is enhanced to make it
Adreno™ friendly by incorporating texture memory usage and Adreno™ friendly layouts.
OpenCLML is an SDK release by Qualcomm that provides kernel acceleration library
for most of the deep learning operators.</p>
<p>This guide is organized to demonstrate various design aspects of</p>
<ul class="simple">
<li><p><a class="reference internal" href="#opencl-enhancements"><span class="std std-ref">OpenCL Backend Ehnahcements</span></a></p></li>
<li><p><a class="reference internal" href="#about-openclml"><span class="std std-ref">About OpenCLML</span></a></p></li>
<li><p><a class="reference internal" href="#build-deploy"><span class="std std-ref">Build and Deploy</span></a></p></li>
</ul>
</div>
<div class="section" id="opencl-backend-enhancements">
<span id="opencl-enhancements"></span><h2>OpenCL Backend Enhancements<a class="headerlink" href="#opencl-backend-enhancements" title="Permalink to this headline">¶</a></h2>
<p>OpenCL backend of TVM is enhanced to take advantage of Adreno™ specific features like
- Texture memory usage.
- Adreno™ friendly activation layouts.
- Brand new schedules to accelerate with above features.</p>
<p>One of the Adreno™’s advantages is the clever handling of textures. At
the moment, TVM is able to benefit from this by having texture support
for Adreno™. The graph below shows the Adreno™ A5x architecture.</p>
<p><img alt="High-level overview of the Adreno™ A5x architecture for OpenCL" src="https://raw.githubusercontent.com/tlc-pack/web-data/main/images/how-to/adreno_architecture.png" /></p>
<p><em>Fig. 1 High-level overview of the Adreno™ A5x architecture for OpenCL</em></p>
<p><em>source:</em> <a class="reference external" href="https://dl.acm.org/doi/10.1145/3204919.3204935">OpenCL Optimization and Best Practices for Qualcomm Adreno™ GPUs</a></p>
<p>Reasons of using textures:</p>
<ul class="simple">
<li><p>Texture processor (TP) has a dedicated L1 cache, which is read-only cache and stores data
fetched from level-2 (L2) cache for texture operations (primary
reason)</p></li>
<li><p>The handling of image boundaries is built-in.</p></li>
<li><p>Supports numerous image format and data type combinations with
support for automatic format conversions</p></li>
</ul>
<p>Overall, with textures, it is possible to achieve a significant performance boost
compared to OpenCL buffer based solutions.</p>
<p>In general we specify target as <code class="docutils literal notranslate"><span class="pre">target=&quot;opencl&quot;</span></code> for a regular OpenCL based target which generates the kernels as shown below.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">__kernel</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">tvmgen_default_fused_nn_conv2d_kernel0</span><span class="p">(</span><span class="n">__global</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="kr">restrict</span><span class="w"> </span><span class="n">p0</span><span class="p">,</span><span class="w"> </span><span class="n">__global</span><span class="w"> </span><span class="kt">double</span><span class="o">*</span><span class="w"> </span><span class="kr">restrict</span><span class="w"> </span><span class="n">p1</span><span class="p">,</span><span class="w"> </span><span class="n">__global</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="kr">restrict</span><span class="w"> </span><span class="n">conv2d_nhwc</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="c1">// body..</span>
</pre></div>
</div>
<p>Above OpenCL kernel definition has <code class="docutils literal notranslate"><span class="pre">__global</span> <span class="pre">float*</span></code> poniters which are essestially OpenCL <code class="docutils literal notranslate"><span class="pre">buffer</span></code>  objects.</p>
<p>When enabled texture based enhancements by modifying target definition as <code class="docutils literal notranslate"><span class="pre">target=&quot;opencl</span> <span class="pre">-device=adreno&quot;</span></code> we can see the generated
kernels using texture backed OpenCL image objects as shown below.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span></span><span class="n">__kernel</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="n">tvmgen_default_fused_nn_conv2d_kernel0</span><span class="p">(</span><span class="n">__write_only</span><span class="w"> </span><span class="n">image2d_t</span><span class="w"> </span><span class="n">pad_temp_global_texture</span><span class="p">,</span><span class="w"> </span><span class="n">__read_only</span><span class="w"> </span><span class="n">image2d_t</span><span class="w"> </span><span class="n">p0</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="c1">// body..</span>
</pre></div>
</div>
<p><em>image2d_t</em> is a built-in OpenCL types that represents two-dimensional image object and provides several additional functions.
When we use <em>image2d_t</em> we read <em>4 elements at one time</em>, and it helps to utilize hardware in a more efficient way.</p>
<p>Please refer to <a class="reference internal" href="#advanced-usage"><span class="std std-ref">Advanced Usage</span></a> for more details about generation and inspection of kernel sources.</p>
</div>
<div class="section" id="about-openclml">
<span id="id1"></span><h2>About OpenCLML<a class="headerlink" href="#about-openclml" title="Permalink to this headline">¶</a></h2>
<p>OpenCLML is a SDK released by Qualcomm that provides accelerated deep learning operators.
These operators are exposed as an extension <code class="docutils literal notranslate"><span class="pre">cl_qcom_ml_ops</span></code> to standard OpenCL specification.
Please refer <a class="reference external" href="https://developer.qualcomm.com/blog/accelerate-your-models-our-opencl-ml-sdk">Accelerate your models with our OpenCL ML SDK</a> for more details.</p>
<p>OpenCLML is integrated into TVM as a <a class="reference external" href="https://tvm.apache.org/docs/dev/how_to/relay_bring_your_own_codegen.html?highlight=bring%20your%20own">BYOC</a> solution.
OpenCLML operators can use same context and can be enqueued on same command queue as used in native OpenCL.
We took advantage of this to avoid any context switching over heads while fallback to native OpenCL.</p>
</div>
<div class="section" id="tvm-for-adreno">
<span id="build-deploy"></span><h2>TVM for Adreno™<a class="headerlink" href="#tvm-for-adreno" title="Permalink to this headline">¶</a></h2>
<p>This section gives instructions about various ways of building and deploying model
to Adreno™ target. Adreno™ is a remote target which is connected to the host via ADB connection.
Deploying the compiled model here require use some tools on host as well as on target.</p>
<p>TVM has simplified user friendly command line based tools as well as
developer centric python API interface for various steps like auto tuning, building and deploying.</p>
<p><img alt="Adreno deployment pipeline" src="https://raw.githubusercontent.com/tlc-pack/web-data/main/images/how-to/Adreno-Deployment-Pipeline.jpg" /></p>
<p><em>Fig.2 Build and Deployment pipeline on Adreno devices</em></p>
<p>The figure above demonstrates a generalized pipeline for various stages listed below.</p>
<p><strong>Model import:</strong>
At this stage we import a model from well known frameworks like Tensorflow, PyTorch, ONNX …etc.
This stage converts the given model into TVM’s relay module format. Alternatively one can build a relay module manually
by using TVM’s operator inventory too. TVM module generated here is a target independent representation of the graph.</p>
<p><strong>Auto Tuning:</strong>
At this stage we tune the TVM generated kernels specific to a target. Auto tuning process requires
target device availability and in case of a remote target like Adreno™ on Android device we use RPC Setup for communication.
Later sections in this guide will detail about RPC Setup for Android device. Auto tuning is not a necessary step for
compilation of a model. It is necessary for acheiving best performance out of TVM generated kernels.</p>
<p><strong>Compilation:</strong>
At this stage we compile the model for specific target. Given we auto tuned the module in previous stage,
TVM compilation make use of the tuning log for genetrating best performing kernels. TVM compilation process produces artifacts
containing kernel shared lib, graph definition in json format and parameters binary file in TVM specific format.</p>
<p><strong>Deploy (or test run) on Target:</strong>
At this stage we run the TVM compilation output on the target. Deployment is possible from python
environment using RPC Setup and also using TVM’s native tool which is native binary cross compiled for Android.
At this stage we can run the compiled model on Android target and unit test output correctness and performance aspects.</p>
<p><strong>Application Integration:</strong>
This stage is all about integrating TVM compiled model in applications. Here we discuss about
interfacing tvm runtime from Android (cpp native environment or from JNI) for setting input and getting output.</p>
<p><strong>Advanced Usage:</strong>
This section advanced user interests like viewing generated source code, altering precision of the module …etc.</p>
<p>This tutorial covers all the above aspects as part of below sections.</p>
<ul class="simple">
<li><p><a class="reference internal" href="#development-environment"><span class="std std-ref">Development environment</span></a></p></li>
<li><p><a class="reference internal" href="#rpc-setup"><span class="std std-ref">RPC Setup</span></a></p></li>
<li><p><a class="reference internal" href="#commandline-interface"><span class="std std-ref">Commandline tools</span></a></p></li>
<li><p><a class="reference internal" href="#python-interface"><span class="std std-ref">Python interface</span></a></p></li>
<li><p><a class="reference internal" href="#application-integration"><span class="std std-ref">Application Integration</span></a></p></li>
<li><p><a class="reference internal" href="#advanced-usage"><span class="std std-ref">Advanced Usage</span></a></p></li>
</ul>
</div>
<div class="section" id="development-environment-setup-automatic">
<span id="development-environment"></span><h2>Development Environment Setup : Automatic<a class="headerlink" href="#development-environment-setup-automatic" title="Permalink to this headline">¶</a></h2>
<p>TVM ships a predefined docker container environment with all prerequisites to get started quickly.
You may also refer to <a class="reference internal" href="#manual-setup"><span class="std std-ref">Manual Environment Setup</span></a> for more control on the dependencies.</p>
<p>For docker setup the pre requisite is just docker tool availabilty on host.</p>
<p>Below commands can build a docker image for adreno.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">docker</span><span class="o">/</span><span class="n">build</span><span class="o">.</span><span class="n">sh</span> <span class="n">ci_adreno</span>
<span class="n">docker</span> <span class="n">tag</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ci_adreno</span> <span class="n">ci_adreno</span>
</pre></div>
</div>
<p>Now we can build both host and target utils with below command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">ci</span><span class="o">.</span><span class="n">py</span> <span class="n">adreno</span> <span class="o">-</span><span class="n">i</span>
</pre></div>
</div>
<p>To build TVM with OpenCLML SDK we need export the OpenCLML SDK as shown below while building</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">ADRENO_OPENCL</span><span class="o">=&lt;</span><span class="n">Path</span> <span class="n">to</span> <span class="n">OpenCLML</span> <span class="n">SDK</span><span class="o">&gt;</span>
<span class="o">./</span><span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">ci</span><span class="o">.</span><span class="n">py</span> <span class="n">adreno</span> <span class="o">-</span><span class="n">i</span>
</pre></div>
</div>
<p>On successful compilation this leaves us into a docker shell. The build leaves two folders</p>
<ul>
<li><p>build-adreno:  The host side TVM compiler build.</p></li>
<li><p>build-adreno-target : Contains the android target components</p>
<blockquote>
<div><ul class="simple">
<li><p>libtvm_runtime.so : TVM runtime library</p></li>
<li><p>tvm_rpc : The rpc runtime environment tool</p></li>
<li><p>rtvm : A native stand alone tool</p></li>
</ul>
</div></blockquote>
</li>
</ul>
<p>While using docker environment the android device is shared with host. Hence, it is required
to have adb version <code class="docutils literal notranslate"><span class="pre">1.0.41</span></code> on the host as the docker used the same version.</p>
<p>We can check adb devices availability inside docker environment too.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>user@ci-adreno-fpeqs:~$ adb devices
List of devices attached
aaaabbbb     device
ccccdddd     device
</pre></div>
</div>
</div>
<div class="section" id="development-environment-setup-manual">
<span id="manual-setup"></span><h2>Development Environment Setup : Manual<a class="headerlink" href="#development-environment-setup-manual" title="Permalink to this headline">¶</a></h2>
<p>Manual build process require building of host and target components.</p>
<p>Below command will configure the build the host compiler</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="n">build</span>
<span class="n">cd</span> <span class="n">build</span>
<span class="n">cp</span> <span class="o">../</span><span class="n">cmake</span><span class="o">/</span><span class="n">config</span><span class="o">.</span><span class="n">cmake</span> <span class="o">.</span>

<span class="c1"># Enable RPC capability to communicate to remote device.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_RPC</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># We use graph executor for any host(x86) side verification of the model.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_GRAPH_EXECUTOR</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># Enable backtrace if possible for more ebug information on any crash.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_LIBBACKTRACE</span> <span class="n">AUTO</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># The target_host will be llvm.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_LLVM</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
</pre></div>
</div>
<p>Additionally we can push below config entry to compile with OpenCLML support.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export ADRENO_OPENCL=&lt;Path to OpenCLML SDK&gt;
echo set\(USE_CLML ${ADRENO_OPENCL}\) &gt;&gt; config.cmake
</pre></div>
</div>
<p>now we can build as shown below</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmake</span> <span class="o">..</span>
<span class="n">make</span>
</pre></div>
</div>
<p>Finally we can export python path as</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export PYTHONPATH=$TVM_HOME/python:${PYTHONPATH}
python3 -c &quot;import tvm&quot; # Verify tvm python package
</pre></div>
</div>
<p>Now, we can configure and build the target components with below configuration
Target build require Android NDK to be installed.</p>
<ul class="simple">
<li><p>Read documentation about <em>Android NDK installation</em> here: <a class="reference external" href="https://developer.android.com/ndk">https://developer.android.com/ndk</a></p></li>
<li><p>To get access to adb tools you can see <em>Android Debug Bridge installation</em> here: <a class="reference external" href="https://developer.android.com/studio/command-line/adb">https://developer.android.com/studio/command-line/adb</a></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="n">build</span><span class="o">-</span><span class="n">adreno</span>
<span class="n">cd</span> <span class="n">build</span><span class="o">-</span><span class="n">adreno</span>
<span class="n">cp</span> <span class="o">../</span><span class="n">cmake</span><span class="o">/</span><span class="n">config</span><span class="o">.</span><span class="n">cmake</span> <span class="o">.</span>
<span class="c1"># Enable OpenCL backend.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_OPENCL</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># Enable RPC functionality.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_RPC</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># Build tvm_rpc tool that runs on target device.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_CPP_RPC</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># Build native rtvm deploy tool.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_CPP_RTVM</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># We use graph executor for deploying on devices like Android.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_GRAPH_EXECUTOR</span> <span class="n">ON</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># Backtrace enablement if possible.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_LIBBACKTRACE</span> <span class="n">AUTO</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="c1"># Adreno supports 32bit alignment for OpenCL allocations rather 64bit.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_KALLOC_ALIGNMENT</span> <span class="mi">32</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>

<span class="c1"># Android build related defines.</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">ANDROID_ABI</span> <span class="n">arm64</span><span class="o">-</span><span class="n">v8a</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">ANDROID_PLATFORM</span> <span class="n">android</span><span class="o">-</span><span class="mi">28</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">MACHINE_NAME</span> <span class="n">aarch64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
</pre></div>
</div>
<p>Additionally we can push below config to compile with OpenCLML support.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">ADRENO_OPENCL</span><span class="o">=&lt;</span><span class="n">Path</span> <span class="n">to</span> <span class="n">OpenCLML</span> <span class="n">SDK</span><span class="o">&gt;</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_CLML</span> <span class="s2">&quot;$</span><span class="si">{ADRENO_OPENCL}</span><span class="s2">&quot;</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
<span class="n">echo</span> <span class="nb">set</span>\<span class="p">(</span><span class="n">USE_CLML_GRAPH_EXECUTOR</span> <span class="s2">&quot;$</span><span class="si">{ADRENO_OPENCL}</span><span class="s2">&quot;</span>\<span class="p">)</span> <span class="o">&gt;&gt;</span> <span class="n">config</span><span class="o">.</span><span class="n">cmake</span>
</pre></div>
</div>
<p>For Android target build <code class="docutils literal notranslate"><span class="pre">ANDROID_NDK_HOME</span></code> is a dependency and we should have the same in the enviromnet variable.
Below commands will build Adreno™ target components</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cmake</span> <span class="o">-</span><span class="n">DCMAKE_TOOLCHAIN_FILE</span><span class="o">=</span><span class="s2">&quot;$</span><span class="si">{ANDROID_NDK_HOME}</span><span class="s2">/build/cmake/android.toolchain.cmake&quot;</span> \
   <span class="o">-</span><span class="n">DANDROID_ABI</span><span class="o">=</span><span class="n">arm64</span><span class="o">-</span><span class="n">v8a</span> \
   <span class="o">-</span><span class="n">DANDROID_PLATFORM</span><span class="o">=</span><span class="n">android</span><span class="o">-</span><span class="mi">28</span> \
   <span class="o">-</span><span class="n">DCMAKE_SYSTEM_VERSION</span><span class="o">=</span><span class="mi">1</span> \
   <span class="o">-</span><span class="n">DCMAKE_FIND_ROOT_PATH</span><span class="o">=</span><span class="s2">&quot;$</span><span class="si">{ADRENO_OPENCL}</span><span class="s2">&quot;</span> \
   <span class="o">-</span><span class="n">DCMAKE_FIND_ROOT_PATH_MODE_PROGRAM</span><span class="o">=</span><span class="n">NEVER</span> \
   <span class="o">-</span><span class="n">DCMAKE_FIND_ROOT_PATH_MODE_LIBRARY</span><span class="o">=</span><span class="n">ONLY</span> \
   <span class="o">-</span><span class="n">DCMAKE_CXX_COMPILER</span><span class="o">=</span><span class="s2">&quot;$</span><span class="si">{ANDROID_NDK_HOME}</span><span class="s2">/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android28-clang++&quot;</span> \
   <span class="o">-</span><span class="n">DCMAKE_C_COMPILER</span><span class="o">=</span><span class="s2">&quot;$</span><span class="si">{ANDROID_NDK_HOME}</span><span class="s2">/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android28-clang&quot;</span> \
   <span class="o">-</span><span class="n">DMACHINE_NAME</span><span class="o">=</span><span class="s2">&quot;aarch64-linux-gnu&quot;</span> <span class="o">..</span>

<span class="n">make</span> <span class="n">tvm_runtime</span> <span class="n">tvm_rpc</span> <span class="n">rtvm</span>
</pre></div>
</div>
</div>
<div class="section" id="rpc-setup">
<span id="id2"></span><h2>RPC Setup<a class="headerlink" href="#rpc-setup" title="Permalink to this headline">¶</a></h2>
<p>RPC Setup allows remote target access over TCP/IP networking interface. RPC Setup is essential for auto tuning stage as tuning
involves running of auto generated kernels on real device and optimize the same by using machine learning approach. Please refer
<a class="reference external" href="https://tvm.apache.org/docs/how_to/tune_with_autotvm/index.html">Auto-Tune with Templates and AutoTVM</a> got more details about AutoTVM.</p>
<p>RPC Setup is also useful to deply the compiled model to a remote device from python interface or <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> tool from host device.</p>
<p>RPC Setup has multiple components as listed below.</p>
<p><strong>TVM Tracker:</strong>
TVM tracker is a host side daemon that manages remote devices and serve them to host side applications. Applications
can connect to this tracker and acquire a remote device handle to communicate.</p>
<p><strong>TVM RPC:</strong>
TVM RPC is a native application that runs on the remote device (Android in our case) and registers itself to the TVM Tracker
running on the host.</p>
<p>Hence, for RPC based setup we will have above components running on host and target device. Below sections explain how to setup the same
manually and also inside docker using automated tools.</p>
<p><strong>Automated RPC Setup:</strong>
Here we will explain how to setup RPC in docker environment.</p>
<p>Below command launches tracker in docker environment, where tracker listens on port 9190.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">ci</span><span class="o">.</span><span class="n">py</span> <span class="n">adreno</span> <span class="o">-</span><span class="n">i</span> <span class="c1"># Launch a new shell on the anreno docker</span>
<span class="n">source</span>  <span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup</span><span class="o">-</span><span class="n">adreno</span><span class="o">-</span><span class="n">env</span><span class="o">.</span><span class="n">sh</span> <span class="o">-</span><span class="n">e</span> <span class="n">tracker</span> <span class="o">-</span><span class="n">p</span> <span class="mi">9190</span>
</pre></div>
</div>
<p>Now, the below comand can run TVM RPC on remote android device with id <code class="docutils literal notranslate"><span class="pre">abcdefgh</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">ci</span><span class="o">.</span><span class="n">py</span> <span class="n">adreno</span> <span class="o">-</span><span class="n">i</span> <span class="c1"># Launch a new shell on adreno docker.</span>
<span class="n">source</span>  <span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup</span><span class="o">-</span><span class="n">adreno</span><span class="o">-</span><span class="n">env</span><span class="o">.</span><span class="n">sh</span> <span class="o">-</span><span class="n">e</span> <span class="n">device</span> <span class="o">-</span><span class="n">p</span> <span class="mi">9190</span> <span class="o">-</span><span class="n">d</span> <span class="n">abcdefgh</span>
</pre></div>
</div>
<p>Further, below command can be used to query the RPC setup details on any other docker terminals.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">./</span><span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">ci</span><span class="o">.</span><span class="n">py</span> <span class="n">adreno</span> <span class="o">-</span><span class="n">i</span> <span class="c1"># Launch a new shell on adreno docker.</span>
<span class="n">source</span>  <span class="n">tests</span><span class="o">/</span><span class="n">scripts</span><span class="o">/</span><span class="n">setup</span><span class="o">-</span><span class="n">adreno</span><span class="o">-</span><span class="n">env</span><span class="o">.</span><span class="n">sh</span> <span class="o">-</span><span class="n">e</span> <span class="n">query</span> <span class="o">-</span><span class="n">p</span> <span class="mi">9190</span>
</pre></div>
</div>
<p><strong>Manual RPC Setup:</strong></p>
<p>Please refer to the tutorial
<a class="reference external" href="https://tvm.apache.org/docs/how_to/deploy_models/deploy_model_on_adreno.html">How To Deploy model on Adreno</a>
for manual RPC environment setup.</p>
<p>This concludes RPC Setup and we have rpc-tracker available on host <code class="docutils literal notranslate"><span class="pre">127.0.0.1</span></code> (rpc-tracker) and port <code class="docutils literal notranslate"><span class="pre">9190</span></code> (rpc-port).</p>
</div>
<div class="section" id="commandline-tools">
<span id="commandline-interface"></span><h2>Commandline Tools<a class="headerlink" href="#commandline-tools" title="Permalink to this headline">¶</a></h2>
<p>Here we describe entire compilation process using command line tools. TVM has command line utility
<a class="reference external" href="https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html">tvmc</a> to perform
model import, auto tuning, compilation and deply over rpc.
<a class="reference external" href="https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html">tvmc</a>  has many options to explore and try.</p>
<p><strong>Model Import &amp; Tuning:</strong>
Use the below command to import a model from any framework and auto tune the same.
Here we use a model from Keras and it uses RPC setup for tuning and finally generates tuning log file
<code class="docutils literal notranslate"><span class="pre">keras-resnet50.log</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">tvm</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">tvmc</span> <span class="n">tune</span> <span class="o">--</span><span class="n">target</span><span class="o">=</span><span class="s2">&quot;opencl -device=adreno&quot;</span> \
<span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">host</span><span class="o">=</span><span class="s2">&quot;llvm -mtriple=aarch64-linux-gnu&quot;</span> \
<span class="n">resnet50</span><span class="o">.</span><span class="n">h5</span> <span class="o">-</span><span class="n">o</span> \
<span class="n">keras</span><span class="o">-</span><span class="n">resnet50</span><span class="o">.</span><span class="n">log</span> \
<span class="o">--</span><span class="n">early</span><span class="o">-</span><span class="n">stopping</span> <span class="mi">0</span> <span class="o">--</span><span class="n">repeat</span> <span class="mi">30</span> <span class="o">--</span><span class="n">rpc</span><span class="o">-</span><span class="n">key</span> <span class="n">android</span> \
<span class="o">--</span><span class="n">rpc</span><span class="o">-</span><span class="n">tracker</span> <span class="mf">127.0.0.1</span><span class="p">:</span><span class="mi">9190</span> <span class="o">--</span><span class="n">trials</span> <span class="mi">1024</span> \
<span class="o">--</span><span class="n">tuning</span><span class="o">-</span><span class="n">records</span> <span class="n">keras</span><span class="o">-</span><span class="n">resnet50</span><span class="o">-</span><span class="n">records</span><span class="o">.</span><span class="n">log</span> <span class="o">--</span><span class="n">tuner</span> <span class="n">xgb</span>
</pre></div>
</div>
<p><strong>Model Compilation:</strong></p>
<p>Use below command for compiling the model and produce TVM compiler outputs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>python3 -m tvm.driver.tvmc compile \
--cross-compiler ${ANDROID_NDK_HOME}/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android28-clang \
--target=&quot;opencl, llvm&quot; --target-llvm-mtriple aarch64-linux-gnu --target-opencl-device adreno \
--tuning-records keras-resnet50.log -o keras-resnet50.tar resnet50.h5
</pre></div>
</div>
<p>While enabled OpenCLML offloading we need to add target <code class="docutils literal notranslate"><span class="pre">clml</span></code> as shown below. Tuning log is valid for OpenCLML offloading also
as the OpenCL path is fallback option for any operator didn’t go through OpenCLML path. The tuning log will be used for such operators.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>python3 -m tvm.driver.tvmc compile \
--cross-compiler ${ANDROID_NDK_HOME}/toolchains/llvm/prebuilt/linux-x86_64/bin/aarch64-linux-android28-clang \
--target=&quot;opencl, clml, llvm&quot; --desired-layout NCHW --target-llvm-mtriple aarch64-linux-gnu --target-opencl-device adreno \
--tuning-records keras-resnet50.log -o keras-resnet50.tar resnet50.h5
</pre></div>
</div>
<p>On successful compilation, above command produce <code class="docutils literal notranslate"><span class="pre">keras-resnet50.tar</span></code>.
It is a compressed archive with kernel shared lib(mod.so), graph json(mod.json) and params binary(mod.params).</p>
<p><strong>Deploy &amp; Run on Target:</strong></p>
<p>Running the compiled model on Android target is possible in RPC way as well as native deployment.</p>
<p>We can use below tvmc command to deploy on remore target via RPC based setup.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">tvm</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">tvmc</span> <span class="n">run</span> <span class="o">--</span><span class="n">device</span><span class="o">=</span><span class="s2">&quot;cl&quot;</span> <span class="n">keras</span><span class="o">-</span><span class="n">resnet50</span><span class="o">.</span><span class="n">tar</span> \
<span class="o">--</span><span class="n">rpc</span><span class="o">-</span><span class="n">key</span> <span class="n">android</span> <span class="o">--</span><span class="n">rpc</span><span class="o">-</span><span class="n">tracker</span> <span class="mf">127.0.0.1</span><span class="p">:</span><span class="mi">9190</span> <span class="o">--</span><span class="nb">print</span><span class="o">-</span><span class="n">time</span>
</pre></div>
</div>
<p><a class="reference external" href="https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html">tvmc</a> based run has more options
to initialize the input in various modes like fill, random ..etc.</p>
<p><code class="docutils literal notranslate"><span class="pre">tvmc</span></code> based deployment generally a quick verification of compiled model on target from remote host via RPC setup.</p>
<p>Production generally uses native deploymenmt environment like Android JNI or CPP native environments.
Here we need to use cross compiled <code class="docutils literal notranslate"><span class="pre">tvm_runtime</span></code> interface to deploy the tvm compilation output, i.e. <code class="docutils literal notranslate"><span class="pre">TVMPackage</span></code>.</p>
<p>TVM has a standalone tool <code class="docutils literal notranslate"><span class="pre">rtvm</span></code> to deploy and run the model natively on ADB shell. The build process produces this tool under build-adreno-target.
Please refer to <a class="reference external" href="https://github.com/apache/tvm/tree/main/apps/cpp_rtvm">rtvm</a> for more details about this tool.</p>
<p>While integrating inside existing Android application TVM has multiple options. For JNI or CPP native we may use <a class="reference external" href="https://github.com/apache/tvm/blob/main/include/tvm/runtime/c_runtime_api.h">C Runtime API</a>
You may refer to <code class="docutils literal notranslate"><span class="pre">rtvm</span></code>’s simplified interface <a class="reference external" href="https://github.com/apache/tvm/blob/main/apps/cpp_rtvm/tvm_runner.h">TVMRunner</a> also.</p>
</div>
<div class="section" id="python-interface">
<span id="id5"></span><h2>Python Interface<a class="headerlink" href="#python-interface" title="Permalink to this headline">¶</a></h2>
<p>This section explains importing, auto tuning, compiling and running a model using python interface.TVM has a high level interface through <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> abstraction as well as low level relay api. We will discuss about both of these in details.</p>
<p><strong>TVMC Interface:</strong></p>
<p>While using <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> python interface we first load a model that produces <code class="docutils literal notranslate"><span class="pre">TVMCModel</span></code>. <code class="docutils literal notranslate"><span class="pre">TVMCModel</span></code> will be used for Auto Tuning to produce tuning cache.
Compilation process uses <code class="docutils literal notranslate"><span class="pre">TVMCModel</span></code> and tuning cache (optional) to produce <code class="docutils literal notranslate"><span class="pre">TVMCPackage</span></code>. Now, <code class="docutils literal notranslate"><span class="pre">TVMCPackage</span></code> will be saved to file system or
can be used to deploy and run on target device.</p>
<p>Please refer to the tutorial for the same
<a class="reference external" href="https://tvm.apache.org/docs/how_to/deploy_models/deploy_model_on_adreno_tvmc.html">How To Deploy model on Adreno using TVMC</a></p>
<p>Saved <code class="docutils literal notranslate"><span class="pre">TVMCPackage</span></code> can be used for native deployment using <code class="docutils literal notranslate"><span class="pre">rtvm</span></code> utility too.</p>
<p>Also, please refer to <a class="reference external" href="https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html">tvmc</a>
documentation for more details about the api interface.</p>
<p><strong>Relay Interface:</strong></p>
<p>Relay api interface gives lower level api access to the tvm compiler interface.
Similar to <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> interface relay api interface provides various frontend API to convert models to a relay <code class="docutils literal notranslate"><span class="pre">Module</span></code>.
Relay <code class="docutils literal notranslate"><span class="pre">Module</span></code> will be used for all kinds transforms like precision conversions, CLML offloading and other custom transforms if any.
The resulting Module will be used for Auto Tuning too. Finally, we use <code class="docutils literal notranslate"><span class="pre">relay.build</span></code> API to generate library module.
From this library module, we can export compilation artifacts like module shared library (mod.so), params(mod.params) and json graph(mod.json).
This library module will be used to create graph runtime to deploy and run on target device.</p>
<p>Please refer to the tutorial <a class="reference external" href="https://tvm.apache.org/docs/how_to/deploy_models/deploy_model_on_adreno.html">How To Deploy model on Adreno</a>
for a step by step explanation of the same.</p>
<p>Additionally, TVM also supports Java interface through <a class="reference external" href="https://github.com/apache/tvm/tree/main/jvm">TVM4J</a></p>
</div>
<div class="section" id="application-integration">
<span id="id8"></span><h2>Application Integration<a class="headerlink" href="#application-integration" title="Permalink to this headline">¶</a></h2>
<p>TVM compilation output is represented as module shared lib (mod.so), graph json(mod.json) and params (mod.params).
Archived representation of TVMPackage is also contains the same.</p>
<p>In general a CPP/C based interface will be sufficient for any Android application integration.</p>
<p>TVM natively expose <code class="docutils literal notranslate"><span class="pre">c_runtime_api</span></code> for loading a TVM compiled module and run the same.</p>
<p>Alternatively one may refer to <a class="reference external" href="https://github.com/apache/tvm/blob/main/apps/cpp_rtvm/tvm_runner.h">cpp_rtvm</a>
<code class="docutils literal notranslate"><span class="pre">TVMRunner</span></code> interface too for further simplified version of the same.</p>
</div>
<div class="section" id="advanced-usage">
<span id="id9"></span><h2>Advanced Usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline">¶</a></h2>
<p>This section details some of the advanced usage and additional information while using Adreno™ target on TVM.</p>
<div class="section" id="generated-source-inspection">
<h3>Generated Source Inspection<a class="headerlink" href="#generated-source-inspection" title="Permalink to this headline">¶</a></h3>
<p>Apart from standard tvm compilation artifacts kernel library (mod.so), graph (mod.json) and params (mod.params)
we can also generate opencl kernel source, clml offloaded graph …etc from lib handle as shown below.
TVM compilation output is organized as a TVM module and many other TVM modules imported into it.</p>
<p>Below snippet can dump CLML sub graphs in json format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Look for &quot;clml&quot; typed module imported.</span>
<span class="n">clml_modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">mod</span><span class="p">:</span> <span class="n">mod</span><span class="o">.</span><span class="n">type_key</span> <span class="o">==</span> <span class="s2">&quot;clml&quot;</span><span class="p">,</span> <span class="n">lib</span><span class="o">.</span><span class="n">get_lib</span><span class="p">()</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">))</span>
<span class="c1"># Loop through all clml sub graphs and dump the json formatted CLML sub graphs.</span>
<span class="k">for</span> <span class="n">cmod</span> <span class="ow">in</span> <span class="n">clml_modules</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CLML Src:&quot;</span><span class="p">,</span> <span class="n">cmod</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
</pre></div>
</div>
<p>Similarly, below snippet can extract opencl kernel source from the compiled TVM module.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Similarly we can dump open kernel source too as shown below</span>
<span class="c1"># Look for &quot;opencl&quot; typed module imported.</span>
<span class="n">opencl_modules</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">mod</span><span class="p">:</span> <span class="n">mod</span><span class="o">.</span><span class="n">type_key</span> <span class="o">==</span> <span class="s2">&quot;opencl&quot;</span><span class="p">,</span> <span class="n">lib</span><span class="o">.</span><span class="n">get_lib</span><span class="p">()</span><span class="o">.</span><span class="n">imported_modules</span><span class="p">))</span>
<span class="c1"># Now dump kernel source for each OpenCL targetted sub graph.</span>
<span class="k">for</span> <span class="n">omod</span> <span class="ow">in</span> <span class="n">opencl_modules</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;OpenCL Src:&quot;</span><span class="p">,</span> <span class="n">omod</span><span class="o">.</span><span class="n">get_source</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="section" id="precisions">
<h3>Precisions<a class="headerlink" href="#precisions" title="Permalink to this headline">¶</a></h3>
<p>The right choice of precision for a specific workload can greatly increase the efficiency of the solution,
shifting the initial balance of precision and speed to the side that is a priority for the problem.</p>
<p>We can choose from <em>float16</em>, <em>float16_acc32</em> (Mixed Precision), <em>float32</em> (standard).</p>
<p><strong>Float16</strong></p>
<p>To leverage the GPU hardware capabilities and utilize the benefits of half precision computation and memory management,
we can convert an original model having floating points operation to a model operating with half precision.
Choosing lower precision will positively affect the performance of the model, but it may also have a decrease in the accuracy of the model.</p>
<p>To do the conversion you need to call adreno specific transformation API as soon as relay module is generated through any frontend.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm.driver.tvmc.transform</span> <span class="kn">import</span> <span class="n">apply_graph_transforms</span>
<span class="n">mod</span>  <span class="o">=</span> <span class="n">apply_graph_transforms</span><span class="p">(</span>
         <span class="n">mod</span><span class="p">,</span>
         <span class="p">{</span>
             <span class="s2">&quot;mixed_precision&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s2">&quot;mixed_precision_ops&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;nn.conv2d&quot;</span><span class="p">,</span> <span class="s2">&quot;nn.dense&quot;</span><span class="p">],</span>
             <span class="s2">&quot;mixed_precision_calculation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;float16&quot;</span><span class="p">,</span>
             <span class="s2">&quot;mixed_precision_acc_type&quot;</span><span class="p">:</span> <span class="s2">&quot;float16&quot;</span><span class="p">,</span>
         <span class="p">},</span>
     <span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tvm.driver.tvmc.transform.apply_graph_transforms</span></code> is simplified API over <code class="docutils literal notranslate"><span class="pre">ToMixedPrecision</span></code> pass to get desired precision.</p>
<p>We can then compile our model in any convinient way</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span>  <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
        <span class="n">mod</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">target_host</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>While using <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> python interface, the below arguments enables precision conversion to float16.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mixed_precision</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="n">mixed_precision_ops</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;nn.conv2d&quot;</span><span class="p">,</span> <span class="s2">&quot;nn.dense&quot;</span><span class="p">],</span>
<span class="n">mixed_precision_calculation_type</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span><span class="p">,</span>
<span class="n">mixed_precision_acc_type</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span>
</pre></div>
</div>
<p>Similarly, <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> command line interface option bas below listed options.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--mixed-precision
--mixed-precision-ops<span class="w"> </span>nn.conv2d<span class="w"> </span>nn.dense
--mixed-precision-calculation-type<span class="w"> </span>float16
--mixed-precision-acc-type<span class="w"> </span>float16
</pre></div>
</div>
<p><strong>float16_acc32 (Mixed Precision)</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">ToMixedPrecision</span></code> pass traverse over the network and split network to clusters of ops dealing with float or float16 data types.
The clusters are defined by three types of operations:
- Operations always be converted into float16 data type
- Operations which can be converted if they followed by converted cluster
- Operations never be converted to the float16 data type
This list is defined in the ToMixedPrecision implementation here
<a class="reference external" href="https://github.com/apache/tvm/blob/main/python/tvm/relay/transform/mixed_precision.py#L34">relay/transform/mixed_precision.py</a>
and can be overridden by user.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">ToMixedPrecision</span></code> method is a pass to convert an FP32 relay graph into an FP16 version (with
FP16 or FP32 accumulation dtypes). Doing this transformation is useful for reducing model size
as it halves the expected size of the weights (FP16_acc16 case).</p>
<p><code class="docutils literal notranslate"><span class="pre">ToMixedPrecision</span></code> pass usage is simplified into a simple call as shown below for usage.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm.driver.tvmc.transform</span> <span class="kn">import</span> <span class="n">apply_graph_transforms</span>
<span class="n">mod</span>  <span class="o">=</span> <span class="n">apply_graph_transforms</span><span class="p">(</span>
         <span class="n">mod</span><span class="p">,</span>
         <span class="p">{</span>
             <span class="s2">&quot;mixed_precision&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
             <span class="s2">&quot;mixed_precision_ops&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;nn.conv2d&quot;</span><span class="p">,</span> <span class="s2">&quot;nn.dense&quot;</span><span class="p">],</span>
             <span class="s2">&quot;mixed_precision_calculation_type&quot;</span><span class="p">:</span> <span class="s2">&quot;float16&quot;</span><span class="p">,</span>
             <span class="s2">&quot;mixed_precision_acc_type&quot;</span><span class="p">:</span> <span class="s2">&quot;float32&quot;</span><span class="p">,</span>
         <span class="p">},</span>
     <span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tvm.driver.tvmc.transform.apply_graph_transforms</span></code> is simplified API over <code class="docutils literal notranslate"><span class="pre">ToMixedPrecision</span></code> pass to get desired precision.</p>
<p>We can then compile our model in any convinient way</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span>  <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">):</span>
    <span class="n">lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span>
        <span class="n">mod</span><span class="p">,</span> <span class="n">target_host</span><span class="o">=</span><span class="n">target_host</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>While using <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> python interface, the below arguments enables precision conversion to float16.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mixed_precision</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="n">mixed_precision_ops</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;nn.conv2d&quot;</span><span class="p">,</span> <span class="s2">&quot;nn.dense&quot;</span><span class="p">],</span>
<span class="n">mixed_precision_calculation_type</span> <span class="o">=</span> <span class="s2">&quot;float16&quot;</span><span class="p">,</span>
<span class="n">mixed_precision_acc_type</span> <span class="o">=</span> <span class="s2">&quot;float32&quot;</span>
</pre></div>
</div>
<p>Similarly, <code class="docutils literal notranslate"><span class="pre">tvmc</span></code> command line interface option bas below listed options.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>--mixed-precision
--mixed-precision-ops<span class="w"> </span>nn.conv2d<span class="w"> </span>nn.dense
--mixed-precision-calculation-type<span class="w"> </span>float16
--mixed-precision-acc-type<span class="w"> </span>float32
</pre></div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="integrate.html" class="btn btn-neutral float-right" title="Integrate TVM into Your Project" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="android.html" class="btn btn-neutral float-left" title="Deploy to Android" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2023 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>