{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# microTVM PyTorch Tutorial\n**Authors**:\n[Mehrdad Hessar](https://github.com/mehrdadh)\n\nThis tutorial is showcasing microTVM host-driven AoT compilation with\na PyTorch model. This tutorial can be executed on a x86 CPU using C runtime (CRT).\n\n**Note:** This tutorial only runs on x86 CPU using CRT and does not run on Zephyr\nsince the model would not fit on our current supported Zephyr boards.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pathlib\n\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nfrom PIL import Image\n\nimport tvm\nfrom tvm import relay\nfrom tvm.contrib.download import download_testdata\nfrom tvm.relay.backend import Executor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a pre-trained PyTorch model\n\nTo begin with, load pre-trained MobileNetV2 from torchvision. Then,\ndownload a cat image and preprocess it to use as the model input.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = torchvision.models.quantization.mobilenet_v2(weights=\"DEFAULT\", quantize=True)\nmodel = model.eval()\n\ninput_shape = [1, 3, 224, 224]\ninput_data = torch.randn(input_shape)\nscripted_model = torch.jit.trace(model, input_data).eval()\n\nimg_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\nimg_path = download_testdata(img_url, \"cat.png\", module=\"data\")\nimg = Image.open(img_path).resize((224, 224))\n\n# Preprocess the image and convert to tensor\nmy_preprocess = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\nimg = my_preprocess(img)\nimg = np.expand_dims(img, 0)\n\ninput_name = \"input0\"\nshape_list = [(input_name, input_shape)]\nrelay_mod, params = relay.frontend.from_pytorch(scripted_model, shape_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Target, Runtime and Executor\n\nIn this tutorial we use AOT host-driven executor. To compile the model\nfor an emulated embedded environment on an x86 machine we use C runtime (CRT)\nand we use `host` micro target. Using this setup, TVM compiles the model\nfor C runtime which can run on a x86 CPU machine with the same flow that\nwould run on a physical microcontroller.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Simulate a microcontroller on the host machine. Uses the main() from `src/runtime/crt/host/main.cc`\n# To use physical hardware, replace \"host\" with another physical micro target, e.g. `nrf52840`\n# or `mps2_an521`. See more more target examples in micro_train.py and micro_tflite.py tutorials.\ntarget = tvm.target.target.micro(\"host\")\n\n# Use the C runtime (crt) and enable static linking by setting system-lib to True\nruntime = tvm.relay.backend.Runtime(\"crt\", {\"system-lib\": True})\n\n# Use the AOT executor rather than graph or vm executors. Don't use unpacked API or C calling style.\nexecutor = Executor(\"aot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile the model\n\nNow, we compile the model for the target:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with tvm.transform.PassContext(\n    opt_level=3,\n    config={\"tir.disable_vectorize\": True},\n):\n    module = tvm.relay.build(\n        relay_mod, target=target, runtime=runtime, executor=executor, params=params\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a microTVM project\n\nNow that we have the compiled model as an IRModule, we need to create a firmware project\nto use the compiled model with microTVM. To do this, we use Project API.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "template_project_path = pathlib.Path(tvm.micro.get_microtvm_template_projects(\"crt\"))\nproject_options = {\"verbose\": False, \"memory_size_bytes\": 6 * 1024 * 1024}\n\ntemp_dir = tvm.contrib.utils.tempdir() / \"project\"\nproject = tvm.micro.generate_project(\n    str(template_project_path),\n    module,\n    temp_dir,\n    project_options,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build, flash and execute the model\nNext, we build the microTVM project and flash it. Flash step is specific to\nphysical microcontroller and it is skipped if it is simulating a microcontroller\nvia the host `main.cc`` or if a Zephyr emulated board is selected as the target.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "project.build()\nproject.flash()\n\ninput_data = {input_name: tvm.nd.array(img.astype(\"float32\"))}\nwith tvm.micro.Session(project.transport()) as session:\n    aot_executor = tvm.runtime.executor.aot_executor.AotModule(session.create_aot_executor())\n    aot_executor.set_input(**input_data)\n    aot_executor.run()\n    result = aot_executor.get_output(0).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Look up synset name\nLook up prediction top 1 index in 1000 class synset.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "synset_url = (\n    \"https://raw.githubusercontent.com/Cadene/\"\n    \"pretrained-models.pytorch/master/data/\"\n    \"imagenet_synsets.txt\"\n)\nsynset_name = \"imagenet_synsets.txt\"\nsynset_path = download_testdata(synset_url, synset_name, module=\"data\")\nwith open(synset_path) as f:\n    synsets = f.readlines()\n\nsynsets = [x.strip() for x in synsets]\nsplits = [line.split(\" \") for line in synsets]\nkey_to_classname = {spl[0]: \" \".join(spl[1:]) for spl in splits}\n\nclass_url = (\n    \"https://raw.githubusercontent.com/Cadene/\"\n    \"pretrained-models.pytorch/master/data/\"\n    \"imagenet_classes.txt\"\n)\nclass_path = download_testdata(class_url, \"imagenet_classes.txt\", module=\"data\")\nwith open(class_path) as f:\n    class_id_to_key = f.readlines()\n\nclass_id_to_key = [x.strip() for x in class_id_to_key]\n\n# Get top-1 result for TVM\ntop1_tvm = np.argmax(result)\ntvm_class_key = class_id_to_key[top1_tvm]\n\n# Convert input to PyTorch variable and get PyTorch result for comparison\nwith torch.no_grad():\n    torch_img = torch.from_numpy(img)\n    output = model(torch_img)\n\n    # Get top-1 result for PyTorch\n    top1_torch = np.argmax(output.numpy())\n    torch_class_key = class_id_to_key[top1_torch]\n\nprint(\"Relay top-1 id: {}, class name: {}\".format(top1_tvm, key_to_classname[tvm_class_key]))\nprint(\"Torch top-1 id: {}, class name: {}\".format(top1_torch, key_to_classname[torch_class_key]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}