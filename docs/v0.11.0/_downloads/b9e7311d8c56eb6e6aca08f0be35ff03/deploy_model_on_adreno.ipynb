{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Deploy the Pretrained Model on Adreno\n**Author**: Daniil Barinov\n\nThis article is a step-by-step tutorial to deploy pretrained Pytorch ResNet-18 model on Adreno (on different precisions).\n\nFor us to begin with, PyTorch must be installed.\nTorchVision is also required since we will be using it as our model zoo.\n\nA quick solution is to install it via pip:\n\n```bash\npip install torch\npip install torchvision\n```\nBesides that, you should have TVM builded for Android.\nSee the following instructions on how to build it.\n\n[Deploy to Adreno GPU](https://tvm.apache.org/docs/how_to/deploy/adreno.html)\n\nAfter the build section there should be two files in *build* directory \u00ablibtvm_runtime.so\u00bb and \u00abtvm_rpc\u00bb.\nLet's push them to the device and run TVM RPC Server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TVM RPC Server\nTo get the hash of the device use:\n\n```bash\nadb devices\n```\nThen to upload these two files to the device you should use:\n\n```bash\nadb -s <device_hash> push {libtvm_runtime.so,tvm_rpc} /data/local/tmp\n```\nAt this moment you will have \u00ablibtvm_runtime.so\u00bb and \u00abtvm_rpc\u00bb on path /data/local/tmp on your device.\nSometimes cmake can\u2019t find \u00ablibc++_shared.so\u00bb. Use:\n\n```bash\nfind ${ANDROID_NDK_HOME} -name libc++_shared.so\n```\nto find it and also push it with adb on the desired device:\n\n```bash\nadb -s <device_hash> push libc++_shared.so /data/local/tmp\n```\nWe are now ready to run the TVM RPC Server.\nLaunch rpc_tracker with following line in 1st console:\n\n```bash\npython3 -m tvm.exec.rpc_tracker --port 9190\n```\nThen we need to run tvm_rpc server from under the desired device in 2nd console:\n\n```bash\nadb -s <device_hash> reverse tcp:9190 tcp:9190\nadb -s <device_hash> forward tcp:9090 tcp:9090\nadb -s <device_hash> forward tcp:9091 tcp:9091\nadb -s <device_hash> forward tcp:9092 tcp:9092\nadb -s <device_hash> forward tcp:9093 tcp:9093\nadb -s <device_hash> shell LD_LIBRARY_PATH=/data/local/tmp /data/local/tmp/tvm_rpc server --host=0.0.0.0 --port=9090 --tracker=127.0.0.1:9190 --key=android --port-end=9190\n```\nBefore proceeding to compile and infer model, specify TVM_TRACKER_HOST and TVM_TRACKER_PORT\n\n```bash\nexport TVM_TRACKER_HOST=0.0.0.0\nexport TVM_TRACKER_PORT=9190\n```\ncheck that the tracker is running and the device is available\n\n```bash\npython -m tvm.exec.query_rpc_tracker --port 9190\n```\nFor example, if we have 1 Android device,\nthe output can be:\n\n```bash\nQueue Status\n----------------------------------\nkey          total  free  pending\n----------------------------------\nandroid      1      1     0\n----------------------------------\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a test image\nAs an example we would use classical cat image from ImageNet\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image\nfrom tvm.contrib.download import download_testdata\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimg_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\nimg_path = download_testdata(img_url, \"cat.png\", module=\"data\")\nimg = Image.open(img_path).resize((224, 224))\nplt.imshow(img)\nplt.show()\n\n# Preprocess the image and convert to tensor\nfrom torchvision import transforms\n\nmy_preprocess = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\nimg = my_preprocess(img)\nimg = np.expand_dims(img, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load pretrained Pytorch model\nCreate a Relay graph from a Pytorch ResNet-18 model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport torch\nimport torchvision\nimport tvm\nfrom tvm import te\nfrom tvm import relay, rpc\nfrom tvm.contrib import utils, ndk\nfrom tvm.contrib import graph_executor\n\nmodel_name = \"resnet18\"\nmodel = getattr(torchvision.models, model_name)(pretrained=True)\nmodel = model.eval()\n\n# We grab the TorchScripted model via tracing\ninput_shape = [1, 3, 224, 224]\ninput_data = torch.randn(input_shape)\nscripted_model = torch.jit.trace(model, input_data).eval()\n\n# Input name can be arbitrary\ninput_name = \"input0\"\nshape_list = [(input_name, img.shape)]\nmod, params = relay.frontend.from_pytorch(scripted_model, shape_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precisions\nSince TVM support Mixed Precision, we need to register mixed_precision_conversion:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.relay.op import register_mixed_precision_conversion\n\nconv2d_acc = \"float32\"\n\n\n@register_mixed_precision_conversion(\"nn.conv2d\", level=11)\ndef conv2d_mixed_precision_rule(call_node: \"relay.Call\", mixed_precision_type: str):\n    global conv2d_acc\n    return [\n        relay.transform.mixed_precision.MIXED_PRECISION_ALWAYS,\n        conv2d_acc,\n        mixed_precision_type,\n    ]\n\n\n@register_mixed_precision_conversion(\"nn.dense\", level=11)\ndef conv2d_mixed_precision_rule(call_node: \"relay.Call\", mixed_precision_type: str):\n    global conv2d_acc\n    return [\n        relay.transform.mixed_precision.MIXED_PRECISION_ALWAYS,\n        conv2d_acc,\n        mixed_precision_type,\n    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and also define the conversion function itself\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def convert_to_dtype(mod, dtype):\n    # downcast to float16\n    if dtype == \"float16\" or dtype == \"float16_acc32\":\n        global conv2d_acc\n        conv2d_acc = \"float16\" if dtype == \"float16\" else \"float32\"\n        from tvm.ir import IRModule\n\n        mod = IRModule.from_expr(mod)\n        seq = tvm.transform.Sequential(\n            [relay.transform.InferType(), relay.transform.ToMixedPrecision()]\n        )\n        with tvm.transform.PassContext(opt_level=3):\n            mod = seq(mod)\n    return mod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's choose \"float16_acc32\" for example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dtype = \"float16_acc32\"\nmod = convert_to_dtype(mod[\"main\"], dtype)\ndtype = \"float32\" if dtype == \"float32\" else \"float16\"\n\nprint(mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see in the IR, the architecture now contains cast operations, which are\nneeded to convert to FP16 precision.\nYou can also use \"float16\" or \"float32\" precisions as other dtype options.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compile the model with relay\nSpecify Adreno target before compiling to generate texture\nleveraging kernels and get all the benefits of textures\nNote: This generated example running on our x86 server for demonstration.\nIf running it on the Android device, we need to\nspecify its instruction set. Set :code:`local_demo` to False if you want\nto run this tutorial with a real device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "local_demo = True\n\n# by default on CPU target will execute.\n# select 'cpu', 'opencl' and 'vulkan'\ntest_target = \"cpu\"\n\n# Change target configuration.\n# Run `adb shell cat /proc/cpuinfo` to find the arch.\narch = \"arm64\"\ntarget = tvm.target.Target(\"llvm -mtriple=%s-linux-android\" % arch)\n\nif local_demo:\n    target = tvm.target.Target(\"llvm\")\nelif test_target == \"opencl\":\n    target = tvm.target.Target(\"opencl\", host=target)\nelif test_target == \"vulkan\":\n    target = tvm.target.Target(\"vulkan\", host=target)\n\nwith tvm.transform.PassContext(opt_level=3):\n    lib = relay.build(mod, target=target, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Model Remotely by RPC\nUsing RPC you can deploy the model from host\nmachine to the remote Adreno device\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "rpc_tracker_host = os.environ.get(\"TVM_TRACKER_HOST\", \"127.0.0.1\")\nrpc_tracker_port = int(os.environ.get(\"TVM_TRACKER_PORT\", 9190))\nkey = \"android\"\n\nif local_demo:\n    remote = rpc.LocalSession()\nelse:\n    tracker = rpc.connect_tracker(rpc_tracker_host, rpc_tracker_port)\n    # When running a heavy model, we should increase the `session_timeout`\n    remote = tracker.request(key, priority=0, session_timeout=60)\n\nif local_demo:\n    dev = remote.cpu(0)\nelif test_target == \"opencl\":\n    dev = remote.cl(0)\nelif test_target == \"vulkan\":\n    dev = remote.vulkan(0)\nelse:\n    dev = remote.cpu(0)\n\ntemp = utils.tempdir()\ndso_binary = \"dev_lib_cl.so\"\ndso_binary_path = temp.relpath(dso_binary)\nfcompile = ndk.create_shared if not local_demo else None\nlib.export_library(dso_binary_path, fcompile)\nremote_path = \"/data/local/tmp/\" + dso_binary\nremote.upload(dso_binary_path)\nrlib = remote.load_module(dso_binary)\nm = graph_executor.GraphModule(rlib[\"default\"](dev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run inference\nWe now can set inputs, infer our model and get predictions as output\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m.set_input(input_name, tvm.nd.array(img.astype(\"float32\")))\nm.run()\ntvm_output = m.get_output(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get predictions and performance statistic\nThis piece of code displays the top-1 and top-5 predictions, as\nwell as provides information about the model's performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from os.path import join, isfile\nfrom matplotlib import pyplot as plt\nfrom tvm.contrib import download\n\n# Download ImageNet categories\ncateg_url = \"https://github.com/uwsampl/web-data/raw/main/vta/models/\"\ncateg_fn = \"synset.txt\"\ndownload.download(join(categ_url, categ_fn), categ_fn)\nsynset = eval(open(categ_fn).read())\n\ntop_categories = np.argsort(tvm_output.asnumpy()[0])\ntop5 = np.flip(top_categories, axis=0)[:5]\n\n# Report top-1 classification result\nprint(\"Top-1 id: {}, class name: {}\".format(top5[1 - 1], synset[top5[1 - 1]]))\n\n# Report top-5 classification results\nprint(\"\\nTop5 predictions: \\n\")\nprint(\"\\t#1:\", synset[top5[1 - 1]])\nprint(\"\\t#2:\", synset[top5[2 - 1]])\nprint(\"\\t#3:\", synset[top5[3 - 1]])\nprint(\"\\t#4:\", synset[top5[4 - 1]])\nprint(\"\\t#5:\", synset[top5[5 - 1]])\nprint(\"\\t\", top5)\nImageNetClassifier = False\nfor k in top_categories[-5:]:\n    if \"cat\" in synset[k]:\n        ImageNetClassifier = True\nassert ImageNetClassifier, \"Failed ImageNet classifier validation check\"\n\nprint(\"Evaluate inference time cost...\")\nprint(m.benchmark(dev, number=1, repeat=10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}