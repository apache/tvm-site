{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nGet Started with Tensor Expression\n==================================\n**Author**: `Tianqi Chen <https://tqchen.github.io>`_\n\nThis is an introductory tutorial to the Tensor expression language in TVM.\nTVM uses a domain specific tensor expression for efficient kernel construction.\n\nIn this tutorial, we will demonstrate the basic workflow to use\nthe tensor expression language.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import absolute_import, print_function\n\nimport tvm\nimport tvm.testing\nfrom tvm import te\nimport numpy as np\n\n# Global declarations of environment.\n\ntgt_host = \"llvm\"\n# Change it to respective GPU if gpu is enabled Ex: cuda, opencl, rocm\ntgt = \"cuda\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vector Add Example\n------------------\nIn this tutorial, we will use a vector addition example to demonstrate\nthe workflow.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Describe the Computation\n------------------------\nAs a first step, we need to describe our computation.\nTVM adopts tensor semantics, with each intermediate result\nrepresented as a multi-dimensional array. The user needs to describe\nthe computation rule that generates the tensors.\n\nWe first define a symbolic variable n to represent the shape.\nWe then define two placeholder Tensors, A and B, with given shape (n,)\n\nWe then describe the result tensor C, with a compute operation.  The\ncompute function takes the shape of the tensor, as well as a lambda\nfunction that describes the computation rule for each position of\nthe tensor.\n\nNo computation happens during this phase, as we are only declaring how\nthe computation should be done.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n = te.var(\"n\")\nA = te.placeholder((n,), name=\"A\")\nB = te.placeholder((n,), name=\"B\")\nC = te.compute(A.shape, lambda i: A[i] + B[i], name=\"C\")\nprint(type(C))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Schedule the Computation\n------------------------\nWhile the above lines describe the computation rule, we can compute\nC in many ways since the axis of C can be computed in a data\nparallel manner.  TVM asks the user to provide a description of the\ncomputation called a schedule.\n\nA schedule is a set of transformation of computation that transforms\nthe loop of computations in the program.\n\nAfter we construct the schedule, by default the schedule computes\nC in a serial manner in a row-major order.\n\n.. code-block:: c\n\n  for (int i = 0; i < n; ++i) {\n    C[i] = A[i] + B[i];\n  }\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "s = te.create_schedule(C.op)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We used the split construct to split the first axis of C,\nthis will split the original iteration axis into product of\ntwo iterations. This is equivalent to the following code.\n\n.. code-block:: c\n\n  for (int bx = 0; bx < ceil(n / 64); ++bx) {\n    for (int tx = 0; tx < 64; ++tx) {\n      int i = bx * 64 + tx;\n      if (i < n) {\n        C[i] = A[i] + B[i];\n      }\n    }\n  }\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bx, tx = s[C].split(C.op.axis[0], factor=64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally we bind the iteration axis bx and tx to threads in the GPU\ncompute grid. These are GPU specific constructs that allow us\nto generate code that runs on GPU.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if tgt == \"cuda\" or tgt == \"rocm\" or tgt.startswith(\"opencl\"):\n    s[C].bind(bx, te.thread_axis(\"blockIdx.x\"))\n    s[C].bind(tx, te.thread_axis(\"threadIdx.x\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compilation\n-----------\nAfter we have finished specifying the schedule, we can compile it\ninto a TVM function. By default TVM compiles into a type-erased\nfunction that can be directly called from the python side.\n\nIn the following line, we use tvm.build to create a function.\nThe build function takes the schedule, the desired signature of the\nfunction (including the inputs and outputs) as well as target language\nwe want to compile to.\n\nThe result of compilation fadd is a GPU device function (if GPU is\ninvolved) as well as a host wrapper that calls into the GPU\nfunction.  fadd is the generated host wrapper function, it contains\na reference to the generated device function internally.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd = tvm.build(s, [A, B, C], tgt, target_host=tgt_host, name=\"myadd\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the Function\n----------------\nThe compiled TVM function is exposes a concise C API\nthat can be invoked from any language.\n\nWe provide a minimal array API in python to aid quick testing and prototyping.\nThe array API is based on the `DLPack <https://github.com/dmlc/dlpack>`_ standard.\n\n- We first create a GPU context.\n- Then tvm.nd.array copies the data to the GPU.\n- fadd runs the actual computation.\n- asnumpy() copies the GPU array back to the CPU and we can use this to verify correctness\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ctx = tvm.context(tgt, 0)\n\nn = 1024\na = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)\nb = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)\nc = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)\nfadd(a, b, c)\ntvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inspect the Generated Code\n--------------------------\nYou can inspect the generated code in TVM. The result of tvm.build\nis a TVM Module. fadd is the host module that contains the host wrapper,\nit also contains a device module for the CUDA (GPU) function.\n\nThe following code fetches the device module and prints the content code.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if tgt == \"cuda\" or tgt == \"rocm\" or tgt.startswith(\"opencl\"):\n    dev_module = fadd.imported_modules[0]\n    print(\"-----GPU code-----\")\n    print(dev_module.get_source())\nelse:\n    print(fadd.get_source())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Code Specialization\n\n  As you may have noticed, the declarations of A, B and C all\n  take the same shape argument, n. TVM will take advantage of this\n  to pass only a single shape argument to the kernel, as you will find in\n  the printed device code. This is one form of specialization.\n\n  On the host side, TVM will automatically generate check code\n  that checks the constraints in the parameters. So if you pass\n  arrays with different shapes into fadd, an error will be raised.\n\n  We can do more specializations. For example, we can write\n  :code:`n = tvm.runtime.convert(1024)` instead of :code:`n = te.var(\"n\")`,\n  in the computation declaration. The generated function will\n  only take vectors with length 1024.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Save Compiled Module\n--------------------\nBesides runtime compilation, we can save the compiled modules into\na file and load them back later. This is called ahead of time compilation.\n\nThe following code first performs the following steps:\n\n- It saves the compiled host module into an object file.\n- Then it saves the device module into a ptx file.\n- cc.create_shared calls a compiler (gcc) to create a shared library\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.contrib import cc\nfrom tvm.contrib import util\n\ntemp = util.tempdir()\nfadd.save(temp.relpath(\"myadd.o\"))\nif tgt == \"cuda\":\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.ptx\"))\nif tgt == \"rocm\":\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.hsaco\"))\nif tgt.startswith(\"opencl\"):\n    fadd.imported_modules[0].save(temp.relpath(\"myadd.cl\"))\ncc.create_shared(temp.relpath(\"myadd.so\"), [temp.relpath(\"myadd.o\")])\nprint(temp.listdir())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Module Storage Format\n\n  The CPU (host) module is directly saved as a shared library (.so).\n  There can be multiple customized formats of the device code.\n  In our example, the device code is stored in ptx, as well as a meta\n  data json file. They can be loaded and linked separately via import.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load Compiled Module\n--------------------\nWe can load the compiled module from the file system and run the code.\nThe following code loads the host and device module separately and\nre-links them together. We can verify that the newly loaded function works.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd1 = tvm.runtime.load_module(temp.relpath(\"myadd.so\"))\nif tgt == \"cuda\":\n    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.ptx\"))\n    fadd1.import_module(fadd1_dev)\n\nif tgt == \"rocm\":\n    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.hsaco\"))\n    fadd1.import_module(fadd1_dev)\n\nif tgt.startswith(\"opencl\"):\n    fadd1_dev = tvm.runtime.load_module(temp.relpath(\"myadd.cl\"))\n    fadd1.import_module(fadd1_dev)\n\nfadd1(a, b, c)\ntvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pack Everything into One Library\n--------------------------------\nIn the above example, we store the device and host code separately.\nTVM also supports export everything as one shared library.\nUnder the hood, we pack the device modules into binary blobs and link\nthem together with the host code.\nCurrently we support packing of Metal, OpenCL and CUDA modules.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fadd.export_library(temp.relpath(\"myadd_pack.so\"))\nfadd2 = tvm.runtime.load_module(temp.relpath(\"myadd_pack.so\"))\nfadd2(a, b, c)\ntvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Runtime API and Thread-Safety\n\n  The compiled modules of TVM do not depend on the TVM compiler.\n  Instead, they only depend on a minimum runtime library.\n  The TVM runtime library wraps the device drivers and provides\n  thread-safe and device agnostic calls into the compiled functions.\n\n  This means that you can call the compiled TVM functions from any thread,\n  on any GPUs.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Generate OpenCL Code\n--------------------\nTVM provides code generation features into multiple backends,\nwe can also generate OpenCL code or LLVM code that runs on CPU backends.\n\nThe following code blocks generate OpenCL code, creates array on an OpenCL\ndevice, and verifies the correctness of the code.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if tgt.startswith(\"opencl\"):\n    fadd_cl = tvm.build(s, [A, B, C], tgt, name=\"myadd\")\n    print(\"------opencl code------\")\n    print(fadd_cl.imported_modules[0].get_source())\n    ctx = tvm.cl(0)\n    n = 1024\n    a = tvm.nd.array(np.random.uniform(size=n).astype(A.dtype), ctx)\n    b = tvm.nd.array(np.random.uniform(size=n).astype(B.dtype), ctx)\n    c = tvm.nd.array(np.zeros(n, dtype=C.dtype), ctx)\n    fadd_cl(a, b, c)\n    tvm.testing.assert_allclose(c.asnumpy(), a.asnumpy() + b.asnumpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n-------\nThis tutorial provides a walk through of TVM workflow using\na vector add example. The general workflow is\n\n- Describe your computation via a series of operations.\n- Describe how we want to compute use schedule primitives.\n- Compile to the target function we want.\n- Optionally, save the function to be loaded later.\n\nYou are more than welcome to checkout other examples and\ntutorials to learn more about the supported operations, scheduling primitives\nand other features in TVM.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}