{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Relax Creation\nThis tutorial demonstrates how to create Relax functions and programs.\nWe'll cover various ways to define Relax functions, including using TVMScript,\nand relax NNModule API.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Relax programs using TVMScript\nTVMScript is a domain-specific language for representing Apache TVM's\nintermediate representation (IR). It is a Python dialect that can be used\nto define an IRModule, which contains both TensorIR and Relax functions.\n\nIn this section, we will show how to define a simple MLP model with only\nhigh-level Relax operators using TVMScript.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm import relax, topi\nfrom tvm.script import ir as I\nfrom tvm.script import relax as R\nfrom tvm.script import tir as T\n\n\n@I.ir_module\nclass RelaxModule:\n    @R.function\n    def forward(\n        data: R.Tensor((\"n\", 784), dtype=\"float32\"),\n        w0: R.Tensor((128, 784), dtype=\"float32\"),\n        b0: R.Tensor((128,), dtype=\"float32\"),\n        w1: R.Tensor((10, 128), dtype=\"float32\"),\n        b1: R.Tensor((10,), dtype=\"float32\"),\n    ) -> R.Tensor((\"n\", 10), dtype=\"float32\"):\n        with R.dataflow():\n            lv0 = R.matmul(data, R.permute_dims(w0)) + b0\n            lv1 = R.nn.relu(lv0)\n            lv2 = R.matmul(lv1, R.permute_dims(w1)) + b1\n            R.output(lv2)\n        return lv2\n\n\nRelaxModule.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Relax is not only a graph-level IR, but also supports cross-level\nrepresentation and transformation. To be specific, we can directly call\nTensorIR functions in Relax function.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@I.ir_module\nclass RelaxModuleWithTIR:\n    @T.prim_func\n    def relu(x: T.handle, y: T.handle):\n        n, m = T.int64(), T.int64()\n        X = T.match_buffer(x, (n, m), \"float32\")\n        Y = T.match_buffer(y, (n, m), \"float32\")\n        for i, j in T.grid(n, m):\n            with T.block(\"relu\"):\n                vi, vj = T.axis.remap(\"SS\", [i, j])\n                Y[vi, vj] = T.max(X[vi, vj], T.float32(0))\n\n    @R.function\n    def forward(\n        data: R.Tensor((\"n\", 784), dtype=\"float32\"),\n        w0: R.Tensor((128, 784), dtype=\"float32\"),\n        b0: R.Tensor((128,), dtype=\"float32\"),\n        w1: R.Tensor((10, 128), dtype=\"float32\"),\n        b1: R.Tensor((10,), dtype=\"float32\"),\n    ) -> R.Tensor((\"n\", 10), dtype=\"float32\"):\n        n = T.int64()\n        cls = RelaxModuleWithTIR\n        with R.dataflow():\n            lv0 = R.matmul(data, R.permute_dims(w0)) + b0\n            lv1 = R.call_tir(cls.relu, lv0, R.Tensor((n, 128), dtype=\"float32\"))\n            lv2 = R.matmul(lv1, R.permute_dims(w1)) + b1\n            R.output(lv2)\n        return lv2\n\n\nRelaxModuleWithTIR.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>You may notice that the printed output is different from the written\n  TVMScript code. This is because we print the IRModule in a standard\n  format, while we support syntax sugar for the input\n\n  For example, we can combine multiple operators into a single line, as\n\n```python\nlv0 = R.matmul(data, R.permute_dims(w0)) + b0\n```\n  However, the normalized expression requires only one operation in one\n  binding. So the printed output is different from the written TVMScript code,\n  as\n\n```python\nlv: R.Tensor((784, 128), dtype=\"float32\") = R.permute_dims(w0, axes=None)\nlv1: R.Tensor((n, 128), dtype=\"float32\") = R.matmul(data, lv, out_dtype=\"void\")\nlv0: R.Tensor((n, 128), dtype=\"float32\") = R.add(lv1, b0)</p></div>\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Relax programs using NNModule API\nBesides TVMScript, we also provide a PyTorch-like API for defining neural networks.\nIt is designed to be more intuitive and easier to use than TVMScript.\n\nIn this section, we will show how to define the same MLP model using\nRelax NNModule API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.relax.frontend import nn\n\n\nclass NNModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After we define the NNModule, we can export it to TVM IRModule via\n``export_tvm``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod, params = NNModule().export_tvm({\"forward\": {\"x\": nn.spec.Tensor((\"n\", 784), \"float32\")}})\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also insert customized function calls into the NNModule, such as\nTensor Expression(TE), TensorIR functions or other TVM packed functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@T.prim_func\ndef tir_linear(x: T.handle, w: T.handle, b: T.handle, z: T.handle):\n    M, N, K = T.int64(), T.int64(), T.int64()\n    X = T.match_buffer(x, (M, K), \"float32\")\n    W = T.match_buffer(w, (N, K), \"float32\")\n    B = T.match_buffer(b, (N,), \"float32\")\n    Z = T.match_buffer(z, (M, N), \"float32\")\n    for i, j, k in T.grid(M, N, K):\n        with T.block(\"linear\"):\n            vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n            with T.init():\n                Z[vi, vj] = 0\n            Z[vi, vj] = Z[vi, vj] + X[vi, vk] * W[vj, vk]\n    for i, j in T.grid(M, N):\n        with T.block(\"add\"):\n            vi, vj = T.axis.remap(\"SS\", [i, j])\n            Z[vi, vj] = Z[vi, vj] + B[vj]\n\n\nclass NNModuleWithTIR(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        n = x.shape[0]\n        # We can call external functions using nn.extern\n        x = nn.extern(\n            \"env.linear\",\n            [x, self.fc1.weight, self.fc1.bias],\n            out=nn.Tensor.placeholder((n, 128), \"float32\"),\n        )\n        # We can also call TensorIR via Tensor Expression API in TOPI\n        x = nn.tensor_expr_op(topi.nn.relu, \"relu\", [x])\n        # We can also call other TVM packed functions\n        x = nn.tensor_ir_op(\n            tir_linear,\n            \"tir_linear\",\n            [x, self.fc2.weight, self.fc2.bias],\n            out=nn.Tensor.placeholder((n, 10), \"float32\"),\n        )\n        return x\n\n\nmod, params = NNModuleWithTIR().export_tvm(\n    {\"forward\": {\"x\": nn.spec.Tensor((\"n\", 784), \"float32\")}}\n)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Relax programs using Block Builder API\nIn addition to the above APIs, we also provide a Block Builder API for\ncreating Relax programs. It is a IR builder API, which is more\nlow-level and widely used in TVM's internal logic, e.g writing a\ncustomized pass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bb = relax.BlockBuilder()\nn = T.int64()\nx = relax.Var(\"x\", R.Tensor((n, 784), \"float32\"))\nfc1_weight = relax.Var(\"fc1_weight\", R.Tensor((128, 784), \"float32\"))\nfc1_bias = relax.Var(\"fc1_bias\", R.Tensor((128,), \"float32\"))\nfc2_weight = relax.Var(\"fc2_weight\", R.Tensor((10, 128), \"float32\"))\nfc2_bias = relax.Var(\"fc2_bias\", R.Tensor((10,), \"float32\"))\nwith bb.function(\"forward\", [x, fc1_weight, fc1_bias, fc2_weight, fc2_bias]):\n    with bb.dataflow():\n        lv0 = bb.emit(relax.op.matmul(x, relax.op.permute_dims(fc1_weight)) + fc1_bias)\n        lv1 = bb.emit(relax.op.nn.relu(lv0))\n        gv = bb.emit(relax.op.matmul(lv1, relax.op.permute_dims(fc2_weight)) + fc2_bias)\n        bb.emit_output(gv)\n    bb.emit_func_output(gv)\n\nmod = bb.get()\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Also, Block Builder API supports building cross-level IRModule with both\nRelax functions, TensorIR functions and other TVM packed functions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "bb = relax.BlockBuilder()\nwith bb.function(\"forward\", [x, fc1_weight, fc1_bias, fc2_weight, fc2_bias]):\n    with bb.dataflow():\n        lv0 = bb.emit(\n            relax.call_dps_packed(\n                \"env.linear\",\n                [x, fc1_weight, fc1_bias],\n                out_sinfo=relax.TensorStructInfo((n, 128), \"float32\"),\n            )\n        )\n        lv1 = bb.emit_te(topi.nn.relu, lv0)\n        tir_gv = bb.add_func(tir_linear, \"tir_linear\")\n        gv = bb.emit(\n            relax.call_tir(\n                tir_gv,\n                [lv1, fc2_weight, fc2_bias],\n                out_sinfo=relax.TensorStructInfo((n, 10), \"float32\"),\n            )\n        )\n        bb.emit_output(gv)\n    bb.emit_func_output(gv)\nmod = bb.get()\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the Block Builder API is not as user-friendly as the above APIs,\nbut it is lowest-level API and works closely with the IR definition. We\nrecommend using the above APIs for users who only want to define and\ntransform a ML model. But for those who want to build more complex\ntransformations, the Block Builder API is a more flexible choice.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nThis tutorial demonstrates how to create Relax programs using TVMScript,\nNNModule API, Block Builder API and PackedFunc API for different use cases.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}