{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Deploy the Pretrained Model on Adreno\u2122\n**Author**: Daniil Barinov, Siva Rama Krishna\n\nThis article is a step-by-step tutorial to deploy pretrained Pytorch ResNet-18 model on Adreno (on different precisions).\n\nFor us to begin with, PyTorch must be installed.\nTorchVision is also required since we will be using it as our model zoo.\n\nA quick solution is to install it via pip:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\npip install torch\npip install torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Besides that, you should have TVM builded for Android.\nSee the following instructions on how to build it.\n\n[Deploy to Adreno GPU](https://tvm.apache.org/docs/how_to/deploy/adreno.html)\n\nAfter the build section there should be two files in *build* directory \u00ablibtvm_runtime.so\u00bb and \u00abtvm_rpc\u00bb.\nLet's push them to the device and run TVM RPC Server.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TVM RPC Server\nTo get the hash of the device use:\n\n```bash\nadb devices\n```\nSet the android device to use, if you have several devices connected to your computer.\n\n```bash\nexport ANDROID_SERIAL=<device-hash>\n```\nThen to upload these two files to the device you should use:\n\n```bash\nadb push {libtvm_runtime.so,tvm_rpc} /data/local/tmp\n```\nAt this moment you will have \u00ablibtvm_runtime.so\u00bb and \u00abtvm_rpc\u00bb on path /data/local/tmp on your device.\nSometimes cmake can\u2019t find \u00ablibc++_shared.so\u00bb. Use:\n\n```bash\nfind ${ANDROID_NDK_HOME} -name libc++_shared.so\n```\nto find it and also push it with adb on the desired device:\n\n```bash\nadb push libc++_shared.so /data/local/tmp\n```\nWe are now ready to run the TVM RPC Server.\nLaunch rpc_tracker with following line in 1st console:\n\n```bash\npython3 -m tvm.exec.rpc_tracker --port 9190\n```\nThen we need to run tvm_rpc server from under the desired device in 2nd console:\n\n```bash\nadb reverse tcp:9190 tcp:9190\nadb forward tcp:5000 tcp:5000\nadb forward tcp:5002 tcp:5001\nadb forward tcp:5003 tcp:5002\nadb forward tcp:5004 tcp:5003\nadb shell LD_LIBRARY_PATH=/data/local/tmp /data/local/tmp/tvm_rpc server --host=0.0.0.0 --port=5000 --tracker=127.0.0.1:9190 --key=android --port-end=5100\n```\nBefore proceeding to compile and infer model, specify TVM_TRACKER_HOST and TVM_TRACKER_PORT\n\n```bash\nexport TVM_TRACKER_HOST=0.0.0.0\nexport TVM_TRACKER_PORT=9190\n```\ncheck that the tracker is running and the device is available\n\n```bash\npython -m tvm.exec.query_rpc_tracker --port 9190\n```\nFor example, if we have 1 Android device,\nthe output can be:\n\n```bash\nQueue Status\n----------------------------------\nkey          total  free  pending\n----------------------------------\nandroid      1      1     0\n----------------------------------\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport torch\nimport torchvision\nimport tvm\nfrom tvm import te\nfrom tvm import relay, rpc\nfrom tvm.contrib import utils, ndk\nfrom tvm.contrib import graph_executor\nfrom tvm.relay.op.contrib import clml\nfrom tvm import autotvm\n\n# Below are set of configuration that controls the behaviour of this script like\n# local run or device run, target definitions,  dtype setting and auto tuning enablement.\n# Change these settings as needed if required.\n\n# Adreno devices are efficient with float16 compared to float32\n# Given the expected output doesn't effect by lowering precision\n# it's advisable to use lower precision.\n# We have a helper API to make the precision conversion simple and\n# it supports dtype with \"float16\" and \"float16_acc32\" modes.\n# Let's choose \"float16\" for calculation and \"float32\" for accumulation.\n\ncalculation_dtype = \"float16\"\nacc_dtype = \"float32\"\n\n# Specify Adreno target before compiling to generate texture\n# leveraging kernels and get all the benefits of textures\n# Note: This generated example running on our x86 server for demonstration.\n# If running it on the Android device, we need to\n# specify its instruction set. Set :code:`local_demo` to False if you want\n# to run this tutorial with a real device over rpc.\nlocal_demo = True\n\n# by default on CPU target will execute.\n# select 'cpu', 'opencl' and 'opencl -device=adreno'\ntest_target = \"cpu\"\n\n# Change target configuration.\n# Run `adb shell cat /proc/cpuinfo` to find the arch.\narch = \"arm64\"\ntarget = tvm.target.Target(\"llvm -mtriple=%s-linux-android\" % arch)\n\n# Auto tuning is compute intensive and time taking task,\n# hence disabling for default run. Please enable it if required.\nis_tuning = False\ntune_log = \"adreno-resnet18.log\"\n\n# To enable OpenCLML accelerated operator library.\nenable_clml = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get a PyTorch Model\nGet resnet18 from torchvision models\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_name = \"resnet18\"\nmodel = getattr(torchvision.models, model_name)(pretrained=True)\nmodel = model.eval()\n\n# We grab the TorchScripted model via tracing\ninput_shape = [1, 3, 224, 224]\ninput_data = torch.randn(input_shape)\nscripted_model = torch.jit.trace(model, input_data).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load a test image\nAs an example we would use classical cat image from ImageNet\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from PIL import Image\nfrom tvm.contrib.download import download_testdata\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimg_url = \"https://github.com/dmlc/mxnet.js/blob/main/data/cat.png?raw=true\"\nimg_path = download_testdata(img_url, \"cat.png\", module=\"data\")\nimg = Image.open(img_path).resize((224, 224))\nplt.imshow(img)\nplt.show()\n\n# Preprocess the image and convert to tensor\nfrom torchvision import transforms\n\nmy_preprocess = transforms.Compose(\n    [\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ]\n)\nimg = my_preprocess(img)\nimg = np.expand_dims(img, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Convert PyTorch model to Relay module\nTVM has frontend api for various frameworks under relay.frontend and now\nfor pytorch model import we have relay.frontend.from_pytorch api.\nInput name can be arbitrary\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_name = \"input0\"\nshape_list = [(input_name, img.shape)]\n\nmod, params = relay.frontend.from_pytorch(scripted_model, shape_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precisions\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Adreno devices are efficient with float16 compared to float32\n# Given the expected output doesn't effect by lowering precision\n# it's advisable to use lower precision.\n\n# TVM support Mixed Precision through ToMixedPrecision transformation pass.\n# We may need to register precision rules like precision type, accumultation\n# datatype ...etc. for the required operators to override the default settings.\n# The below helper api simplifies the precision conversions across the module.\n\n# Calculation dtype is set to \"float16\" and accumulation dtype is set to \"float32\"\n# in configuration section above.\n\nfrom tvm.driver.tvmc.transform import apply_graph_transforms\n\nmod = apply_graph_transforms(\n    mod,\n    {\n        \"mixed_precision\": True,\n        \"mixed_precision_ops\": [\"nn.conv2d\", \"nn.dense\"],\n        \"mixed_precision_calculation_type\": calculation_dtype,\n        \"mixed_precision_acc_type\": acc_dtype,\n    },\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see in the IR, the architecture now contains cast operations, which are\nneeded to convert to FP16 precision.\nYou can also use \"float16\" or \"float32\" precisions as other dtype options.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare TVM Target\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# This generated example running on our x86 server for demonstration.\n\n# To deply and tun on real target over RPC please set :code:`local_demo` to False in above configuration sestion.\n# Also, :code:`test_target` is set to :code:`llvm` as this example to make compatible for x86 demonstration.\n# Please change it to :code:`opencl` or :code:`opencl -device=adreno` for RPC target in configuration above.\n\nif local_demo:\n    target = tvm.target.Target(\"llvm\")\nelif test_target.find(\"opencl\"):\n    target = tvm.target.Target(test_target, host=target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AutoTuning\nThe below few instructions can auto tune the relay module with xgboost being the tuner algorithm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Auto Tuning process involces stages of extracting the tasks, defining tuning congiguration and\n# tuning each task for best performing kernel configuration.\n\n# Get RPC related settings.\nrpc_tracker_host = os.environ.get(\"TVM_TRACKER_HOST\", \"127.0.0.1\")\nrpc_tracker_port = int(os.environ.get(\"TVM_TRACKER_PORT\", 9190))\nkey = \"android\"\n\n# Auto tuning is compute intensive and time taking task.\n# It is set to False in above configuration as this script runs in x86 for demonstration.\n# Please to set :code:`is_tuning` to True to enable auto tuning.\n\nif is_tuning:\n    # Auto Tuning Stage 1: Extract tunable tasks\n    tasks = autotvm.task.extract_from_program(\n        mod, target=test_target, target_host=target, params=params\n    )\n\n    # Auto Tuning Stage 2: Define tuning configuration\n    tmp_log_file = tune_log + \".tmp\"\n    measure_option = autotvm.measure_option(\n        builder=autotvm.LocalBuilder(\n            build_func=ndk.create_shared, timeout=15\n        ),  # Build the test kernel locally\n        runner=autotvm.RPCRunner(  # The runner would be on a remote device.\n            key,  # RPC Key\n            host=rpc_tracker_host,  # Tracker host\n            port=int(rpc_tracker_port),  # Tracker port\n            number=3,  # Number of runs before averaging\n            timeout=600,  # RPC Timeout\n        ),\n    )\n    n_trial = 1024  # Number of iteration of training before choosing the best kernel config\n    early_stopping = False  # Can be enabled to stop tuning while the loss is not minimizing.\n\n    # Auto Tuning Stage 3: Iterate through the tasks and tune.\n    from tvm.autotvm.tuner import XGBTuner\n\n    for i, tsk in enumerate(reversed(tasks[:3])):\n        print(\"Task:\", tsk)\n        prefix = \"[Task %2d/%2d] \" % (i + 1, len(tasks))\n        tuner_obj = XGBTuner(tsk, loss_type=\"rank\")\n\n        tsk_trial = min(n_trial, len(tsk.config_space))\n        tuner_obj.tune(\n            n_trial=tsk_trial,\n            early_stopping=early_stopping,\n            measure_option=measure_option,\n            callbacks=[\n                autotvm.callback.progress_bar(tsk_trial, prefix=prefix),\n                autotvm.callback.log_to_file(tmp_log_file),\n            ],\n        )\n    # Auto Tuning Stage 4: Pick the best performing configurations from the overall log.\n    autotvm.record.pick_best(tmp_log_file, tune_log)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enable OpenCLML Offloading\nOpenCLML offloading will try to accelerate supported operators\nby using OpenCLML proprietory operator library.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# By default :code:`enable_clml` is set to False in above configuration section.\n\nif not local_demo and enable_clml:\n    mod = clml.partition_for_clml(mod, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compilation\nUse tuning cache if exists.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if os.path.exists(tune_log):\n    with autotvm.apply_history_best(tune_log):\n        with tvm.transform.PassContext(opt_level=3):\n            lib = relay.build(mod, target=target, params=params)\nelse:\n    with tvm.transform.PassContext(opt_level=3):\n        lib = relay.build(mod, target=target, params=params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Model Remotely by RPC\nUsing RPC you can deploy the model from host\nmachine to the remote Adreno device\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if local_demo:\n    remote = rpc.LocalSession()\nelse:\n    tracker = rpc.connect_tracker(rpc_tracker_host, rpc_tracker_port)\n    # When running a heavy model, we should increase the `session_timeout`\n    remote = tracker.request(key, priority=0, session_timeout=60)\n\nif local_demo:\n    dev = remote.cpu(0)\nelif test_target.find(\"opencl\"):\n    dev = remote.cl(0)\nelse:\n    dev = remote.cpu(0)\n\ntemp = utils.tempdir()\ndso_binary = \"dev_lib_cl.so\"\ndso_binary_path = temp.relpath(dso_binary)\nfcompile = ndk.create_shared if not local_demo else None\nlib.export_library(dso_binary_path, fcompile)\nremote_path = \"/data/local/tmp/\" + dso_binary\nremote.upload(dso_binary_path)\nrlib = remote.load_module(dso_binary)\nm = graph_executor.GraphModule(rlib[\"default\"](dev))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run inference\nWe now can set inputs, infer our model and get predictions as output\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "m.set_input(input_name, tvm.nd.array(img.astype(\"float32\")))\nm.run()\ntvm_output = m.get_output(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Get predictions and performance statistic\nThis piece of code displays the top-1 and top-5 predictions, as\nwell as provides information about the model's performance\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from os.path import join, isfile\nfrom matplotlib import pyplot as plt\nfrom tvm.contrib import download\n\n# Download ImageNet categories\ncateg_url = \"https://github.com/uwsampl/web-data/raw/main/vta/models/\"\ncateg_fn = \"synset.txt\"\ndownload.download(join(categ_url, categ_fn), categ_fn)\nsynset = eval(open(categ_fn).read())\n\ntop_categories = np.argsort(tvm_output.asnumpy()[0])\ntop5 = np.flip(top_categories, axis=0)[:5]\n\n# Report top-1 classification result\nprint(\"Top-1 id: {}, class name: {}\".format(top5[1 - 1], synset[top5[1 - 1]]))\n\n# Report top-5 classification results\nprint(\"\\nTop5 predictions: \\n\")\nprint(\"\\t#1:\", synset[top5[1 - 1]])\nprint(\"\\t#2:\", synset[top5[2 - 1]])\nprint(\"\\t#3:\", synset[top5[3 - 1]])\nprint(\"\\t#4:\", synset[top5[4 - 1]])\nprint(\"\\t#5:\", synset[top5[5 - 1]])\nprint(\"\\t\", top5)\nImageNetClassifier = False\nfor k in top_categories[-5:]:\n    if \"cat\" in synset[k]:\n        ImageNetClassifier = True\nassert ImageNetClassifier, \"Failed ImageNet classifier validation check\"\n\nprint(\"Evaluate inference time cost...\")\nprint(m.benchmark(dev, number=1, repeat=10))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}