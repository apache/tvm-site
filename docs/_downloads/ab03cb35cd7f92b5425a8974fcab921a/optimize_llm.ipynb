{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Optimize Large Language Model\nAs large language models (LLMs) have become a popular research topic in many different fields,\ndeploying them on cloud and edge devices has become a challenging task. In this tutorial, we will\ndemonstrate how to optimize a large language model using Apache TVM. We will use a pre-trained\nTinyLlama model from Hugging Face and deploy it on various devices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review Overall Flow\nThe overall flow consists of the following steps:\n\n- **Construct or Import a Model**: Construct a neural network model or import a pre-trained\n  model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains\n  all the information needed for compilation, including high-level Relax functions for\n  computational graph, and low-level TensorIR functions for tensor program.\n- **Perform Composable Optimizations**: Perform a series of optimization transformations,\n  such as graph optimizations, tensor program optimizations, and library dispatching.\n- **Build and Universal Deployment**: Build the optimized model to a deployable module to the\n  universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Construct the model architecture\nWe will use a pre-trained TinyLlama model from Hugging Face. However, usually we only load the\npre-trained weight from Hugging Face but not the model architecture. We need to construct the\nmodel architecture by ourselves. Apache TVM prepares a PyTorch-liked API to construct the model\narchitecture. We can use the API to construct the model architecture.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import dataclasses\nimport enum\nimport os\nfrom pathlib import Path\nfrom pprint import pprint\nfrom typing import List, Optional\n\nimport tvm\nfrom tvm import dlight, relax, te, tir\nfrom tvm.relax import register_pipeline\nfrom tvm.relax.frontend import nn\nfrom tvm.relax.frontend.nn import Tensor, op\nfrom tvm.relax.frontend.nn.llm.kv_cache import PagedKVCache, TIRPagedKVCache\nfrom tvm.runtime import ShapeTuple"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, we need to define the model configuration. The configuration includes the key parameters\nof the model, such as hidden size, intermediate size, etc. Here for convenience, we define a\nconstant config specially for the TinyLlama model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@dataclasses.dataclass\nclass LlamaConfig:\n    hidden_size: int = 2048\n    intermediate_size: int = 5632\n    num_attention_heads: int = 32\n    num_hidden_layers: int = 22\n    rms_norm_eps: float = 1e-05\n    vocab_size: int = 32000\n    rope_theta: int = 10000\n    context_window_size: int = 2048\n    prefill_chunk_size: int = 2048\n    num_key_value_heads: int = 4\n    head_dim: int = 64  # hidden_size // num_attention_heads\n\n\ndev = tvm.device(\"cuda\", 0)\ntarget = tvm.target.Target.from_device(dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define the RoPE mode of the Paged KV cache. The RoPE mode is used to apply the\nRelative Positional Encoding (RoPE) to the query and key tensors. The RoPE mode can be set to\n`NONE`, `NORMAL`, or `INLINE`. If the RoPE mode is `NONE`, the KV cache will not apply RoPE to\nthe query and key tensors. If the RoPE mode is `NORMAL`, RoPE will be applied to the key tensor\nbefore adding the key tensor to the cache. If the RoPE mode is `INLINE`, RoPE will be applied to\nthe query and key tensors in the attention kernel on-the-fly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RopeMode(enum.IntEnum):\n    \"\"\"The RoPE mode of the Paged KV cache.\n    If it is none, the KV cache will not apply RoPE to q and k.\n    If it is normal, RoPE will be applied to k before adding k to cache.\n    Otherwise, RoPE will be applied to q/k in attention kernel on-the-fly.\n    \"\"\"\n\n    NONE = 0\n    NORMAL = 1\n    INLINE = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Secondly, we define the model architecture. The model architecture consists of three parts:\n\n- Embedding layer: The embedding layer converts the input token IDs to the hidden states.\n- Decoder layers: The decoder layers are the core of the model. Each decoder layer consists of\n  a self-attention layer and a feed-forward network (FFN) layer.\n- Output layer: The output layer converts the hidden states to the logits.\n\nFirst we define the FFN layer. Note that the following FFN layer is optimized implementation\nwhere we fuse the gate and up projection into one kernel.\nThe naive implementation of FFN layer is: ``FFN(x) = down_proj(silu(gate(x)) * up(x))``\nWe could combine the ``gate`` and ``up`` projection into one kernel for better performance.\nThe optimized implementation is:\n\n```python\nconcat_x = gate_up(x)\ngate_x, up_x = split(concat_x, 2, axis=-1)\nFFN(x) = down_proj(silu(gate_x) * up_x)\n```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LlamaFFN(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.gate_up_proj = nn.Linear(\n            in_features=config.hidden_size,\n            out_features=2 * config.intermediate_size,\n            bias=False,\n        )\n        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n\n    def forward(self, x: Tensor):\n        concat_x1_x2 = self.gate_up_proj(x)\n        x1, x2 = op.split(concat_x1_x2, 2, axis=-1)\n        return self.down_proj(op.silu(x1) * x2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we define the self-attention layer. The self-attention layer consists of three parts:\n\n- QKV projection: The QKV projection converts the input hidden states to the query, key, and\n  value tensors.\n- Attention: The attention layer computes the attention scores and applies the softmax\n  operation.\n- Output projection: The output projection converts the attention output to the hidden states.\n\nWe perform optimizations on the different parts of the self-attention layer:\n\n- QKV projection: We leverage the horizontal fusion on QKV projection and fuse them into one\n  kernel.\n- Attention: We leverage the horizontal fusion on attention and fuse the QKV projection and\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LlamaAttention(nn.Module):  # pylint: disable=too-many-instance-attributes\n    def __init__(self, config: LlamaConfig):\n        self.head_dim = config.head_dim\n        self.num_q_heads = config.num_attention_heads\n        self.num_kv_heads = config.num_key_value_heads\n        # horizontal fusion on QKV projection\n        self.qkv_proj = nn.Linear(\n            in_features=config.hidden_size,\n            out_features=(self.num_q_heads + 2 * self.num_kv_heads) * self.head_dim,\n            bias=False,\n        )\n        self.o_proj = nn.Linear(self.num_q_heads * self.head_dim, config.hidden_size, bias=False)\n\n    def forward(self, hidden_states: Tensor, paged_kv_cache: PagedKVCache, layer_id: int):\n        d, h_q, h_kv = self.head_dim, self.num_q_heads, self.num_kv_heads\n        b, s, _ = hidden_states.shape\n        # QKV Projection\n        qkv = self.qkv_proj(hidden_states)\n        qkv = op.reshape(qkv, (b, s, h_q + h_kv + h_kv, d))\n        # Attention\n        output = op.reshape(\n            paged_kv_cache.attention_with_fused_qkv(layer_id, qkv, self.num_q_heads),\n            (b, s, h_q * d),\n        )\n        # Output Projection\n        return self.o_proj(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define the model architecture with FFN and self-attention layers.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        rms_norm_eps = config.rms_norm_eps\n        self.self_attn = LlamaAttention(config)\n        self.mlp = LlamaFFN(config)\n        self.input_layernorm = nn.RMSNorm(config.hidden_size, -1, rms_norm_eps, bias=False)\n        self.post_attention_layernorm = nn.RMSNorm(config.hidden_size, -1, rms_norm_eps, bias=False)\n\n    def forward(self, hidden_states: Tensor, paged_kv_cache: PagedKVCache, layer_id: int):\n        hidden_states += self.self_attn(\n            self.input_layernorm(hidden_states), paged_kv_cache, layer_id\n        )\n        hidden_states += self.mlp(self.post_attention_layernorm(hidden_states))\n        return hidden_states\n\n\nclass LlamaModel(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        assert config.hidden_size % config.num_attention_heads == 0\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)\n        self.layers = nn.ModuleList(\n            [LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)]\n        )\n        self.norm = nn.RMSNorm(config.hidden_size, -1, config.rms_norm_eps, bias=False)\n\n    def forward(self, input_embed: Tensor, paged_kv_cache: PagedKVCache):\n        hidden_states = input_embed\n        for layer_id, layer in enumerate(self.layers):\n            hidden_states = layer(hidden_states, paged_kv_cache, layer_id)\n        hidden_states = self.norm(hidden_states)\n        return hidden_states\n\n\nclass LlamaForCasualLM(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        self.model = LlamaModel(config)\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n        self.num_hidden_layers = config.num_hidden_layers\n        self.num_attention_heads = config.num_attention_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.head_dim = config.head_dim\n        self.hidden_size = config.hidden_size\n        self.vocab_size = config.vocab_size\n        self.rope_theta = config.rope_theta\n        self.dtype = \"float32\"\n\n    def to(self, dtype: Optional[str] = None):\n        super().to(dtype=dtype)\n        if dtype is not None:\n            self.dtype = dtype\n\n    def embed(self, input_ids: Tensor):\n        return self.model.embed_tokens(input_ids)\n\n    def get_logits(self, hidden_states: Tensor):\n        logits = self.lm_head(hidden_states)\n        if logits.dtype != \"float32\":\n            logits = logits.astype(\"float32\")\n        return logits\n\n    def prefill(self, input_embed: Tensor, paged_kv_cache: PagedKVCache):\n        def _index(x: te.Tensor):  # x[:-1,:]\n            b, s, d = x.shape\n            return te.compute((b, 1, d), lambda i, _, k: x[i, s - 1, k], name=\"index\")\n\n        hidden_states = self.model(input_embed, paged_kv_cache)\n        hidden_states = op.tensor_expr_op(_index, name_hint=\"index\", args=[hidden_states])\n        logits = self.get_logits(hidden_states)\n        return logits, paged_kv_cache\n\n    def decode(self, input_embed: Tensor, paged_kv_cache: PagedKVCache):\n        hidden_states = self.model(input_embed, paged_kv_cache)\n        logits = self.get_logits(hidden_states)\n        return logits, paged_kv_cache\n\n    def create_tir_paged_kv_cache(\n        self,\n        max_batch_size: tir.Var,\n        max_total_seq_len: tir.Var,\n        prefill_chunk_size: tir.Var,\n        page_size: tir.Var,\n    ) -> PagedKVCache:\n        return TIRPagedKVCache(\n            max_batch_size=max_batch_size,\n            max_total_seq_len=max_total_seq_len,\n            prefill_chunk_size=prefill_chunk_size,\n            page_size=page_size,\n            support_sliding_window=0,\n            layer_partition=relax.ShapeExpr([0, self.num_hidden_layers]),\n            num_hidden_layers=self.num_hidden_layers,\n            num_attention_heads=self.num_attention_heads,\n            num_key_value_heads=self.num_key_value_heads,\n            head_dim=self.head_dim,\n            rope_mode=RopeMode.NORMAL,\n            rope_scale=1,\n            rope_theta=self.rope_theta,\n            rope_scaling={},\n            rope_ext_factors=relax.PrimValue(0),\n            rotary_dim=self.head_dim,\n            dtype=self.dtype,\n            target=target,\n        )\n\n    def get_default_spec(self):\n        mod_spec = {\n            \"embed\": {\n                \"input_ids\": nn.spec.Tensor([\"seq_len\"], \"int32\"),\n                \"$\": {\n                    \"param_mode\": \"packed\",\n                    \"effect_mode\": \"none\",\n                },\n            },\n            \"prefill\": {\n                \"input_embed\": nn.spec.Tensor([1, \"seq_len\", self.hidden_size], self.dtype),\n                \"paged_kv_cache\": nn.spec.Object(object_type=PagedKVCache),\n                \"$\": {\n                    \"param_mode\": \"packed\",\n                    \"effect_mode\": \"none\",\n                },\n            },\n            \"decode\": {\n                \"input_embed\": nn.spec.Tensor([1, 1, self.hidden_size], self.dtype),\n                \"paged_kv_cache\": nn.spec.Object(object_type=PagedKVCache),\n                \"$\": {\n                    \"param_mode\": \"packed\",\n                    \"effect_mode\": \"none\",\n                },\n            },\n            \"create_tir_paged_kv_cache\": {\n                \"max_batch_size\": int,\n                \"max_total_seq_len\": int,\n                \"prefill_chunk_size\": int,\n                \"page_size\": int,\n                \"$\": {\n                    \"param_mode\": \"none\",\n                    \"effect_mode\": \"none\",\n                },\n            },\n        }\n        return nn.spec.ModuleSpec.from_raw(mod_spec, self)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export the model to Relax IRModule\nAfter defining the model architecture, we can export the model to the Relax IRModule.\nFor demonstration, we only show the part of the model architecture. and parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_config = LlamaConfig()\nmodel = LlamaForCasualLM(model_config)\nmodel.to(\"float16\")\nmod, named_params = model.export_tvm(spec=model.get_default_spec())\nprefill_str = mod[\"prefill\"].script()\nprint(*prefill_str.split(\"\\n\")[3:20], sep=\"\\n\")  # Only show the first 10 lines for demonstration\nprint(\"        ...\")\n\nprint(\"\\nParameters:\")\npprint(named_params[:5])  # Only show the first 5 parameters for demonstration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Optimization Pipeline\nWe define a series of optimization passes to optimize the model. The optimization pipeline\nis designed specifically for the LLMs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@register_pipeline(\"opt_llm\")\ndef _pipeline(  # pylint: disable=too-many-arguments\n    ext_mods: List[nn.ExternModule] = None,\n):\n    ext_mods = ext_mods or []\n\n    @tvm.transform.module_pass(opt_level=0)\n    def _pipeline(mod: tvm.ir.IRModule, _ctx: tvm.transform.PassContext) -> tvm.ir.IRModule:\n        seq = tvm.transform.Sequential(\n            [\n                # Phase 1. Passes on high-level operator graph\n                # We can enable cublas for further optimization\n                relax.transform.FuseTransposeMatmul(),\n                # Phase 2. Lowering to TIR, inherited TVM Relax's official \"zero\" pipeline\n                relax.transform.LegalizeOps(),\n                relax.transform.AnnotateTIROpPattern(),\n                relax.transform.FoldConstant(),\n                relax.transform.FuseOps(),\n                relax.transform.FuseTIR(),\n                # Phase 3. Passes on TIR\n                relax.transform.DeadCodeElimination(),\n                # Phase 4. Low-level Optimizations\n                dlight.ApplyDefaultSchedule(\n                    dlight.gpu.Matmul(),\n                    dlight.gpu.GEMV(),\n                    dlight.gpu.Reduction(),\n                    dlight.gpu.GeneralReduction(),\n                    dlight.gpu.Fallback(),\n                ),\n                # Phase 5. Lowering to VM bytecode\n                relax.transform.RewriteDataflowReshape(),\n                relax.transform.ToNonDataflow(),\n                relax.transform.RemovePurityChecking(),\n                relax.transform.CallTIRRewrite(),\n                relax.transform.StaticPlanBlockMemory(),\n                relax.transform.RewriteCUDAGraph(),\n                relax.transform.LowerAllocTensor(),\n                relax.transform.KillAfterLastUse(),\n                relax.transform.LowerRuntimeBuiltin(),\n                relax.transform.VMShapeLower(),\n                relax.transform.AttachGlobalSymbol(),\n                relax.transform.AttachExternModules(ext_mods),\n            ]\n        )\n        mod = seq(mod)\n        return mod\n\n    return _pipeline\n\n\nwith target:\n    ex = relax.build(mod, target, pipeline=relax.get_pipeline(\"opt_llm\"))\n    vm = relax.VirtualMachine(ex, dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the model weights\nWe load the pre-trained weights from Hugging Face and prepare the model weights.\nThe pre-trained weights are stored in the Hugging Face format. We need to load the weights\nand prepare the model parameters.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Note that we won't execute the following code in this tutorial because the pre-trained weights\n  are not available in the CI environment.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "IS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\n\nHF_WEIGHT_PATH = None\n# HF_WEIGHT_PATH = Path(\"/path/to/TinyLlama-1.1B-Chat-v1.0/\")\n\nif not IS_IN_CI:\n    import numpy as np\n    import safetensors.torch\n    import torch\n\n    if HF_WEIGHT_PATH is None or not HF_WEIGHT_PATH.exists():\n        raise ValueError(\"Please set the HF_WEIGHT_PATH to the path of the pre-trained weights.\")\n\n    # Torch format weights\n    param_dict = safetensors.torch.load_file(HF_WEIGHT_PATH / \"model.safetensors\", device=\"cpu\")\n    # Numpy format weights\n    param_dict = {\n        k: v.half().numpy() if v.dtype == torch.bfloat16 else v.numpy()\n        for k, v in param_dict.items()\n    }\n\n    named_params = dict(named_params)\n    for i in range(model_config.num_hidden_layers):\n        # Add QKV in self attention\n        attn = f\"model.layers.{i}.self_attn\"\n        param_dict[f\"{attn}.qkv_proj.weight\"] = np.concatenate(\n            [\n                param_dict.pop(f\"{attn}.q_proj.weight\"),  # Pop the old parameters to save memory\n                param_dict.pop(f\"{attn}.k_proj.weight\"),\n                param_dict.pop(f\"{attn}.v_proj.weight\"),\n            ],\n            axis=0,\n        )\n        # Add gates in MLP\n        mlp = f\"model.layers.{i}.mlp\"\n        param_dict[f\"{mlp}.gate_up_proj.weight\"] = np.concatenate(\n            [\n                param_dict.pop(f\"{mlp}.gate_proj.weight\"),\n                param_dict.pop(f\"{mlp}.up_proj.weight\"),\n            ],\n            axis=0,\n        )\n\n    # Convert params into ndarray\n    params = [\n        tvm.nd.array(param_dict[k].astype(\"float16\"), device=dev) for k in named_params.keys()\n    ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the compiled model\nAfter the model and weights are ready, we can deploy the compiled model on the target device.\nThe language models inference includes two steps: prefill and decode. The prefill step is\nused to process the input tokens and store the KVCache. The decode step is used to generate\nthe token until the end token is generated.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenization\nThe first step is to tokenize the input prompt and embed the tokens into the hidden states.\nThe tokenization and embedding are the same as the original model. We use the HF tokenizer\nto tokenize the input prompt and embed the tokens into the hidden states.\nNote that different models require different tokenization and prompt format, please refer to\nthe model documentation for the correct tokenization and prompt format.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not IS_IN_CI:\n    from transformers import AutoTokenizer\n\n    tokenizer = AutoTokenizer.from_pretrained(HF_WEIGHT_PATH)\n    messages = [\n        {\"role\": \"user\", \"content\": \"What's your name?\"},\n    ]\n    prompt = tokenizer.apply_chat_template(messages)\n    input_len = len(prompt)\n\n    # Load prompt tokens into TVM ndarray on the target device\n    tokens = tvm.nd.array(np.array(prompt).astype(\"int32\"), device=dev)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the KVCache\nBefore starting the inference, we need to create the KVCache. The KVCache is used to store the\nkey and value tensors for the attention layer. Apache TVM provides a PagedKVCache to store the\nkey and value tensors. We create the PagedKVCache with the specified parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not IS_IN_CI:\n    kv_cache = vm[\"create_tir_paged_kv_cache\"](\n        ShapeTuple([1]),  # max_batch_size=1\n        ShapeTuple([2048]),  # max_total_seq_len=2048\n        ShapeTuple([2048]),  # prefill_chunk_size=2048\n        ShapeTuple([16]),  # page_size=16\n    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding\nThe next step is to embed the tokens into the hidden states. We use the `embed` function\ncompiled in the Relax IRModule to embed the tokens into the hidden states.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "nd_view_func = tvm.get_global_func(\"vm.builtin.reshape\")\n\n\ndef embed(tokens, params):\n    _embed = vm[\"embed\"](tokens, params)\n    # Reshape hidden from [seq_len, hidden_size] to [1, seq_len, hidden_size]\n    _embed = nd_view_func(_embed, ShapeTuple([1, _embed.shape[0], _embed.shape[1]]))\n    return _embed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prefill\nBefore running the forward pass, we first get some help functions for preparation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "add_sequence_func = tvm.get_global_func(\"vm.builtin.kv_state_add_sequence\")\nbegin_forward_func = tvm.get_global_func(\"vm.builtin.kv_state_begin_forward\")\nend_forward_func = tvm.get_global_func(\"vm.builtin.kv_state_end_forward\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we are creating a new sequence, we need to call `add_sequence_func` to initialize\nthe request. Additionally, we need to call `begin_forward_func` to start the forward pass,\nand `end_forward_func` to end the forward pass.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not IS_IN_CI:\n    seq_id = 0\n    add_sequence_func(kv_cache, seq_id)\n    hidden_states = embed(tokens, params)\n    begin_forward_func(kv_cache, ShapeTuple([seq_id]), ShapeTuple([input_len]))\n    logits, kv_cache = vm[\"prefill\"](hidden_states, kv_cache, params)\n    end_forward_func(kv_cache)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we have the output logits from the prefill step. The logits are used to generate the token\nvia sampling. Let's sample the token from the logits.\n\nIn this tutorial, we simplify the sampling process and pick the token with the highest\nprobability. In practice, we should sample the token based on the probability distribution.\nAlso, to make the tutorial concise, we execute the sample process on CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def sample_token(logits):\n    logits_np = logits.numpy()\n    return np.argmax(logits_np)\n\n\nif not IS_IN_CI:\n    last_token = sample_token(logits)\n    output_tokens = [last_token]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Decode\nAfter the prefill step, we can start the decode step. The decode step is used to generate the\ntoken until the end token is generated. We use the `decode` function compiled in the Relax\nIRModule to generate the token.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not IS_IN_CI:\n    print(\"The generated token:\")\n\n    while last_token != tokenizer.eos_token_id:\n        tokens = tvm.nd.array(np.array([last_token]).astype(\"int32\"), device=dev)\n        hidden_states = embed(tokens, params)\n        begin_forward_func(kv_cache, ShapeTuple([seq_id]), ShapeTuple([1]))\n        logits, kv_cache = vm[\"decode\"](hidden_states, kv_cache, params)\n\n        end_forward_func(kv_cache)\n        last_token = sample_token(logits)\n        output_tokens.append(last_token)\n\n    print(tokenizer.decode(output_tokens))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}