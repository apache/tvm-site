{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Transformation\nIn this section, we will dive into the transformation of Relax programs.\nTransformations is one of the key ingredients of the compilation flows\nfor optimizing and integrating with hardware backends.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's first create a simple Relax program as what we have done in\nthe `previous section <relax-creation>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm import IRModule, relax\nfrom tvm.relax.frontend import nn\n\n\nclass NNModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        return x\n\n\norigin_mod, params = NNModule().export_tvm(\n    {\"forward\": {\"x\": nn.spec.Tensor((\"n\", 784), \"float32\")}}\n)\norigin_mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply transformations\nPasses are the main way to apply transformations to the program.\nWe can apply passes to the program. As first step, let's apply\na built-in pass ``LegalizeOps`` to lower the high-level operators\ninto low-level operators.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = tvm.relax.transform.LegalizeOps()(origin_mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see from the output, the high-level operators (aka ``relax.op``) in the program\nare replaced by their corresponding low-level operators (aka ``relax.call_tir``).\n\nThen let's trying to apply the operator fusion, which is a wide-used optimization technique\nin ML compilers. Note that in relax, fusion optimizations are done with the collaboration of\na set of passes. We can apply them in a sequence.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = tvm.ir.transform.Sequential(\n    [\n        tvm.relax.transform.AnnotateTIROpPattern(),\n        tvm.relax.transform.FuseOps(),\n        tvm.relax.transform.FuseTIR(),\n    ]\n)(mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As result, we can see that the ``matmul``, ``add`` and ``relu`` operators are fused\ninto one kernel (aka one ``call_tir``).\n\nFor all built-in passes, please refer to :py:class:`relax.transform`.\n\n## Custom Passes\nWe can also define our own passes. Let's taking an example of rewrite the ``relu``\noperator to ``gelu`` operator.\n\nFirst, we need to write a Relax IR Mutator to do the rewriting.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.relax.expr_functor import PyExprMutator, mutator\n\n\n@mutator\nclass ReluRewriter(PyExprMutator):\n    def __init__(self, mod):\n        super().__init__(mod)\n\n    def visit_call_(self, call: relax.Call) -> relax.Expr:\n        # visit the relax.Call expr, and only handle the case when op is relax.nn.relu\n        if call.op.name == \"relax.nn.relu\":\n            return relax.op.nn.gelu(call.args[0])\n\n        return super().visit_call_(call)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we can write a pass to apply the mutator to the whole module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@tvm.transform.module_pass(opt_level=0, name=\"ReluToGelu\")\nclass ReluToGelu:  # pylint: disable=too-few-public-methods\n    def transform_module(self, mod: IRModule, _ctx: tvm.transform.PassContext) -> IRModule:\n        \"\"\"IRModule-level transformation\"\"\"\n        rewriter = ReluRewriter(mod)\n        for g_var, func in mod.functions_items():\n            if isinstance(func, relax.Function):\n                func = rewriter.visit_expr(func)\n                rewriter.builder_.update_func(g_var, func)\n        return rewriter.builder_.get()\n\n\nmod = ReluToGelu()(origin_mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The printed output shows that the ``relax.nn.relu`` operator is\nrewritten to ``relax.nn.gelu`` operator.\n\nFor the details of the mutator, please refer to :py:class:`relax.expr_functor.PyExprMutator`.\n\n## Summary\nIn this section, we have shown how to apply transformations to the Relax program.\nWe have also shown how to define and apply custom transformations.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}