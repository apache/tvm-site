{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Transformation\nIn this section, we will get to the main ingredients of the compilation flows -\ntransformations of primitive tensor functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the `previous section <tir-learning>`, we have given an example of how to write\n``mm_relu`` using TensorIR. In practice, there can be multiple ways to implement\nthe same functionality, and each implementation can result in different performance.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial primarily illustrates the application of TensorIR Transformation,\n  rather than delving into optimization techniques.</p></div>\n\nFirst, let's take a look at the implementation of ``mm_relu`` in the previous section:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm.script import ir as I\nfrom tvm.script import tir as T\n\n\n@I.ir_module\nclass MyModule:\n    @T.prim_func\n    def main(\n        A: T.Buffer((128, 128), \"float32\"),\n        B: T.Buffer((128, 128), \"float32\"),\n        C: T.Buffer((128, 128), \"float32\"),\n    ):\n        T.func_attr({\"tir.noalias\": T.bool(True)})\n        Y = T.alloc_buffer((128, 128))\n        for i, j, k in T.grid(128, 128, 128):\n            with T.block(\"Y\"):\n                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n                with T.init():\n                    Y[vi, vj] = T.float32(0)\n                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n        for i, j in T.grid(128, 128):\n            with T.block(\"C\"):\n                vi, vj = T.axis.remap(\"SS\", [i, j])\n                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before we transform the function, let's first evaluate the performance of the\noriginal implementation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\na_np = np.random.uniform(size=(128, 128)).astype(\"float32\")\nb_np = np.random.uniform(size=(128, 128)).astype(\"float32\")\nc_np = a_np @ b_np\n\na_nd = tvm.nd.array(a_np)\nb_nd = tvm.nd.array(b_np)\nc_nd = tvm.nd.array(np.zeros((128, 128), dtype=\"float32\"))\n\n\ndef evaluate(mod: tvm.IRModule):\n    lib = tvm.build(mod, target=\"llvm\")\n    # check correctness\n    lib(a_nd, b_nd, c_nd)\n    np.testing.assert_allclose(c_nd.numpy(), c_np, rtol=1e-5)\n    # evaluate performance\n    f_timer = lib.time_evaluator(\"main\", tvm.cpu())\n    print(f_timer(a_nd, b_nd, c_nd))\n\n\nevaluate(MyModule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialization Schedule\nWe initiate the process of code transformation by establishing a Schedule helper class,\nutilizing the provided **MyModule** as input.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch = tvm.tir.Schedule(MyModule)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loop Tiling\nSubsequently, we execute the requisite operations to acquire a reference to\nblock **Y** and its associated loops.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "block_Y = sch.get_block(\"Y\")\ni, j, k = sch.get_loops(block_Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now proceed to execute the transformations. The initial modification involves\nsplitting loop ``j`` into two separate loops, with the inner loop possessing a\nlength of 4. It is crucial to understand that the transformation process is procedural;\nthus, inadvertent execution of the block twice will yield an error stating the\nnon-existence of variable ``j``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "j0, j1 = sch.split(j, factors=[None, 8])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The outcome of the transformation can be examined, as it is retained within ``sch.mod``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Following the initial transformation phase, two supplementary loops, ``j_0`` and ``j_1``,\nhave been generated with respective ranges of 32 and 4. The subsequent\naction involves reordering these two loops.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.reorder(j0, k, j1)\nsch.mod.show()\nevaluate(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leverage Localities\nSubsequently, we will execute two additional transformation steps to achieve a different\nvariant. First, we employ a primitive known as **reverse_compute_at** to relocate block\n**C** to an inner loop of **Y**.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "block_C = sch.get_block(\"C\")\nsch.reverse_compute_at(block_C, j0)\nsch.mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Rewrite Reduction\nUntil now, the reduction initialization and update step have been maintained together\nwithin a single block body. This amalgamated form facilitates loop transformations,\nas the outer loops ``i``, ``j`` of initialization and updates generally need to remain\nsynchronized.\n\nFollowing the loop transformations, we can segregate the initialization of Y's elements\nfrom the reduction update via the **decompose_reduction** primitive.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.decompose_reduction(block_Y, k)\nsch.mod.show()\nevaluate(sch.mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Trace the Transformation\nTensorIR schedule is a procedural language, and the transformation is executed in a\nstep-by-step manner. We can trace the transformation by printing the schedule or the\nhistory of the schedule.\n\nWe've already see the schedule by printing ``sch.mod``. We can also print the history\nof the schedule by ``sch.trace``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.trace.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Alternatively, we can output the IRModule in conjunction with the historical trace.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sch.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}