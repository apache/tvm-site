{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# IRModule\nThis tutorial presents the core abstraction of Apache TVM Unity, the IRModule.\nThe IRModule encompasses the **entirety** of the ML models, incorporating the\ncomputational graph, tensor programs, and potential calls to external libraries.\n    :depth: 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tvm\nfrom tvm import relax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create IRModule\nIRModules can be initialized in various ways. We demonstrate a few of them\nbelow.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nfrom torch import fx, nn\nfrom tvm.relax.frontend.torch import from_fx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import from existing models\nThe most common way to initialize an IRModule is to import from an existing\nmodel. Apache TVM Unity accommodates imports from a range of frameworks,\nsuch as PyTorch and ONNX. This tutorial solely demonstrates the import process\nfrom PyTorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a dummy model\nclass TorchModel(nn.Module):\n    def __init__(self):\n        super(TorchModel, self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        return x\n\n\n# Give the input shape and data type\ninput_info = [((1, 784), \"float32\")]\n\n# Convert the model to IRModule\nwith torch.no_grad():\n    torch_fx_model = fx.symbolic_trace(TorchModel())\n    mod_from_torch = from_fx(torch_fx_model, input_info, keep_params_as_input=True)\n\nmod_from_torch, params_from_torch = relax.frontend.detach_params(mod_from_torch)\n# Print the IRModule\nmod_from_torch.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Write with Relax NN Module\nApache TVM Unity also provides a set of PyTorch-liked APIs, to help users\nwrite the IRModule directly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.relax.frontend import nn\n\n\nclass RelaxModel(nn.Module):\n    def __init__(self):\n        super(RelaxModel, self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(256, 10)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        return x\n\n\nmod_from_relax, params_from_relax = RelaxModel().export_tvm(\n    {\"forward\": {\"x\": nn.spec.Tensor((1, 784), \"float32\")}}\n)\nmod_from_relax.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create via TVMScript\nTVMScript is a Python-based DSL for IRModules. We are able to\ndirectly output the IRModule in the TVMScript syntax, or alternatively,\nparse the TVMScript to obtain an IRModule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.script import ir as I\nfrom tvm.script import relax as R\n\n\n@I.ir_module\nclass TVMScriptModule:\n    @R.function\n    def main(\n        x: R.Tensor((1, 784), dtype=\"float32\"),\n        fc1_weight: R.Tensor((256, 784), dtype=\"float32\"),\n        fc1_bias: R.Tensor((256,), dtype=\"float32\"),\n        fc2_weight: R.Tensor((10, 256), dtype=\"float32\"),\n        fc2_bias: R.Tensor((10,), dtype=\"float32\"),\n    ) -> R.Tensor((1, 10), dtype=\"float32\"):\n        R.func_attr({\"num_input\": 1})\n        with R.dataflow():\n            permute_dims = R.permute_dims(fc1_weight, axes=None)\n            matmul = R.matmul(x, permute_dims, out_dtype=\"void\")\n            add = R.add(matmul, fc1_bias)\n            relu = R.nn.relu(add)\n            permute_dims1 = R.permute_dims(fc2_weight, axes=None)\n            matmul1 = R.matmul(relu, permute_dims1, out_dtype=\"void\")\n            add1 = R.add(matmul1, fc2_bias)\n            gv = add1\n            R.output(gv)\n        return gv\n\n\nmod_from_script = TVMScriptModule\nmod_from_script.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attributes of an IRModule\nAn IRModule is a collection of functions, indexed by GlobalVars.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = mod_from_torch\nprint(mod.get_global_vars())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can access the functions in the IRModule by indexing with the GlobalVars\nor their names\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# index by global var name\nprint(mod[\"main\"])\n# index by global var, and checking they are the same function\n(gv,) = mod.get_global_vars()\nassert mod[gv] == mod[\"main\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformations on IRModules\nTransformations are the import component of Apache TVM Unity. One transformation\ntakes in an IRModule and outputs another IRModule. We can apply a sequence of\ntransformations to an IRModule to obtain a new IRModule. That is the common way to\noptimize a model.\n\nIn this getting started tutorial, we only demonstrate how to apply transformations\nto an IRModule. For details of each transformation, please refer to the\n`Transformation API Reference <api-relax-transformation>`\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We first apply **LegalizeOps** transformation to the IRModule. This transformation\nwill convert the Relax module into a mixed stage, with both Relax and TensorIR function\nwithin the same module. Meanwhile, the Relax operators will be converted into ``call_tir``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = mod_from_torch\nmod = relax.transform.LegalizeOps()(mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the transformation, there are much more functions inside the module. Let's print\nthe global vars again.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(mod.get_global_vars())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, Apache TVM Unity provides a set of default transformation pipelines for users,\nto simplify the transformation process. We can then apply the default pipeline to the module.\nThe default **zero** pipeline contains very fundamental transformations, including:\n\n- **LegalizeOps**: This transform converts the Relax operators into `call_tir` functions\n  with the corresponding TensorIR Functions. After this transform, the IRModule will\n  contain both Relax functions and TensorIR functions.\n- **AnnotateTIROpPattern**: This transform annotates the pattern of the TensorIR functions,\n  preparing them for subsequent operator fusion.\n- **FoldConstant**: This pass performs constant folding, optimizing operations\n  involving constants.\n- **FuseOps and FuseTIR**: These two passes work together to fuse operators based on the\n  patterns annotated in the previous step (AnnotateTIROpPattern). These passes transform\n  both Relax functions and TensorIR functions.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Here, we have applied **LegalizeOps** twice in the flow. The second time is useless but\n  harmless.\n\n  Every passes can be duplicated in the flow, since we ensure the passes can handle all legal\n  IRModule inputs. This design can help users to construct their own pipeline.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod = relax.get_pipeline(\"zero\")(mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the IRModule Universally\nAfter the optimization, we can compile the model into a TVM runtime module.\nNotably, Apache TVM Unity provides the ability of universal deployment, which means\nwe can deploy the same IRModule on different backends, including CPU, GPU, and other emerging\nbackends.\n\n### Deploy on CPU\nWe can deploy the IRModule on CPU by specifying the target as ``llvm``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exec = relax.build(mod, target=\"llvm\")\ndev = tvm.cpu()\nvm = relax.VirtualMachine(exec, dev)\n\nraw_data = np.random.rand(1, 784).astype(\"float32\")\ndata = tvm.nd.array(raw_data, dev)\ncpu_out = vm[\"main\"](data, *params_from_torch[\"main\"]).numpy()\nprint(cpu_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploy on GPU\nBesides, CPU backend, we can also deploy the IRModule on GPU. GPU requires\nprograms containing extra information, such as the thread bindings and shared memory\nallocations. We need a further transformation to generate the GPU programs.\n\nWe use ``DLight`` to generate the GPU programs. In this tutorial, we won't go into\nthe details of ``DLight``.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm import dlight as dl\n\nwith tvm.target.Target(\"cuda\"):\n    gpu_mod = dl.ApplyDefaultSchedule(\n        dl.gpu.Matmul(),\n        dl.gpu.Fallback(),\n    )(mod)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can compile the IRModule on GPU, the similar way as we did on CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "exec = relax.build(gpu_mod, target=\"cuda\")\ndev = tvm.device(\"cuda\", 0)\nvm = relax.VirtualMachine(exec, dev)\n# Need to allocate data and params on GPU device\ndata = tvm.nd.array(raw_data, dev)\ngpu_params = [tvm.nd.array(p, dev) for p in params_from_torch[\"main\"]]\ngpu_out = vm[\"main\"](data, *gpu_params).numpy()\nprint(gpu_out)\n\n# Check the correctness of the results\nassert np.allclose(cpu_out, gpu_out, atol=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Deploy on Other Backends\nApache TVM Unity also supports other backends, such as different kinds of GPUs\n(Metal, ROCm, Vulkan and OpenCL), different kinds of CPUs (x86, ARM), and other\nemerging backends (e.g., WebAssembly). The deployment process is similar to the\nGPU backend.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}