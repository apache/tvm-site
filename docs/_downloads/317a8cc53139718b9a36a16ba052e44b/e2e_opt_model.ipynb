{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# End-to-End Optimize Model\nThis tutorial demonstrates how to optimize a machine learning model using Apache TVM. We will\nuse a pre-trained ResNet-18 model from PyTorch and end-to-end optimize it using TVM's Relax API.\nPlease note that default end-to-end optimization may not suit complex models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preparation\nFirst, we prepare the model and input information. We use a pre-trained ResNet-18 model from\nPyTorch.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport torch\nfrom torch.export import export\nfrom torchvision.models.resnet import ResNet18_Weights, resnet18\n\ntorch_model = resnet18(weights=ResNet18_Weights.DEFAULT).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review Overall Flow\nThe overall flow consists of the following steps:\n\n- **Construct or Import a Model**: Construct a neural network model or import a pre-trained\n  model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains\n  all the information needed for compilation, including high-level Relax functions for\n  computational graph, and low-level TensorIR functions for tensor program.\n- **Perform Composable Optimizations**: Perform a series of optimization transformations,\n  such as graph optimizations, tensor program optimizations, and library dispatching.\n- **Build and Universal Deployment**: Build the optimized model to a deployable module to the\n  universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert the model to IRModule\nNext step, we convert the model to an IRModule using the Relax frontend for PyTorch for further\noptimization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm import relax\nfrom tvm.relax.frontend.torch import from_exported_program\n\n# Give an example argument to torch.export\nexample_args = (torch.randn(1, 3, 224, 224, dtype=torch.float32),)\n\n# Convert the model to IRModule\nwith torch.no_grad():\n    exported_program = export(torch_model, example_args)\n    mod = from_exported_program(exported_program, keep_params_as_input=True)\n\nmod, params = relax.frontend.detach_params(mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IRModule Optimization\nApache TVM Unity provides a flexible way to optimize the IRModule. Everything centered\naround IRModule optimization can be composed with existing pipelines. Note that each\ntransformation can be combined as an optimization pipeline via ``tvm.ir.transform.Sequential``.\n\nIn this tutorial, we focus on the end-to-end optimization of the model via auto-tuning. We\nleverage MetaSchedule to tune the model and store the tuning logs to the database. We also\napply the database to the model to get the best performance.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TOTAL_TRIALS = 8000  # Change to 20000 for better performance if needed\ntarget = tvm.target.Target(\"nvidia/geforce-rtx-3090-ti\")  # Change to your target device\nwork_dir = \"tuning_logs\"\n\n# Skip running in CI environment\nIS_IN_CI = os.getenv(\"CI\", \"\") == \"true\"\nif not IS_IN_CI:\n    mod = relax.get_pipeline(\"static_shape_tuning\", target=target, total_trials=TOTAL_TRIALS)(mod)\n\n    # Only show the main function\n    mod[\"main\"].show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and Deploy\nFinally, we build the optimized model and deploy it to the target device.\nWe skip this step in the CI environment.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if not IS_IN_CI:\n    ex = relax.build(mod, target=\"cuda\")\n    dev = tvm.device(\"cuda\", 0)\n    vm = relax.VirtualMachine(ex, dev)\n    # Need to allocate data and params on GPU device\n    gpu_data = tvm.nd.array(np.random.rand(1, 3, 224, 224).astype(\"float32\"), dev)\n    gpu_params = [tvm.nd.array(p, dev) for p in params[\"main\"]]\n    gpu_out = vm[\"main\"](gpu_data, *gpu_params).numpy()\n\n    print(gpu_out.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}