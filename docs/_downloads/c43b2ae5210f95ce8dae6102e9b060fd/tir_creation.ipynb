{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# TensorIR Creation\nIn this section, we will introduce the methods to write a TensorIR function\nin Apache TVM Unity. This tutorial presumes familiarity with the fundamental concepts of TensorIR.\nIf not already acquainted, please refer to `tir-learning` initially.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial concentrates on the construction of **standalone** TensorIR functions. The\n    techniques presented here are not requisite for end users to compile Relax models.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create TensorIR using TVMScript\nThe most straightforward way to create a TensorIR function via TVMScript.\nTVMScript is a TVM Python dialect that represents TensorIR in TVM.\n\n### Standard Format\nLet's take an example of ``mm_relu`` from `tir-learning`. Here is the complete\nformat of the ir_module and in TVMScript:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tvm\nfrom tvm.script import ir as I\nfrom tvm.script import tir as T\n\n\n@I.ir_module\nclass MyModule:\n    @T.prim_func\n    def mm_relu(\n        A: T.Buffer((128, 128), \"float32\"),\n        B: T.Buffer((128, 128), \"float32\"),\n        C: T.Buffer((128, 128), \"float32\"),\n    ):\n        Y = T.alloc_buffer((128, 128), dtype=\"float32\")\n        for i in range(128):\n            for j in range(128):\n                for k in range(128):\n                    with T.block(\"Y\"):\n                        vi = T.axis.spatial(128, i)\n                        vj = T.axis.spatial(128, j)\n                        vk = T.axis.reduce(128, k)\n                        T.reads(A[vi, vk], B[vk, vj])\n                        T.writes(Y[vi, vj])\n                        with T.init():\n                            Y[vi, vj] = T.float32(0)\n                        Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n        for i in range(128):\n            for j in range(128):\n                with T.block(\"C\"):\n                    vi = T.axis.spatial(128, i)\n                    vj = T.axis.spatial(128, j)\n                    T.reads(Y[vi, vj])\n                    T.writes(C[vi, vj])\n                    C[vi, vj] = T.max(Y[vi, vj], T.float32(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Concise with Syntactic Sugar\nFor ease of writing, we can employ the following syntactic sugar to\nstreamline the code:\n\n- Utilize ``T.grid`` to condense nested loops;\n- Employ ``T.axis.remap`` to abbreviate block iterator annotations;\n- Exclude ``T.reads`` and ``T.writes`` for blocks whose content can\n  be inferred from the block body;\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@I.ir_module\nclass ConciseModule:\n    @T.prim_func\n    def mm_relu(\n        A: T.Buffer((128, 128), \"float32\"),\n        B: T.Buffer((128, 128), \"float32\"),\n        C: T.Buffer((128, 128), \"float32\"),\n    ):\n        Y = T.alloc_buffer((128, 128), dtype=\"float32\")\n        for i, j, k in T.grid(128, 128, 128):\n            with T.block(\"Y\"):\n                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n                with T.init():\n                    Y[vi, vj] = T.float32(0)\n                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n        for i, j in T.grid(128, 128):\n            with T.block(\"C\"):\n                vi, vj = T.axis.remap(\"SS\", [i, j])\n                C[vi, vj] = T.max(Y[vi, vj], T.float32(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use the following code to verify that the two modules are equivalent:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tvm.ir.structural_equal(MyModule, ConciseModule))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interactive with Python Variables\nDespite TVMScript not being executed by a Python interpreter, limited\ninteraction with Python is feasible. For instance, Python variables can\nbe used to ascertain the shape and data type of a TensorIR.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Python variables\nM = N = K = 128\ndtype = \"float32\"\n\n\n# IRModule in TVMScript\n@I.ir_module\nclass ConciseModuleFromPython:\n    @T.prim_func\n    def mm_relu(\n        A: T.Buffer((M, K), dtype),\n        B: T.Buffer((K, N), dtype),\n        C: T.Buffer((M, N), dtype),\n    ):\n        Y = T.alloc_buffer((M, N), dtype)\n        for i, j, k in T.grid(M, N, K):\n            with T.block(\"Y\"):\n                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n                with T.init():\n                    Y[vi, vj] = T.cast(T.float32(0), dtype)\n                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n        for i, j in T.grid(M, N):\n            with T.block(\"C\"):\n                vi, vj = T.axis.remap(\"SS\", [i, j])\n                C[vi, vj] = T.max(Y[vi, vj], T.cast(T.float32(0), dtype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check the equivalence:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(tvm.ir.structural_equal(ConciseModule, ConciseModuleFromPython))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorIR Function with Dynamic Shapes\nDespite TVMScript not being executed by a Python interpreter, limited\ninteraction with Python is feasible. For instance, Python variables can\nbe used to ascertain the shape and data type of a TensorIR.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@I.ir_module\nclass DynamicShapeModule:\n    @T.prim_func\n    def mm_relu(a: T.handle, b: T.handle, c: T.handle):\n        # Dynamic shape definition\n        M, N, K = T.int32(), T.int32(), T.int32()\n\n        # Bind the input buffers with the dynamic shapes\n        A = T.match_buffer(a, [M, K], dtype)\n        B = T.match_buffer(b, [K, N], dtype)\n        C = T.match_buffer(c, [M, N], dtype)\n        Y = T.alloc_buffer((M, N), dtype)\n        for i, j, k in T.grid(M, N, K):\n            with T.block(\"Y\"):\n                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n                with T.init():\n                    Y[vi, vj] = T.cast(T.float32(0), dtype)\n                Y[vi, vj] = Y[vi, vj] + A[vi, vk] * B[vk, vj]\n        for i, j in T.grid(M, N):\n            with T.block(\"C\"):\n                vi, vj = T.axis.remap(\"SS\", [i, j])\n                C[vi, vj] = T.max(Y[vi, vj], T.cast(T.float32(0), dtype))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now let's check the runtime dynamic shape inference:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def evaluate_dynamic_shape(lib: tvm.runtime.Module, m: int, n: int, k: int):\n    A = tvm.nd.array(np.random.uniform(size=(m, k)).astype(\"float32\"))\n    B = tvm.nd.array(np.random.uniform(size=(k, n)).astype(\"float32\"))\n    C = tvm.nd.array(np.zeros((m, n), dtype=\"float32\"))\n    lib(A, B, C)\n    return C.numpy()\n\n\n# Compile lib only once\ndyn_shape_lib = tvm.compile(DynamicShapeModule, target=\"llvm\")\n# Able to handle different shapes\nprint(evaluate_dynamic_shape(dyn_shape_lib, m=4, n=4, k=4))\nprint(evaluate_dynamic_shape(dyn_shape_lib, m=64, n=64, k=128))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create TensorIR using Tensor Expression\nOften, the specifics of TensorIR are disregarded in favor of expressing the computation more\nsuccinctly, leading to the pragmatic generation of TensorIR. This is where Tensor Expression\n(TE) becomes relevant.\n\nTensor Expression (TE) serves as a domain-specific language delineating a sequence of\ncomputations through an expression-like API.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Tensor Expression comprises two components within the TVM stack: the expression and the\n  schedule. The expression is the domain-specific language embodying the computation pattern,\n  precisely what we're addressing in this section. Conversely, the TE schedule is the legacy\n  scheduling method, has been superseded by the TensorIR schedule in the TVM Unity stack.</p></div>\n\n### Create Static-Shape Functions\nWe use the same example of ``mm_relu`` from the last subsection to demonstrate the\nTE creation method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm import te\n\nA = te.placeholder((128, 128), \"float32\", name=\"A\")\nB = te.placeholder((128, 128), \"float32\", name=\"B\")\nk = te.reduce_axis((0, 128), \"k\")\nY = te.compute((128, 128), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"Y\")\nC = te.compute((128, 128), lambda i, j: te.max(Y[i, j], 0), name=\"C\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here ``te.compute`` takes the signature ``te.compute(output_shape, fcompute)``.\nAnd the fcompute function describes how we want to compute the value of each\nelement ``Y[i, j]`` for a given index:\n\nThe aforementioned lambda expression encapsulates the computation:\n$Y_{i, j} = \\sum_k A_{i, k} \\times B_{k, j}$. Upon defining the computation,\nwe can formulate a TensorIR function by incorporating the pertinent parameters of interest.\nIn this specific instance, we aim to construct a function with two input parameters **A, B**\nand one output parameter **C**.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "te_func = te.create_prim_func([A, B, C]).with_attr({\"global_symbol\": \"mm_relu\"})\nTEModule = tvm.IRModule({\"mm_relu\": te_func})\nTEModule.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Dynamic-Shape Functions\nWe can also create a dynamic-shape function using Tensor Expression. The only difference\nis that we need to specify the shape of the input tensors as symbolic variables.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Declare symbolic variables\nM, N, K = te.var(\"m\"), te.var(\"n\"), te.var(\"k\")\nA = te.placeholder((M, N), \"float32\", name=\"A\")\nB = te.placeholder((K, N), \"float32\", name=\"B\")\nk = te.reduce_axis((0, K), \"k\")\nY = te.compute((M, N), lambda i, j: te.sum(A[i, k] * B[k, j], axis=k), name=\"Y\")\nC = te.compute((M, N), lambda i, j: te.max(Y[i, j], 0), name=\"C\")\n\ndyn_te_func = te.create_prim_func([A, B, C]).with_attr({\"global_symbol\": \"mm_relu\"})\nDynamicTEModule = tvm.IRModule({\"mm_relu\": dyn_te_func})\nDynamicTEModule.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}