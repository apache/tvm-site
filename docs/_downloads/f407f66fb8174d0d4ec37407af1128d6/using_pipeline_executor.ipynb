{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Using Pipeline Executor in Relay\n**Author**: [Hua Jiang](https://github.com/huajsj)\n\nThis is a short tutorial on how to use \"Pipeline Executor\" with Relay.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm import te\nimport numpy as np\nfrom tvm.contrib import graph_executor as runtime\nfrom tvm.relay.op.contrib.cutlass import partition_for_cutlass\nfrom tvm import relay\nfrom tvm.relay import testing\nimport tvm.testing\nfrom tvm.contrib.cutlass import finalize_modules\n\nimg_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a simple network, this network can be a pre-trained model too.\nLet's create a very simple network for demonstration.\nIt consists of convolution, batch normalization, dense, and ReLU activation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def get_network():\n    out_channels = 16\n    batch_size = 1\n    data = relay.var(\"data\", relay.TensorType((batch_size, 3, img_size, img_size), \"float16\"))\n    dense_weight = relay.var(\n        \"dweight\", relay.TensorType((batch_size, 16 * img_size * img_size), \"float16\")\n    )\n    weight = relay.var(\"weight\")\n    bn_gamma = relay.var(\"bn_gamma\")\n    bn_beta = relay.var(\"bn_beta\")\n    bn_mmean = relay.var(\"bn_mean\")\n    bn_mvar = relay.var(\"bn_var\")\n    simple_net = relay.nn.conv2d(\n        data=data, weight=weight, kernel_size=(3, 3), channels=out_channels, padding=(1, 1)\n    )\n    simple_net = relay.nn.batch_norm(simple_net, bn_gamma, bn_beta, bn_mmean, bn_mvar)[0]\n    simple_net = relay.nn.relu(simple_net)\n    simple_net = relay.nn.batch_flatten(simple_net)\n    simple_net = relay.nn.dense(simple_net, dense_weight)\n    simple_net = relay.Function(relay.analysis.free_vars(simple_net), simple_net)\n    data_shape = (batch_size, 3, img_size, img_size)\n    net, params = testing.create_workload(simple_net)\n    return net, params, data_shape\n\n\nnet, params, data_shape = get_network()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting the network into two subgraphs.\nThis function called 'graph_split' from a unit test is just an example. User can create a customized logic\nto split the graph.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import inspect\nimport os\n\ntutorial_dir = os.path.dirname(inspect.getfile(lambda: None))\nos.sys.path.append(os.path.join(tutorial_dir, \"../../../tests/python/relay\"))\nfrom test_pipeline_executor import graph_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Splitting the network into two subgraphs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "split_config = [{\"op_name\": \"nn.relu\", \"op_index\": 0}]\nsubgraphs = graph_split(net[\"main\"], split_config, params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The generated subgraphs should look something like below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\n#subgraphs[0])\n\n def @main(%data: Tensor[(1, 3, img_size, img_size), float16]) {\n  %0 = nn.conv2d(%data, meta[relay.Constant][0] /* ty=Tensor[(16, 3, 3, 3), float16] */, padding=[1, 1, 1, 1], channels=16, kernel_size=[3, 3]) /* ty=Tensor[(1, 16, img_size, img_size), float16] */;\n  %1 = nn.batch_norm(%0, meta[relay.Constant][1] /* ty=Tensor[(16), float16] */, meta[relay.Constant][2] /* ty=Tensor[(16), float16]*/, meta[relay.Constant][3] /* ty=Tensor[(16), float16] */, meta[relay.Constant][4] /* ty=Tensor[(16), float16] */) /* ty=(Tensor[(1,16, img_size, img_size), float16], Tensor[(16), float16], Tensor[(16), float16]) */;\n  %2 = %1.0;\n  nn.relu(%2) /* ty=Tensor[(1, 16, img_size, img_size), float16] */\n }\n\n#subgraphs[1]\n\n def @main(%data_n_0: Tensor[(1, 16, 8, 8), float16] /* ty=Tensor[(1, 16, 8, 8), float16] */) {\n  %0 = nn.batch_flatten(%data_n_0) /* ty=Tensor[(1, 1024), float16] */;\n  nn.dense(%0, meta[relay.Constant][0] /* ty=Tensor[(1, 1024), float16] */, units=None) /* ty=Tensor[(1, 1), float16] */\n }\n\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the subgraph with cutlass target.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cutlass = tvm.target.Target(\n    {\n        \"kind\": \"cutlass\",\n        \"sm\": int(tvm.target.Target(\"cuda\").arch.split(\"_\")[1]),\n        \"use_3xtf32\": True,\n        \"split_k_slices\": [1],\n        \"profile_all_alignments\": False,\n        \"find_first_valid\": True,\n        \"use_multiprocessing\": True,\n        \"use_fast_math\": False,\n        \"tmp_dir\": \"./tmp\",\n    },\n    host=tvm.target.Target(\"llvm\"),\n)\n\n\ndef cutlass_build(mod, target, params=None, target_host=None, mod_name=\"default\"):\n    target = [target, cutlass]\n    lib = relay.build_module.build(\n        mod, target=target, params=params, target_host=target_host, mod_name=mod_name\n    )\n    return lib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the two subgraphs in pipeline with pipeline executor.\nSet 'USE_PIPELINE_EXECUTOR' as ON, and set USE_CUTLASS' as ON  in cmake.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm.contrib import graph_executor, pipeline_executor, pipeline_executor_build"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create subgraph pipeline configuration.\nAssociate a subgraph module with a target.\nUse CUTLASS BYOC to build the second subgraph module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mod0, mod1 = subgraphs[0], subgraphs[1]\n# Use cutlass as the codegen.\nmod1 = partition_for_cutlass(mod1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Get the pipeline executor configuration object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_config = pipeline_executor_build.PipelineConfig()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the compile target of the subgraph module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_config[mod0].target = \"llvm\"\npipe_config[mod0].dev = tvm.cpu(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the compile target of the second subgraph module as cuda.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipe_config[mod1].target = \"cuda\"\npipe_config[mod1].dev = tvm.device(\"cuda\", 0)\npipe_config[mod1].build_func = cutlass_build\npipe_config[mod1].export_cc = \"nvcc\"\n# Create the pipeline by connecting the subgraph modules.\n# The global input will be forwarded to the input interface of the first module named mod0\npipe_config[\"input\"][\"data\"].connect(pipe_config[mod0][\"input\"][\"data\"])\n# The first output of mod0 will be forwarded to the input interface of mod1\npipe_config[mod0][\"output\"][0].connect(pipe_config[mod1][\"input\"][\"data_n_0\"])\n# The first output of mod1 will be the first global output.\npipe_config[mod1][\"output\"][0].connect(pipe_config[\"output\"][0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pipeline configuration as below.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "\"\"\"\nprint(pipe_config)\n Inputs\n  |data: mod0:data\n\n output\n  |output(0) : mod1.output(0)\n\n connections\n  |mod0.output(0)-> mod1.data_n_0\n\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the pipeline executor.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with tvm.transform.PassContext(opt_level=3):\n    pipeline_mod_factory = pipeline_executor_build.build(pipe_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Export the parameter configuration to a file.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "directory_path = tvm.contrib.utils.tempdir().temp_dir\nos.makedirs(directory_path, exist_ok=True)\nconfig_file_name = pipeline_mod_factory.export_library(directory_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the load function to create and initialize PipelineModule.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_module = pipeline_executor.PipelineModule.load_library(config_file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the pipeline executor.\nAllocate input data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "data = np.random.uniform(-1, 1, size=data_shape).astype(\"float16\")\npipeline_module.set_input(\"data\", tvm.nd.array(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the two subgraph in the pipeline mode to get the output asynchronously\nor synchronously. In the following example, it is synchronous.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pipeline_module.run()\noutputs = pipeline_module.get_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use graph_executor for verification.\nRun these two subgraphs in sequence with graph_executor to get the output.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target = \"llvm\"\ndev0 = tvm.device(target, 0)\nlib0 = relay.build_module.build(mod0, target, params=params)\nmodule0 = runtime.GraphModule(lib0[\"default\"](dev0))\ncuda = tvm.target.Target(\"cuda\", host=tvm.target.Target(\"llvm\"))\nlib1 = relay.build_module.build(mod1, [cuda, cutlass], params=params)\nlib1 = finalize_modules(lib1, \"compile.so\", \"./tmp\")\n\ndev1 = tvm.device(\"cuda\", 0)\n\nmodule1 = runtime.GraphModule(lib1[\"default\"](dev1))\n\nmodule0.set_input(\"data\", data)\nmodule0.run()\nout_shape = (1, 16, img_size, img_size)\nout = module0.get_output(0, tvm.nd.empty(out_shape, \"float16\"))\nmodule1.set_input(\"data_n_0\", out)\nmodule1.run()\nout_shape = (1, 1)\nout = module1.get_output(0, tvm.nd.empty(out_shape, \"float16\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Verify the result.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tvm.testing.assert_allclose(outputs[0].numpy(), out.numpy())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}