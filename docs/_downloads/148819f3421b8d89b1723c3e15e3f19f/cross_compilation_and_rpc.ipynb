{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Cross Compilation and RPC\n**Author**: [Ziheng Jiang](https://github.com/ZihengJiang/), [Lianmin Zheng](https://github.com/merrymercy/)\n\nThis tutorial introduces cross compilation and remote device\nexecution with RPC in TVM.\n\nWith cross compilation and RPC, you can **compile a program on your\nlocal machine then run it on the remote device**. It is useful when\nthe remote device resource are limited, like Raspberry Pi and mobile\nplatforms. In this tutorial, we will use the Raspberry Pi for a CPU example\nand the Firefly-RK3399 for an OpenCL example.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build TVM Runtime on Device\n\nThe first step is to build the TVM runtime on the remote device.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>All instructions in both this section and the next section should be\n  executed on the target device, e.g. Raspberry Pi.  We assume the target\n  is running Linux.</p></div>\n\nSince we do compilation on the local machine, the remote device is only used\nfor running the generated code. We only need to build the TVM runtime on\nthe remote device.\n\n```bash\ngit clone --recursive https://github.com/apache/tvm tvm\ncd tvm\nmake runtime -j2\n```\nAfter building the runtime successfully, we need to set environment variables\nin :code:`~/.bashrc` file. We can edit :code:`~/.bashrc`\nusing :code:`vi ~/.bashrc` and add the line below (Assuming your TVM\ndirectory is in :code:`~/tvm`):\n\n```bash\nexport PYTHONPATH=$PYTHONPATH:~/tvm/python\n```\nTo update the environment variables, execute :code:`source ~/.bashrc`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set Up RPC Server on Device\nTo start an RPC server, run the following command on your remote device\n(Which is Raspberry Pi in this example).\n\n```bash\npython -m tvm.exec.rpc_server --host 0.0.0.0 --port=9090\n```\nIf you see the line below, it means the RPC server started\nsuccessfully on your device.\n\n```bash\nINFO:root:RPCServer: bind to 0.0.0.0:9090\n```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Declare and Cross Compile Kernel on Local Machine\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Now we go back to the local machine, which has a full TVM installed\n  (with LLVM).</p></div>\n\nHere we will declare a simple kernel on the local machine:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nimport tvm\nfrom tvm import te\nfrom tvm import rpc\nfrom tvm.contrib import utils\n\nn = tvm.runtime.convert(1024)\nA = te.placeholder((n,), name=\"A\")\nB = te.compute((n,), lambda i: A[i] + 1.0, name=\"B\")\nmod = tvm.IRModule.from_expr(te.create_prim_func([A, B]).with_attr(\"global_symbol\", \"add_one\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we cross compile the kernel.\nThe target should be 'llvm -mtriple=armv7l-linux-gnueabihf' for\nRaspberry Pi 3B, but we use 'llvm' here to make this tutorial runnable\non our webpage building server. See the detailed note in the following block.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "local_demo = True\n\nif local_demo:\n    target = \"llvm\"\nelse:\n    target = \"llvm -mtriple=armv7l-linux-gnueabihf\"\n\nfunc = tvm.compile(mod, target=target)\n# save the lib at a local temp folder\ntemp = utils.tempdir()\npath = temp.relpath(\"lib.tar\")\nfunc.export_library(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>To run this tutorial with a real remote device, change :code:`local_demo`\n  to False and replace :code:`target` in :code:`build` with the appropriate\n  target triple for your device. The target triple which might be\n  different for different devices. For example, it is\n  :code:`'llvm -mtriple=armv7l-linux-gnueabihf'` for Raspberry Pi 3B and\n  :code:`'llvm -mtriple=aarch64-linux-gnu'` for RK3399.\n\n  Usually, you can query the target by running :code:`gcc -v` on your\n  device, and looking for the line starting with :code:`Target:`\n  (Though it may still be a loose configuration.)\n\n  Besides :code:`-mtriple`, you can also set other compilation options\n  like:\n\n  * -mcpu=<cpuname>\n      Specify a specific chip in the current architecture to generate code for. By default this is inferred from the target triple and autodetected to the current architecture.\n  * -mattr=a1,+a2,-a3,...\n      Override or control specific attributes of the target, such as whether SIMD operations are enabled or not. The default set of attributes is set by the current CPU.\n      To get the list of available attributes, you can do:\n\n```bash\nllc -mtriple=<your device target triple> -mattr=help\n```\n  These options are consistent with [llc](http://llvm.org/docs/CommandGuide/llc.html).\n  It is recommended to set target triple and feature set to contain specific\n  feature available, so we can take full advantage of the features of the\n  board.\n  You can find more details about cross compilation attributes from\n  [LLVM guide of cross compilation](https://clang.llvm.org/docs/CrossCompilation.html).</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run CPU Kernel Remotely by RPC\nWe show how to run the generated CPU kernel on the remote device.\nFirst we obtain an RPC session from remote device.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if local_demo:\n    remote = rpc.LocalSession()\nelse:\n    # The following is my environment, change this to the IP address of your target device\n    host = \"10.77.1.162\"\n    port = 9090\n    remote = rpc.connect(host, port)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upload the lib to the remote device, then invoke a device local\ncompiler to relink them. Now `func` is a remote module object.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "remote.upload(path)\nfunc = remote.load_module(\"lib.tar\")\n\n# create arrays on the remote device\ndev = remote.cpu()\na = tvm.runtime.tensor(np.random.uniform(size=1024).astype(A.dtype), dev)\nb = tvm.runtime.tensor(np.zeros(1024, dtype=A.dtype), dev)\n# the function will run on the remote device\nfunc(a, b)\nnp.testing.assert_equal(b.numpy(), a.numpy() + 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "When you want to evaluate the performance of the kernel on the remote\ndevice, it is important to avoid the overhead of network.\n:code:`time_evaluator` will returns a remote function that runs the\nfunction over number times, measures the cost per run on the remote\ndevice and returns the measured cost. Network overhead is excluded.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "time_f = func.time_evaluator(func.entry_name, dev, number=10)\ncost = time_f(a, b).mean\nprint(\"%g secs/op\" % cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run OpenCL Kernel Remotely by RPC\nFor remote OpenCL devices, the workflow is almost the same as above.\nYou can define the kernel, upload files, and run via RPC.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Raspberry Pi does not support OpenCL, the following code is tested on\n   Firefly-RK3399. You may follow this [tutorial](https://gist.github.com/mli/585aed2cec0b5178b1a510f9f236afa2)\n   to setup the OS and OpenCL driver for RK3399.\n\n   Also we need to build the runtime with OpenCL enabled on rk3399 board. In the TVM\n   root directory, execute</p></div>\n\n```bash\ncp cmake/config.cmake .\nsed -i \"s/USE_OPENCL OFF/USE_OPENCL ON/\" config.cmake\nmake runtime -j4\n```\nThe following function shows how we run an OpenCL kernel remotely\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def run_opencl():\n    # NOTE: This is the setting for my rk3399 board. You need to modify\n    # them according to your environment.\n    opencl_device_host = \"10.77.1.145\"\n    opencl_device_port = 9090\n    target = tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n\n    # create schedule for the above \"add one\" compute declaration\n    mod = tvm.IRModule.from_expr(te.create_prim_func([A, B]))\n    sch = tvm.tir.Schedule(mod)\n    (x,) = sch.get_loops(block=sch.get_sblock(\"B\"))\n    xo, xi = sch.split(x, [None, 32])\n    sch.bind(xo, \"blockIdx.x\")\n    sch.bind(xi, \"threadIdx.x\")\n    func = tvm.compile(sch.mod, target=target)\n\n    remote = rpc.connect(opencl_device_host, opencl_device_port)\n\n    # export and upload\n    path = temp.relpath(\"lib_cl.tar\")\n    func.export_library(path)\n    remote.upload(path)\n    func = remote.load_module(\"lib_cl.tar\")\n\n    # run\n    dev = remote.cl()\n    a = tvm.runtime.tensor(np.random.uniform(size=1024).astype(A.dtype), dev)\n    b = tvm.runtime.tensor(np.zeros(1024, dtype=A.dtype), dev)\n    func(a, b)\n    np.testing.assert_equal(b.numpy(), a.numpy() + 1)\n    print(\"OpenCL test passed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy PyTorch Models to Remote Devices with RPC\nThe above examples demonstrate cross compilation and RPC using low-level\nTensorIR (via TE). For deploying complete neural network models from frameworks\nlike PyTorch or ONNX, TVM's Relax provides a higher-level abstraction that is\nbetter suited for end-to-end model compilation.\n\nThis section shows a modern workflow for deploying models to **any remote device**:\n\n1. Import a PyTorch model and convert it to Relax\n2. Cross-compile for the target architecture (ARM, x86, RISC-V, etc.)\n3. Deploy via RPC to a remote device\n4. Run inference remotely\n\nThis workflow is applicable to various deployment scenarios:\n\n- **ARM devices**: Raspberry Pi, NVIDIA Jetson, mobile phones\n- **x86 servers**: Remote Linux servers, cloud instances\n- **Embedded systems**: RISC-V boards, custom hardware\n- **Accelerators**: Remote machines with GPUs, TPUs, or other accelerators\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This example uses PyTorch for demonstration, but the workflow is identical\n   for ONNX models. Simply replace ``from_exported_program()`` with\n   ``from_onnx(model, keep_params_in_input=True)`` and follow the same steps.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# First, let's check if PyTorch is available\ntry:\n    import torch\n    from torch.export import export\n\n    HAS_TORCH = True\nexcept ImportError:\n    HAS_TORCH = False\n\n\ndef run_pytorch_model_via_rpc():\n    \"\"\"\n    Demonstrates the complete workflow of deploying a PyTorch model to an ARM device via RPC.\n    \"\"\"\n    if not HAS_TORCH:\n        print(\"Skipping PyTorch example (PyTorch not installed)\")\n        return\n\n    from tvm import relax\n    from tvm.relax.frontend.torch import from_exported_program\n\n    ######################################################################\n    # Step 1: Define and Export PyTorch Model\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # We use a simple MLP model for demonstration. In practice, this could be\n    # any PyTorch model (ResNet, BERT, etc.).\n\n    class TorchMLP(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.net = torch.nn.Sequential(\n                torch.nn.Flatten(),\n                torch.nn.Linear(28 * 28, 128),\n                torch.nn.ReLU(),\n                torch.nn.Linear(128, 10),\n            )\n\n        def forward(self, data: torch.Tensor) -> torch.Tensor:\n            return self.net(data)\n\n    # Export the model using PyTorch 2.x export API\n    torch_model = TorchMLP().eval()\n    example_args = (torch.randn(1, 1, 28, 28, dtype=torch.float32),)\n\n    with torch.no_grad():\n        exported_program = export(torch_model, example_args)\n\n    ######################################################################\n    # Step 2: Convert to Relax and Prepare for Compilation\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # Convert the exported PyTorch program to TVM's Relax representation\n\n    mod = from_exported_program(exported_program, keep_params_as_input=True)\n    # Separate parameters from the model for flexible deployment\n    mod, params = relax.frontend.detach_params(mod)\n\n    print(\"Converted PyTorch model to Relax:\")\n    print(f\"  - Number of parameters: {len(params['main'])}\")\n\n    ######################################################################\n    # Step 3: Cross-Compile for Target Device\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # Compile the model for the target device architecture. The target\n    # configuration depends on your deployment scenario.\n\n    if local_demo:\n        # For demonstration on local machine, use local target\n        target = tvm.target.Target(\"llvm\")\n        print(\"Using local target for demonstration\")\n    else:\n        # Choose the appropriate target for your device:\n        #\n        # ARM devices:\n        #   - Raspberry Pi 3/4 (32-bit): \"llvm -mtriple=armv7l-linux-gnueabihf\"\n        #   - Raspberry Pi 4 (64-bit) / Jetson: \"llvm -mtriple=aarch64-linux-gnu\"\n        #   - Android: \"llvm -mtriple=aarch64-linux-android\"\n        #\n        # x86 servers:\n        #   - Linux x86_64: \"llvm -mtriple=x86_64-linux-gnu\"\n        #   - With AVX-512: \"llvm -mtriple=x86_64-linux-gnu -mcpu=skylake-avx512\"\n        #\n        # RISC-V:\n        #   - RV64: \"llvm -mtriple=riscv64-unknown-linux-gnu\"\n        #\n        # GPU targets:\n        #   - CUDA: tvm.target.Target(\"cuda\", host=\"llvm -mtriple=x86_64-linux-gnu\")\n        #   - OpenCL: tvm.target.Target(\"opencl\", host=\"llvm -mtriple=aarch64-linux-gnu\")\n        #\n        # For this example, we use ARM 64-bit\n        target = tvm.target.Target(\"llvm -mtriple=aarch64-linux-gnu\")\n        print(f\"Cross-compiling for target: {target}\")\n\n    # Apply optimization pipeline\n    pipeline = relax.get_pipeline()\n    with target:\n        built_mod = pipeline(mod)\n\n    # Compile to executable\n    executable = tvm.compile(built_mod, target=target)\n\n    # Export to shared library\n    lib_path = temp.relpath(\"model_deployed.so\")\n    executable.export_library(lib_path)\n    print(f\"Exported library to: {lib_path}\")\n\n    # Save parameters separately\n    import numpy as np\n\n    params_path = temp.relpath(\"model_params.npz\")\n    param_arrays = {f\"p_{i}\": p.numpy() for i, p in enumerate(params[\"main\"])}\n    np.savez(params_path, **param_arrays)\n    print(f\"Saved parameters to: {params_path}\")\n\n    ######################################################################\n    # Step 4: Deploy to Remote Device via RPC\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # Connect to the remote device, upload the compiled library and parameters,\n    # then run inference remotely. This works for any device with TVM RPC server.\n    #\n    # Note: The following code demonstrates the RPC workflow. In local_demo mode,\n    # we skip actual execution to avoid LocalSession compatibility issues.\n\n    if local_demo:\n        # For demonstration, show the code structure without execution\n        print(\"\\nRPC workflow (works for any remote device):\")\n        print(\"=\" * 50)\n        print(\"1. Start RPC server on target device:\")\n        print(\"   python -m tvm.exec.rpc_server --host 0.0.0.0 --port=9090\")\n        print(\"\\n2. Connect from local machine:\")\n        print(\"   remote = rpc.connect('DEVICE_IP', 9090)\")\n        print(\"\\n3. Upload compiled library:\")\n        print(\"   remote.upload('model_deployed.so')\")\n        print(\"   remote.upload('model_params.npz')\")\n        print(\"\\n4. Load and run remotely:\")\n        print(\"   lib = remote.load_module('model_deployed.so')\")\n        print(\"   vm = relax.VirtualMachine(lib, remote.cpu())\")\n        print(\"   result = vm['main'](input, *params)\")\n        print(\"\\nDevice examples:\")\n        print(\"  - Raspberry Pi: 192.168.1.100\")\n        print(\"  - Remote server: ssh tunnel or direct IP\")\n        print(\"  - NVIDIA Jetson: 10.0.0.50\")\n        print(\"  - Cloud instance: public IP\")\n        print(\"\\nTo run actual RPC, set local_demo=False\")\n        return  # Skip actual RPC execution in demo mode\n\n    # Actual RPC workflow for real deployment\n    # Connect to remote device (works for ARM, x86, RISC-V, etc.)\n    # Make sure the RPC server is running on the device:\n    #   python -m tvm.exec.rpc_server --host 0.0.0.0 --port=9090\n    device_host = \"192.168.1.100\"  # Replace with your device IP\n    device_port = 9090\n    remote = rpc.connect(device_host, device_port)\n    print(f\"Connected to remote device at {device_host}:{device_port}\")\n\n    # Upload library and parameters to remote device\n    remote.upload(lib_path)\n    remote.upload(params_path)\n    print(\"Uploaded files to remote device\")\n\n    # Load the library on the remote device\n    lib = remote.load_module(\"model_deployed.so\")\n\n    # Choose device on remote machine\n    # For CPU: dev = remote.cpu()\n    # For CUDA GPU: dev = remote.cuda(0)\n    # For OpenCL: dev = remote.cl(0)\n    dev = remote.cpu()\n\n    # Create VM and load parameters\n    vm = relax.VirtualMachine(lib, dev)\n\n    # Load parameters from the uploaded file\n    # Note: In practice, you might load this from the remote filesystem\n    params_npz = np.load(params_path)\n    remote_params = [tvm.runtime.tensor(params_npz[f\"p_{i}\"], dev) for i in range(len(params_npz))]\n\n    ######################################################################\n    # Step 5: Run Inference on Remote Device\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # Execute the model on the remote ARM device and retrieve results\n\n    # Prepare input data\n    input_data = np.random.randn(1, 1, 28, 28).astype(\"float32\")\n    remote_input = tvm.runtime.tensor(input_data, dev)\n\n    # Run inference on remote device\n    output = vm[\"main\"](remote_input, *remote_params)\n\n    # Extract result (handle both tuple and single tensor outputs)\n    if isinstance(output, tvm.ir.Array) and len(output) > 0:\n        result = output[0]\n    else:\n        result = output\n\n    # Retrieve result from remote device to local\n    result_np = result.numpy()\n    print(f\"Inference completed on remote device\")\n    print(f\"  Output shape: {result_np.shape}\")\n    print(f\"  Predicted class: {np.argmax(result_np)}\")\n\n    ######################################################################\n    # Step 6: Performance Evaluation (Optional)\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    # Measure inference time on the remote device, excluding network overhead\n\n    time_f = vm.time_evaluator(\"main\", dev, number=10, repeat=3)\n    prof_res = time_f(remote_input, *remote_params)\n    print(f\"Inference time on remote device: {prof_res.mean * 1000:.2f} ms\")\n\n    ######################################################################\n    # Notes on Performance Optimization\n    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    #\n    # For optimal performance on target devices, consider:\n    #\n    # 1. **Auto-tuning with MetaSchedule**: Use automated search to find\n    #    optimal schedules for your specific hardware:\n    #\n    #    .. code-block:: python\n    #\n    #       mod = relax.get_pipeline(\n    #           \"static_shape_tuning\",\n    #           target=target,\n    #           total_trials=2000\n    #       )(mod)\n    #\n    # 2. **Quick optimization with DLight**: Apply pre-defined performant schedules:\n    #\n    #    .. code-block:: python\n    #\n    #       from tvm import dlight as dl\n    #       with target:\n    #           mod = dl.ApplyDefaultSchedule()(mod)\n    #\n    # 3. **Architecture-specific optimizations**:\n    #\n    #    - ARM NEON SIMD: ``-mattr=+neon``\n    #    - x86 AVX-512: ``-mcpu=skylake-avx512``\n    #    - RISC-V Vector: ``-mattr=+v``\n    #\n    #    .. code-block:: python\n    #\n    #       # Example: ARM with NEON\n    #       target = tvm.target.Target(\n    #           \"llvm -mtriple=aarch64-linux-gnu -mattr=+neon\"\n    #       )\n    #\n    #       # Example: x86 with AVX-512\n    #       target = tvm.target.Target(\n    #           \"llvm -mtriple=x86_64-linux-gnu -mcpu=skylake-avx512\"\n    #       )\n    #\n    # See :doc:`e2e_opt_model </how_to/tutorials/e2e_opt_model>` for detailed\n    # tuning examples.\n\n\n# Run the PyTorch RPC example if PyTorch is available\nif HAS_TORCH and local_demo:\n    try:\n        run_pytorch_model_via_rpc()\n    except Exception:\n        pass  # Silently skip if execution fails"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nThis tutorial provides a walk through of cross compilation and RPC\nfeatures in TVM.\n\nWe demonstrated two approaches:\n\n**Low-level TensorIR (TE) approach** - for understanding fundamentals:\n\n- Define computations using Tensor Expression\n- Cross-compile for ARM targets\n- Deploy and run via RPC\n\n**High-level Relax approach** - for deploying complete models:\n\n- Import models from PyTorch (or ONNX)\n- Convert to Relax representation\n- Cross-compile for ARM Linux devices\n- Deploy to remote devices via RPC\n- Run inference and evaluate performance\n\nKey takeaways:\n\n- Set up an RPC server on the remote device\n- Cross-compile on a powerful local machine for resource-constrained targets\n- Upload and execute compiled modules remotely via the RPC API\n- Measure performance excluding network overhead\n\nFor complete model deployment workflows, see also:\n\n- :doc:`export_and_load_executable </how_to/tutorials/export_and_load_executable>` - Export and load compiled models\n- :doc:`e2e_opt_model </how_to/tutorials/e2e_opt_model>` - End-to-end optimization with auto-tuning\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}