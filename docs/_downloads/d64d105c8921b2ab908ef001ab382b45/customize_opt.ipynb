{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Customize Optimization\nOne main design goal of Apache TVM is to enable easy customization of the optimization pipeline\nfor both research or development purposes and iterate the engineering optimizations. In this\ntutorial we will\n    :depth: 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Review Overall Flow\nThe overall flow consists of the following steps:\n\n- **Construct or Import a Model**: Construct a neural network model or import a pre-trained\n  model from other frameworks (e.g. PyTorch, ONNX), and create the TVM IRModule, which contains\n  all the information needed for compilation, including high-level Relax functions for\n  computational graph, and low-level TensorIR functions for tensor program.\n- **Perform Composable Optimizations**: Perform a series of optimization transformations,\n  such as graph optimizations, tensor program optimizations, and library dispatching.\n- **Build and Universal Deployment**: Build the optimized model to a deployable module to the\n  universal runtime, and execute it on different devices, such as CPU, GPU, or other accelerators.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport tempfile\nimport numpy as np\nimport tvm\nfrom tvm import IRModule, relax\nfrom tvm.relax.frontend import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Composable IRModule Optimization\nApache TVM Unity provides a flexible way to optimize the IRModule. Everything centered\naround IRModule optimization can be composed with existing pipelines. Note that each optimization\ncan focus on **part of the computation graph**, enabling partial lowering or partial optimization.\n\nIn this tutorial, we will demonstrate how to optimize a model with Apache TVM Unity.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prepare a Relax Module\nWe first prepare a Relax module. The module can be imported from other frameworks, constructed\nwith NN module frontend or TVMScript. Here we use a simple neural network model as an example.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class RelaxModel(nn.Module):\n    def __init__(self):\n        super(RelaxModel, self).__init__()\n        self.fc1 = nn.Linear(784, 256)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(256, 10, bias=False)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu1(x)\n        x = self.fc2(x)\n        return x\n\n\ninput_shape = (1, 784)\nmod, params = RelaxModel().export_tvm({\"forward\": {\"x\": nn.spec.Tensor(input_shape, \"float32\")}})\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Library Dispatch\nWe would like to quickly try out a variant of library optimization for certain platforms\n(e.g., GPU). We can write a certain dispatching pass for the specific platform and\noperator. Here we demonstrate how to dispatch the CUBLAS library for certain patterns.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial only demonstrates a single operator dispatching for CUBLAS, highlighting\n  the flexibility of the optimization pipeline. In real-world cases, we can import multiple\n  patterns and dispatch them to different kernels.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import cublas pattern\nimport tvm.relax.backend.cuda.cublas as _cublas\n\n\n# Define a new pass for CUBLAS dispatch\n@tvm.transform.module_pass(opt_level=0, name=\"CublasDispatch\")\nclass CublasDispatch:\n    def transform_module(self, mod: IRModule, _ctx: tvm.transform.PassContext) -> IRModule:\n        # Check if CUBLAS is enabled\n        if not tvm.get_global_func(\"relax.ext.cublas\", True):\n            raise Exception(\"CUBLAS is not enabled.\")\n\n        # Get interested patterns\n        patterns = [relax.backend.get_pattern(\"cublas.matmul_transposed_bias_relu\")]\n        # Note in real-world cases, we usually get all patterns\n        # patterns = relax.backend.get_patterns_with_prefix(\"cublas\")\n\n        # Fuse ops by patterns and then run codegen\n        mod = relax.transform.FuseOpsByPattern(patterns, annotate_codegen=True)(mod)\n        mod = relax.transform.RunCodegen()(mod)\n        return mod\n\n\nmod = CublasDispatch()(mod)\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After the dispatching pass, we can see that the first ``nn.Linear`` and ``nn.ReLU`` are fused\nand rewritten to a ``call_dps_packed`` function which call the CUBLAS library. Notably, the\nother part is not changed, which means we can selectively dispatch the optimization for\ncertain computation.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Auto Tuning\nContinuing from the previous example, we can further optimize the model with auto-tuning for\nthe **rest part of the computation**. Here we demonstrate how to use the meta-schedule to auto-tune\nthe model.\n\nWe can use ``MetaScheduleTuneTIR`` pass to simply tuning the model, while ``MetaScheduleApplyDatabase``\npass to apply the best configuration to the model. The tuning process will generate search space,\ntune the model and the following steps will apply the best configuration to the model. Before\nrunning the passes, we need to lowering relax operator into TensorIR functions via ``LegalizeOps``\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>To save CI time and avoid flakiness, we skip the tuning process in CI environment.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "device = tvm.cuda(0)\ntarget = tvm.target.Target.from_device(device)\nif os.getenv(\"CI\", \"\") != \"true\":\n    trials = 2000\n    with target, tempfile.TemporaryDirectory() as tmp_dir:\n        mod = tvm.ir.transform.Sequential(\n            [\n                relax.get_pipeline(\"zero\"),\n                relax.transform.MetaScheduleTuneTIR(work_dir=tmp_dir, max_trials_global=trials),\n                relax.transform.MetaScheduleApplyDatabase(work_dir=tmp_dir),\n            ]\n        )(mod)\n\n    mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DLight Rules\nDLight rules are a set of default rules for scheduling and optimization the kernel.\nDLight rules are designed for fast compilation and **fair** performance. In some cases,\ne.g. language model, DLight provides excellent performance, while for generic models,\nit achieves a balance between performance and compilation time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from tvm import dlight as dl\n\n# Apply DLight rules\nwith target:\n    mod = tvm.ir.transform.Sequential(\n        [\n            relax.get_pipeline(\"zero\"),\n            dl.ApplyDefaultSchedule(  # pylint: disable=not-callable\n                dl.gpu.Matmul(),\n                dl.gpu.GEMV(),\n                dl.gpu.Reduction(),\n                dl.gpu.GeneralReduction(),\n                dl.gpu.Fallback(),\n            ),\n        ]\n    )(mod)\n\nmod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial focuses on the demonstration of the optimization pipeline, instead of\n  pushing the performance to the limit. The current optimization may not be the best.</p></div>\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploy the Optimized Model\nWe can build and deploy the optimized model to the TVM runtime.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ex = tvm.compile(mod, target=\"cuda\")\ndev = tvm.device(\"cuda\", 0)\nvm = relax.VirtualMachine(ex, dev)\n# Need to allocate data and params on GPU device\ndata = tvm.runtime.tensor(np.random.rand(*input_shape).astype(\"float32\"), dev)\ngpu_params = [tvm.runtime.tensor(np.random.rand(*p.shape).astype(p.dtype), dev) for _, p in params]\ngpu_out = vm[\"forward\"](data, *gpu_params).numpy()\nprint(gpu_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\nThis tutorial demonstrates how to customize the optimization pipeline for ML models in Apache TVM.\nWe can easily compose the optimization passes and customize the optimization for different parts\nof the computation graph. The flexibility of the optimization pipeline enables us to quickly\niterate the optimization and improve the performance of the model.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}