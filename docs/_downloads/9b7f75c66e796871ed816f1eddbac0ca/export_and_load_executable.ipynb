{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs the latest dev build of TVM from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm --pre"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Export and Load Relax Executables\n\nThis tutorial walks through exporting a compiled Relax module to a shared\nobject, loading it back into the TVM runtime, and running the result either\ninteractively or from a standalone script. This tutorial demonstrates how\nto turn Relax (or imported PyTorch / ONNX) programs into deployable artifacts\nusing ``tvm.relax`` APIs.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>This tutorial uses PyTorch as the source format, but the export/load workflow\n   is the same for ONNX models. For ONNX, use ``from_onnx(model, keep_params_in_input=True)``\n   instead of ``from_exported_program()``, then follow the same steps for building,\n   exporting, and loading.</p></div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nTVM builds Relax programs into ``tvm.runtime.Executable`` objects. These\ncontain VM bytecode, compiled kernels, and constants. By exporting the\nexecutable with :py:meth:`export_library`, you obtain a shared library (for\nexample ``.so`` on Linux) that can be shipped to another machine, uploaded\nvia RPC, or loaded back later with the TVM runtime. This tutorial shows the\nexact steps end-to-end and explains what files are produced along the way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nfrom pathlib import Path\n\ntry:\n    import torch\n    from torch.export import export\nexcept ImportError:  # pragma: no cover\n    torch = None  # type: ignore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare a Torch MLP and Convert to Relax\nWe start with a small PyTorch MLP so the example remains lightweight. The\nmodel is exported to a :py:class:`torch.export.ExportedProgram` and then\ntranslated into a Relax ``IRModule``.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tvm\nfrom tvm import relax\nfrom tvm.relax.frontend.torch import from_exported_program\n\n# Check dependencies first\nIS_IN_CI = os.getenv(\"CI\", \"\").lower() == \"true\"\nHAS_TORCH = torch is not None\nRUN_EXAMPLE = HAS_TORCH and not IS_IN_CI\n\n\nif HAS_TORCH:\n\n    class TorchMLP(torch.nn.Module):\n        def __init__(self) -> None:\n            super().__init__()\n            self.net = torch.nn.Sequential(\n                torch.nn.Flatten(),\n                torch.nn.Linear(28 * 28, 128),\n                torch.nn.ReLU(),\n                torch.nn.Linear(128, 10),\n            )\n\n        def forward(self, data: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n            return self.net(data)\n\nelse:  # pragma: no cover\n    TorchMLP = None  # type: ignore[misc, assignment]\n\nif RUN_EXAMPLE:\n    torch_model = TorchMLP().eval()\n    example_args = (torch.randn(1, 1, 28, 28, dtype=torch.float32),)\n\n    with torch.no_grad():\n        exported_program = export(torch_model, example_args)\n\n    mod = from_exported_program(exported_program, keep_params_as_input=True)\n\n    # Separate model parameters so they can be bound later (or stored on disk).\n    mod, params = relax.frontend.detach_params(mod)\n\n    print(\"Imported Relax module:\")\n    mod.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build and Export with ``export_library``\nWe build for ``llvm`` to generate CPU code and then export the resulting\nexecutable. Passing ``workspace_dir`` keeps the intermediate packaging files,\nwhich is useful to inspect what was produced.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "TARGET = tvm.target.Target(\"llvm\")\nARTIFACT_DIR = Path(\"relax_export_artifacts\")\nARTIFACT_DIR.mkdir(exist_ok=True)\n\nif RUN_EXAMPLE:\n    # Apply the default Relax compilation pipeline before building.\n    pipeline = relax.get_pipeline()\n    with TARGET:\n        built_mod = pipeline(mod)\n\n    # Build without params - we'll pass them at runtime\n    executable = tvm.compile(built_mod, target=TARGET)\n\n    library_path = ARTIFACT_DIR / \"mlp_cpu.so\"\n    executable.export_library(str(library_path), workspace_dir=str(ARTIFACT_DIR))\n\n    print(f\"Exported runtime library to: {library_path}\")\n\n    # The workspace directory now contains the shared object and supporting files.\n    produced_files = sorted(p.name for p in ARTIFACT_DIR.iterdir())\n    print(\"Artifacts saved:\")\n    for name in produced_files:\n        print(f\"  - {name}\")\n\n    # Generated files:\n    #   - ``mlp_cpu.so``: The main deployable shared library containing VM bytecode,\n    #     compiled kernels, and constants. Note: Since parameters are passed at runtime,\n    #     you will also need to save a separate parameters file (see next section).\n    #   - Intermediate object files (``devc.o``, ``lib0.o``, etc.) are kept in the\n    #     workspace for inspection but are not required for deployment.\n    #\n    #   Note: Additional files like ``*.params``, ``*.metadata.json``, or ``*.imports``\n    #   may appear in specific configurations but are typically embedded into the\n    #   shared library or only generated when needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the Exported Library and Run It\nOnce the shared object is produced, we can reload it back into the TVM runtime\non any machine with a compatible instruction set. The Relax VM consumes the\nruntime module directly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if RUN_EXAMPLE:\n    loaded_rt_mod = tvm.runtime.load_module(str(library_path))\n    dev = tvm.cpu(0)\n    vm = relax.VirtualMachine(loaded_rt_mod, dev)\n\n    # Prepare input data\n    input_tensor = torch.randn(1, 1, 28, 28, dtype=torch.float32)\n    vm_input = tvm.runtime.tensor(input_tensor.numpy(), dev)\n\n    # Prepare parameters (allocate on target device)\n    vm_params = [tvm.runtime.tensor(p, dev) for p in params[\"main\"]]\n\n    # Run inference: pass input data followed by all parameters\n    tvm_output = vm[\"main\"](vm_input, *vm_params)\n\n    # TVM returns Array objects for tuple outputs, access via indexing.\n    # For models imported from PyTorch, outputs are typically tuples (even for single outputs).\n    # For ONNX models, outputs may be a single Tensor directly.\n    if isinstance(tvm_output, tvm.ir.Array) and len(tvm_output) > 0:\n        result_tensor = tvm_output[0]\n    else:\n        result_tensor = tvm_output\n\n    print(\"VM output shape:\", result_tensor.shape)\n    print(\"VM output type:\", type(tvm_output), \"->\", type(result_tensor))\n\n    # You can still inspect the executable after reloading.\n    print(\"Executable stats:\\n\", loaded_rt_mod[\"stats\"]())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Parameters for Deployment\nSince parameters are passed at runtime (not embedded in the ``.so``), we must\nsave them separately for deployment. This is a required step to use the model\non other machines or in standalone scripts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nif RUN_EXAMPLE:\n    # Save parameters to disk\n    params_path = ARTIFACT_DIR / \"model_params.npz\"\n    param_arrays = {f\"p_{i}\": p.numpy() for i, p in enumerate(params[\"main\"])}\n    np.savez(str(params_path), **param_arrays)\n    print(f\"Saved parameters to: {params_path}\")\n\n# Note: Alternatively, you can embed parameters directly into the ``.so`` to\n# create a single-file deployment. Use ``keep_params_as_input=False`` when\n# importing from PyTorch:\n#\n# .. code-block:: python\n#\n#    mod = from_exported_program(exported_program, keep_params_as_input=False)\n#    # Parameters are now embedded as constants in the module\n#    executable = tvm.compile(built_mod, target=TARGET)\n#    # Runtime: vm[\"main\"](input)  # No need to pass params!\n#\n# This creates a single-file deployment (only the ``.so`` is needed), but you\n# lose the flexibility to swap parameters without recompiling. For most\n# production workflows, separating code and parameters (as shown above) is\n# preferred for flexibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading and Running the Exported Model\nTo use the exported model on another machine or in a standalone script, you need\nto load both the ``.so`` library and the parameters file. Here's a complete example\nof how to reload and run the model. Save this as ``run_mlp.py``:\n\nTo make it executable from the command line:\n\n```bash\nchmod +x run_mlp.py\n./run_mlp.py  # Run it like a regular program\n```\nComplete script:\n\n```python\n#!/usr/bin/env python3\nimport numpy as np\nimport tvm\nfrom tvm import relax\n\n# Step 1: Load the compiled library\nlib = tvm.runtime.load_module(\"relax_export_artifacts/mlp_cpu.so\")\n\n# Step 2: Create Virtual Machine\ndevice = tvm.cpu(0)\nvm = relax.VirtualMachine(lib, device)\n\n# Step 3: Load parameters from the .npz file\nparams_npz = np.load(\"relax_export_artifacts/model_params.npz\")\nparams = [tvm.runtime.tensor(params_npz[f\"p_{i}\"], device)\n          for i in range(len(params_npz))]\n\n# Step 4: Prepare input data\ndata = np.random.randn(1, 1, 28, 28).astype(\"float32\")\ninput_tensor = tvm.runtime.tensor(data, device)\n\n# Step 5: Run inference (pass input followed by all parameters)\noutput = vm[\"main\"](input_tensor, *params)\n\n# Step 6: Extract result (output may be tuple or single Tensor)\n# PyTorch models typically return tuples, ONNX models may return a single Tensor\nif isinstance(tvm_output, tvm.ir.Array) and len(tvm_output) > 0:\n    result_tensor = tvm_output[0]\nelse:\n    result_tensor = tvm_output\n\nprint(\"Prediction shape:\", result.shape)\nprint(\"Predicted class:\", np.argmax(result.numpy()))\n```\n**Running on GPU:**\nTo run on GPU instead of CPU, make the following changes:\n\n1. **Compile for GPU** (earlier in the tutorial, around line 112):\n```python\nTARGET = tvm.target.Target(\"cuda\")  # Change from \"llvm\" to \"cuda\"\n```\n2. **Use GPU device in the script**:\n```python\ndevice = tvm.cuda(0)  # Use CUDA device instead of CPU\nvm = relax.VirtualMachine(lib, device)\n\n# Load parameters to GPU\nparams = [tvm.runtime.tensor(params_npz[f\"p_{i}\"], device)  # Note: device parameter\n          for i in range(len(params_npz))]\n\n# Prepare input on GPU\ninput_tensor = tvm.runtime.tensor(data, device)  # Note: device parameter\n```\n   The rest of the script remains the same. All tensors (parameters and inputs)\n   must be allocated on the same device (GPU) as the compiled model.\n\n**Deployment Checklist:**\nWhen moving to another host (via RPC or SCP), you must copy **both** files:\n  1. ``mlp_cpu.so`` (or ``mlp_cuda.so`` for GPU) - The compiled model code\n  2. ``model_params.npz`` - The model parameters (serialized as NumPy arrays)\n\nThe remote machine needs both files in the same directory. The script above\nassumes they are in ``relax_export_artifacts/`` relative to the script location.\nAdjust the paths as needed for your deployment. For GPU deployment, ensure the\ntarget machine has compatible CUDA drivers and the model was compiled for the\nsame GPU architecture.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deploying to Remote Devices\nTo deploy the exported model to a remote ARM Linux device (e.g., Raspberry Pi),\nyou can use TVM's RPC mechanism to cross-compile, upload, and run the model\nremotely. This workflow is useful when:\n\n- The target device has limited resources for compilation\n- You want to fine-tune performance by running on the actual hardware\n- You need to deploy to embedded devices\n\nSee :doc:`cross_compilation_and_rpc </how_to/tutorials/cross_compilation_and_rpc>`\nfor a comprehensive guide on:\n\n- Setting up TVM runtime on the remote device\n- Starting an RPC server on the device\n- Cross-compiling for ARM targets (e.g., ``llvm -mtriple=aarch64-linux-gnu``)\n- Uploading exported libraries via RPC\n- Running inference remotely\n\nQuick example for ARM deployment workflow:\n\n```python\nimport tvm.rpc as rpc\nfrom tvm import relax\n\n# Step 1: Cross-compile for ARM target (on local machine)\nTARGET = tvm.target.Target(\"llvm -mtriple=aarch64-linux-gnu\")\nexecutable = tvm.compile(built_mod, target=TARGET)\nexecutable.export_library(\"mlp_arm.so\")\n\n# Step 2: Connect to remote device RPC server\nremote = rpc.connect(\"192.168.1.100\", 9090)  # Device IP and RPC port\n\n# Step 3: Upload the compiled library and parameters\nremote.upload(\"mlp_arm.so\")\nremote.upload(\"model_params.npz\")\n\n# Step 4: Load and run on remote device\nlib = remote.load_module(\"mlp_arm.so\")\nvm = relax.VirtualMachine(lib, remote.cpu())\n# ... prepare input and params, then run inference\n```\nThe key difference is using an ARM target triple during compilation and\nuploading files via RPC instead of copying them directly.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FAQ\n**Can I run the ``.so`` as a standalone executable (like ``./mlp_cpu.so``)?**\n    No. The ``.so`` file is a shared library, not a standalone executable binary.\n    You cannot run it directly from the terminal. It must be loaded through a TVM\n    runtime program (as shown in the \"Loading and Running\" section above). The\n    ``.so`` bundles VM bytecode and compiled kernels, but still requires the TVM\n    runtime to execute.\n\n**Which devices can run the exported library?**\n    The target must match the ISA you compiled for (``llvm`` in this example).\n    As long as the target triple, runtime ABI, and available devices line up,\n    you can move the artifact between machines. For heterogeneous builds (CPU\n    plus GPU), ship the extra device libraries as well.\n\n**What about the ``.params`` and ``metadata.json`` files?**\n    These auxiliary files are only generated in specific configurations. In this\n    tutorial, since we pass parameters at runtime, they are not generated. When\n    they do appear, they may be kept alongside the ``.so`` for inspection, but\n    the essential content is typically embedded in the shared object itself, so\n    deploying the ``.so`` alone is usually sufficient.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}