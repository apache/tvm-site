

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Use Tensor Expression Debug Display (TEDD) for Visualization &mdash; tvm 0.7.dev1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../_static/tvm-logo-square.png"/>
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tvm_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="External Tensor Functions" href="extern_op.html" />
    <link rel="prev" title="Building a Graph Convolutional Network" href="../frontend/build_gcn.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.7.dev1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installation</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../relay_quick_start.html">Quick Start Tutorial for Compiling Deep Learning Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tensor_expr_get_started.html">Get Started with Tensor Expression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#compile-deep-learning-models">Compile Deep Learning Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#tensor-expression-and-schedules">Tensor Expression and Schedules</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Use Tensor Expression Debug Display (TEDD) for Visualization</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#define-and-schedule-convolution-with-bias-and-relu">Define and Schedule Convolution with Bias and ReLU</a></li>
<li class="toctree-l4"><a class="reference internal" href="#render-graphs-with-tedd">Render Graphs with TEDD</a></li>
<li class="toctree-l4"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="extern_op.html">External Tensor Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="tuple_inputs.html">Compute and Reduce with Tuple Inputs</a></li>
<li class="toctree-l3"><a class="reference internal" href="reduction.html">Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="scan.html">Scan and Recurrent Kernel</a></li>
<li class="toctree-l3"><a class="reference internal" href="intrin_math.html">Intrinsics and Math Functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="schedule_primitives.html">Schedule Primitives in TVM</a></li>
<li class="toctree-l3"><a class="reference internal" href="tensorize.html">Use Tensorize to Leverage Hardware Intrinsics</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#optimize-tensor-operators">Optimize Tensor Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#auto-tuning">Auto tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#developer-tutorials">Developer Tutorials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#topi-tvm-operator-inventory">TOPI: TVM Operator Inventory</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../vta/index.html">VTA: Deep Learning Accelerator Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy/index.html">Deploy and Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contribute to TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/links.html">Links to API References</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/index.html">Design and Developer Guide</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../frontend/tensorflow.html">TensorFlow Frontend</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">tvm</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../index.html">Tutorials</a> &raquo;</li>
        
      <li>Use Tensor Expression Debug Display (TEDD) for Visualization</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/tutorials/language/tedd.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-language-tedd-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="use-tensor-expression-debug-display-tedd-for-visualization">
<span id="sphx-glr-tutorials-language-tedd-py"></span><h1>Use Tensor Expression Debug Display (TEDD) for Visualization<a class="headerlink" href="#use-tensor-expression-debug-display-tedd-for-visualization" title="Permalink to this headline">¶</a></h1>
<p><strong>Author</strong>: <a class="reference external" href="https://github.com/yongfeng-nv">Yongfeng Gu</a></p>
<p>This is an introduction about using TEDD to visualize tensor expressions.</p>
<p>Tensor Expressions are scheduled with primitives.  Although individual
primitives are usually easy to understand, they become complicated quickly
when you put them together. We have introduced an operational model of
schedule primitives in Tensor Expression.</p>
<ul class="simple">
<li><p>the interactions between different schedule primitives,</p></li>
<li><p>the impact of the schedule primitives on the final code generation.</p></li>
</ul>
<p>The operational model is based on a Dataflow Graph, a Schedule Tree and an
IterVar Relationship Graph. Schedule primitives perform operations on these
graphs.</p>
<p>TEDD renders these three graphs from a given schedule.  This tutorial demonstrates
how to use TEDD and how to interpret the rendered graphs.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="k">import</span> <span class="n">te</span>
<span class="kn">import</span> <span class="nn">topi</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="k">import</span> <span class="n">tedd</span>
</pre></div>
</div>
<div class="section" id="define-and-schedule-convolution-with-bias-and-relu">
<h2>Define and Schedule Convolution with Bias and ReLU<a class="headerlink" href="#define-and-schedule-convolution-with-bias-and-relu" title="Permalink to this headline">¶</a></h2>
<p>Let’s build an example Tensor Expression for a convolution followed by Bias and ReLU.
We first connect conv2d, add, and relu TOPIs.  Then, we create a TOPI generic schedule.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="n">batch</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">in_channel</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">in_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">num_filter</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">stride</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">padding</span> <span class="o">=</span> <span class="s2">&quot;SAME&quot;</span>
<span class="n">dilation</span><span class="o">=</span><span class="mi">1</span>

<span class="n">A</span> <span class="o">=</span> <a href="../../api/python/te.html#tvm.te.placeholder" title="View documentation for tvm.te.placeholder"><span class="n">te</span><span class="o">.</span><span class="n">placeholder</span></a><span class="p">((</span><span class="n">in_size</span><span class="p">,</span> <span class="n">in_size</span><span class="p">,</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">batch</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;A&#39;</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <a href="../../api/python/te.html#tvm.te.placeholder" title="View documentation for tvm.te.placeholder"><span class="n">te</span><span class="o">.</span><span class="n">placeholder</span></a><span class="p">((</span><span class="n">kernel</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">in_channel</span><span class="p">,</span> <span class="n">num_filter</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <a href="../../api/python/te.html#tvm.te.placeholder" title="View documentation for tvm.te.placeholder"><span class="n">te</span><span class="o">.</span><span class="n">placeholder</span></a><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_filter</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;bias&#39;</span><span class="p">)</span>

<span class="k">with</span> <a href="../../api/python/target.html#tvm.target.create" title="View documentation for tvm.target.create"><span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">create</span></a><span class="p">(</span><span class="s2">&quot;llvm&quot;</span><span class="p">):</span>
    <span class="n">t_conv</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">conv2d_hwcn</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">stride</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="n">dilation</span><span class="p">)</span>
    <span class="n">t_bias</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">t_conv</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
    <span class="n">t_relu</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">t_bias</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="n">topi</span><span class="o">.</span><span class="n">generic</span><span class="o">.</span><span class="n">schedule_conv2d_hwcn</span><span class="p">([</span><span class="n">t_relu</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="section" id="render-graphs-with-tedd">
<h2>Render Graphs with TEDD<a class="headerlink" href="#render-graphs-with-tedd" title="Permalink to this headline">¶</a></h2>
<p>We render graphs to see the computation
and how it is scheduled.
If you run the tutorial in a Jupyter notebook, you can use the following commented lines
to render SVG figures showing in notebook directly.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="n">tedd</span><span class="o">.</span><span class="n">viz_dataflow_graph</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dot_file_path</span> <span class="o">=</span> <span class="s1">&#39;/tmp/dfg.dot&#39;</span><span class="p">)</span>
<span class="c1">#tedd.viz_dataflow_graph(s, show_svg = True)</span>
</pre></div>
</div>
<img alt="https://github.com/dmlc/web-data/raw/master/tvm/tutorial/tedd_dfg.png" class="align-center" src="https://github.com/dmlc/web-data/raw/master/tvm/tutorial/tedd_dfg.png" />
<p>The first one is a dataflow graph.  Every node represents a stage with name and memory
scope shown in the middle and inputs/outputs information on the sides.
Edges show nodes’ dependency.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="n">tedd</span><span class="o">.</span><span class="n">viz_schedule_tree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dot_file_path</span> <span class="o">=</span> <span class="s1">&#39;/tmp/scheduletree.dot&#39;</span><span class="p">)</span>
<span class="c1">#tedd.viz_schedule_tree(s, show_svg = True)</span>
</pre></div>
</div>
<p>We just rendered the schedule tree graph.  You may notice an warning about ranges not
available.
The message also suggests to call normalize() to infer range information.  We will
skip inspecting the first schedule tree and encourage you to compare the graphs before
and after normalize() for its impact.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="n">s</span> <span class="o">=</span> <span class="n">s</span><span class="o">.</span><span class="n">normalize</span><span class="p">()</span>
<span class="n">tedd</span><span class="o">.</span><span class="n">viz_schedule_tree</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dot_file_path</span> <span class="o">=</span> <span class="s1">&#39;/tmp/scheduletree2.dot&#39;</span><span class="p">)</span>
<span class="c1">#tedd.viz_schedule_tree(s, show_svg = True)</span>
</pre></div>
</div>
<img alt="https://github.com/dmlc/web-data/raw/master/tvm/tutorial/tedd_st.png" class="align-center" src="https://github.com/dmlc/web-data/raw/master/tvm/tutorial/tedd_st.png" />
<p>Now, let us take a close look at the second schedule tree.  Every block under ROOT
represents a
stage.  Stage name shows in the top row and compute shows in the bottom row.
The middle rows are for IterVars, the higher the outer, the lower the inner.
An IterVar row contains its index, name, type, and other optional information.
Let’s use the W.shared stage as an example.  The top row tells
its name, “W.shared”, and memory scope, “Shared”.  Its compute is
<code class="code docutils literal notranslate"><span class="pre">W(ax0,</span> <span class="pre">ax1,</span> <span class="pre">ax2,</span> <span class="pre">ax3)</span></code>.
Its outer most loop IterVar is ax0.ax1.fused.ax2.fused.ax3.fused.outer,
indexed with 0, of kDataPar, bound to threadIdx.y, and with range(min=0, ext=8).
You can also tell
IterVar type with the index box color, shown in the legend.</p>
<p>If a stage doesn’t compute_at any other stage, it has an edge directly to the
ROOT node.  Otherwise, it has an edge pointing to the IterVar it attaches to,
such as W.shared attaches to rx.outer in the middle compute stage.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By definition, IterVars are internal nodes and computes are leaf nodes in
a schedule tree.   The edges among IterVars and compute within one stage are
omitted, making every stage a block, for better readability.</p>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="n">tedd</span><span class="o">.</span><span class="n">viz_itervar_relationship_graph</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">dot_file_path</span> <span class="o">=</span> <span class="s1">&#39;/tmp/itervar.dot&#39;</span><span class="p">)</span>
<span class="c1">#tedd.viz_itervar_relationship_graph(s, show_svg = True)</span>
</pre></div>
</div>
<img alt="https://github.com/dmlc/web-data/raw/master/tvm/tutorial/tedd_itervar_rel.png" class="align-center" src="https://github.com/dmlc/web-data/raw/master/tvm/tutorial/tedd_itervar_rel.png" />
<p>The last one is an IterVar Relationship Graph.  Every subgraph represents a
stage and contains IterVar nodes and transformation nodes.  For example,
W.shared has three split nodes and three fuse nodes.  The rest are IterVar
nodes of the same format as the IterVar rows in Schedule Trees.  Root
IterVars are those not driven by any transformation node, such as ax0; leaf
IterVars don’t drive any transformation node and have non-negative indices,
such as ax0.ax1.fused.ax2.fused.ax3.fused.outer with index of 0.</p>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<p>This tutorial demonstrates the usage of TEDD.  We use an example built
with TOPI to show the schedules under the hood.  You can also use
it before and after any schedule primitive to inspect its effect.</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-language-tedd-py">
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/9a950897eeef498440fbe2f0afe2601f/tedd.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tedd.py</span></code></a></p>
</div>
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/a2f661bf234a167b5458fa28d8fafedc/tedd.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tedd.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="extern_op.html" class="btn btn-neutral float-right" title="External Tensor Functions" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../frontend/build_gcn.html" class="btn btn-neutral float-left" title="Building a Graph Convolutional Network" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Apache Software Foundation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>