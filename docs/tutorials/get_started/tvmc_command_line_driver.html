





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Getting Started with TVM command line driver - TVMC &mdash; tvm 0.8.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/tvm-logo-square.png"/>
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Quick Start Tutorial for Compiling Deep Learning Models" href="relay_quick_start.html" />
    <link rel="prev" title="Get Started Tutorials" href="../index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="../../index.html"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/vta>VTA</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/blog>Blog</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmconf.org>Conference</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/incubator-tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.8.dev0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">How to</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contribute to TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deploy/index.html">Deploy and Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/how_to.html">Developer How-To Guide</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Get Started Tutorials</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Getting Started with TVM command line driver - TVMC</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#using-tvmc">Using TVMC</a></li>
<li class="toctree-l3"><a class="reference internal" href="#obtaining-the-model">Obtaining the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#compiling-the-model">Compiling the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#input-pre-processing">Input pre-processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#running-the-compiled-module">Running the compiled module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#output-post-processing">Output post-processing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tuning-the-model">Tuning the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#final-remarks">Final Remarks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="relay_quick_start.html">Quick Start Tutorial for Compiling Deep Learning Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor_expr_get_started.html">Get Started with Tensor Expression</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#compile-deep-learning-models">Compile Deep Learning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#tensor-expression-and-schedules">Tensor Expression and Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#optimize-tensor-operators">Optimize Tensor Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#autotvm-template-based-auto-tuning">AutoTVM : Template-based Auto Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#autoscheduler-template-free-auto-scheduling">AutoScheduler : Template-free Auto Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#developer-tutorials">Developer Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#topi-tvm-operator-inventory">TOPI: TVM Operator Inventory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html#micro-tvm">Micro TVM</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../api/links.html">Links to Other API References</a></li>
</ul>
<p class="caption"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/index.html">Design and Architecture</a></li>
</ul>
<p class="caption"><span class="caption-text">MISC</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../vta/index.html">VTA: Deep Learning Accelerator Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of content
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="../index.html">Get Started Tutorials</a> <span class="br-arrow">></span></li>
        
      <li>Getting Started with TVM command line driver - TVMC</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/tutorials/get_started/tvmc_command_line_driver.rst.txt" rel="nofollow"> <img src="../../_static//img/source.svg" alt="viewsource"/></a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>Click <a class="reference internal" href="#sphx-glr-download-tutorials-get-started-tvmc-command-line-driver-py"><span class="std std-ref">here</span></a> to download the full example code</p>
</div>
<div class="sphx-glr-example-title section" id="getting-started-with-tvm-command-line-driver-tvmc">
<span id="sphx-glr-tutorials-get-started-tvmc-command-line-driver-py"></span><h1>Getting Started with TVM command line driver - TVMC<a class="headerlink" href="#getting-started-with-tvm-command-line-driver-tvmc" title="Permalink to this headline">¶</a></h1>
<p><strong>Authors</strong>:
<a class="reference external" href="https://github.com/leandron">Leandro Nunes</a>,
<a class="reference external" href="https://github.com/mbaret">Matthew Barrett</a></p>
<p>This tutorial is an introduction to working with TVMC, the TVM command
line driver. TVMC is a tool that exposes TVM features such as
auto-tuning, compiling, profiling and execution of models, via a
command line interface.</p>
<p>In this tutorial we are going to use TVMC to compile, run and tune a
ResNet-50 on a x86 CPU.</p>
<p>We are going to start by downloading ResNet 50 V2. Then, we are going
to use TVMC to compile this model into a TVM module, and use the
compiled module to generate predictions. Finally, we are going to experiment
with the auto-tuning options, that can be used to help the compiler to
improve network performance.</p>
<p>The final goal is to give an overview of TVMC’s capabilities and also
some guidance on where to look for more information.</p>
<div class="section" id="using-tvmc">
<h2>Using TVMC<a class="headerlink" href="#using-tvmc" title="Permalink to this headline">¶</a></h2>
<p>TVMC is a Python application, part of the TVM Python package.
When you install TVM using a Python package, you will get TVMC as
as a command line application called <code class="docutils literal notranslate"><span class="pre">tvmc</span></code>.</p>
<p>Alternatively, if you have TVM as a Python module on your
<code class="docutils literal notranslate"><span class="pre">$PYTHONPATH</span></code>,you can access the command line driver functionality
via the executable python module, <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">tvm.driver.tvmc</span></code>.</p>
<p>For simplicity, this tutorial will mention TVMC command line using
<code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">&lt;options&gt;</span></code>, but the same results can be obtained with
<code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">tvm.driver.tvmc</span> <span class="pre">&lt;options&gt;</span></code>.</p>
<p>You can check the help page using:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>tvmc --help
</pre></div>
</div>
<p>As you can see in the help page, the main features are
accessible via the subcommands <code class="docutils literal notranslate"><span class="pre">tune</span></code>, <code class="docutils literal notranslate"><span class="pre">compile</span></code> and <code class="docutils literal notranslate"><span class="pre">run</span></code>.
To read about specific options under a given subcommand, use
<code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">&lt;subcommand&gt;</span> <span class="pre">--help</span></code>.</p>
<p>In the following sections we will use TVMC to tune, compile and
run a model. But first, we need a model.</p>
</div>
<div class="section" id="obtaining-the-model">
<h2>Obtaining the model<a class="headerlink" href="#obtaining-the-model" title="Permalink to this headline">¶</a></h2>
<p>We are going to use ResNet-50 V2 as an example to experiment with TVMC.
The version below is in ONNX format. To download the file, you can use
the command below:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>wget https://github.com/onnx/models/raw/master/vision/classification/resnet/model/resnet50-v2-7.onnx
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Supported model formats</p>
<p>TVMC supports models created with Keras, ONNX, TensorFlow, TFLite
and Torch. Use the option``–model-format`` if you need to
explicitly provide the model format you are using. See <code class="docutils literal notranslate"><span class="pre">tvmc</span>
<span class="pre">compile</span> <span class="pre">--help</span></code> for more information.</p>
</div>
</div>
<div class="section" id="compiling-the-model">
<h2>Compiling the model<a class="headerlink" href="#compiling-the-model" title="Permalink to this headline">¶</a></h2>
<p>The next step once we’ve downloaded ResNet-50, is to compile it,
To accomplish that, we are going to use <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">compile</span></code>. The
output we get from the compilation process is a TAR package,
that can be used to run our model on the target device.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>tvmc compile <span class="se">\</span>
  --target <span class="s2">&quot;llvm&quot;</span> <span class="se">\</span>
  --output compiled_module.tar <span class="se">\</span>
  resnet50-v2-7.onnx
</pre></div>
</div>
<p>Once compilation finishes, the output <code class="docutils literal notranslate"><span class="pre">compiled_module.tar</span></code> will be created. This
can be directly loaded by your application and run via the TVM runtime APIs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Defining the correct target</p>
<p>Specifying the correct target (option <code class="docutils literal notranslate"><span class="pre">--target</span></code>) can have a huge
impact on the performance of the compiled module, as it can take
advantage of hardware features available on the target. For more
information, please refer to <a class="reference external" href="https://tvm.apache.org/docs/tutorials/autotvm/tune_relay_x86.html#define-network">Auto-tuning a convolutional network
for x86 CPU</a>.</p>
</div>
<p>In the next step, we are going to use the compiled module, providing it
with some inputs, to generate some predictions.</p>
</div>
<div class="section" id="input-pre-processing">
<h2>Input pre-processing<a class="headerlink" href="#input-pre-processing" title="Permalink to this headline">¶</a></h2>
<p>In order to generate predictions, we will need two things:</p>
<ul class="simple">
<li><p>the compiled module, which we just produced;</p></li>
<li><p>a valid input to the model</p></li>
</ul>
<p>Each model is particular when it comes to expected tensor shapes, formats and data
types. For this reason, most models require some pre and
post processing, to ensure the input(s) is valid and to interpret the output(s).</p>
<p>In TVMC, we adopted NumPy’s <code class="docutils literal notranslate"><span class="pre">.npz</span></code> format for both input and output data.
This is a well-supported NumPy format to serialize multiple arrays into a file.</p>
<p>We will use the usual cat image, similar to other TVM tutorials:</p>
<a class="reference internal image-reference" href="https://s3.amazonaws.com/model-server/inputs/kitten.jpg"><img alt="https://s3.amazonaws.com/model-server/inputs/kitten.jpg" class="align-center" src="https://s3.amazonaws.com/model-server/inputs/kitten.jpg" style="width: 224px; height: 224px;" /></a>
<p>For our ResNet 50 V2 model, the input is expected to be in ImageNet format.
Here is an example of a script to pre-process an image for ResNet 50 V2.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">tvm.contrib.download</span> <span class="k">import</span> <span class="n">download_testdata</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="k">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">img_url</span> <span class="o">=</span> <span class="s2">&quot;https://s3.amazonaws.com/model-server/inputs/kitten.jpg&quot;</span>
<span class="n">img_path</span> <span class="o">=</span> <span class="n">download_testdata</span><span class="p">(</span><span class="n">img_url</span><span class="p">,</span> <span class="s2">&quot;imagenet_cat.png&quot;</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>

<span class="c1"># Resize it to 224x224</span>
<span class="n">resized_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">img_path</span><span class="p">)</span><span class="o">.</span><span class="n">resize</span><span class="p">((</span><span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">))</span>
<span class="n">img_data</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.asarray.html#numpy.asarray" title="View documentation for numpy.asarray"><span class="n">np</span><span class="o">.</span><span class="n">asarray</span></a><span class="p">(</span><span class="n">resized_image</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

<span class="c1"># ONNX expects NCHW input, so convert the array</span>
<span class="n">img_data</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.transpose.html#numpy.transpose" title="View documentation for numpy.transpose"><span class="n">np</span><span class="o">.</span><span class="n">transpose</span></a><span class="p">(</span><span class="n">img_data</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Normalize according to ImageNet</span>
<span class="n">imagenet_mean</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array" title="View documentation for numpy.array"><span class="n">np</span><span class="o">.</span><span class="n">array</span></a><span class="p">([</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">])</span>
<span class="n">imagenet_stddev</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.array.html#numpy.array" title="View documentation for numpy.array"><span class="n">np</span><span class="o">.</span><span class="n">array</span></a><span class="p">([</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">])</span>
<span class="n">norm_img_data</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.zeros.html#numpy.zeros" title="View documentation for numpy.zeros"><span class="n">np</span><span class="o">.</span><span class="n">zeros</span></a><span class="p">(</span><span class="n">img_data</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="n">norm_img_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">=</span> <span class="p">(</span><span class="n">img_data</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">/</span> <span class="mi">255</span> <span class="o">-</span> <span class="n">imagenet_mean</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">/</span> <span class="n">imagenet_stddev</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="c1"># Add batch dimension</span>
<span class="n">img_data</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.expand_dims.html#numpy.expand_dims" title="View documentation for numpy.expand_dims"><span class="n">np</span><span class="o">.</span><span class="n">expand_dims</span></a><span class="p">(</span><span class="n">norm_img_data</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Save to .npz (outputs imagenet_cat.npz)</span>
<a href="https://numpy.org/doc/stable/reference/generated/numpy.savez.html#numpy.savez" title="View documentation for numpy.savez"><span class="n">np</span><span class="o">.</span><span class="n">savez</span></a><span class="p">(</span><span class="s2">&quot;imagenet_cat&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">img_data</span><span class="p">)</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre>File /workspace/.tvm_test_data/data/imagenet_cat.png exists, skip.
</pre></div>
</div>
</div>
<div class="section" id="running-the-compiled-module">
<h2>Running the compiled module<a class="headerlink" href="#running-the-compiled-module" title="Permalink to this headline">¶</a></h2>
<p>With both the compiled module and input file in hand, we can run it by
invoking <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">run</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>tvmc run <span class="se">\</span>
  --inputs imagenet_cat.npz <span class="se">\</span>
  --output predictions.npz <span class="se">\</span>
  compiled_module.tar
</pre></div>
</div>
<p>When running the above command, a new file <code class="docutils literal notranslate"><span class="pre">predictions.npz</span></code> should
be produced. It contains the output tensors.</p>
<p>In this example, we are running the model on the same machine that we used
for compilation. In some cases we might want to run it remotely via
an RPC Tracker. To read more about these options please check <code class="docutils literal notranslate"><span class="pre">tvmc</span>
<span class="pre">run</span> <span class="pre">--help</span></code>.</p>
</div>
<div class="section" id="output-post-processing">
<h2>Output post-processing<a class="headerlink" href="#output-post-processing" title="Permalink to this headline">¶</a></h2>
<p>As previously mentioned, each model will have its own particular way
of providing output tensors.</p>
<p>In our case, we need to run some post-processing to render the
outputs from ResNet 50 V2 into a more human-readable form.</p>
<p>The script below shows an example of the post-processing to extract
labels from the output of our compiled module.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">os.path</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">from</span> <span class="nn">scipy.special</span> <span class="k">import</span> <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html#scipy.special.softmax" title="View documentation for scipy.special.softmax"><span class="n">softmax</span></a>

<span class="kn">from</span> <span class="nn">tvm.contrib.download</span> <span class="k">import</span> <span class="n">download_testdata</span>

<span class="c1"># Download a list of labels</span>
<span class="n">labels_url</span> <span class="o">=</span> <span class="s2">&quot;https://s3.amazonaws.com/onnx-model-zoo/synset.txt&quot;</span>
<span class="n">labels_path</span> <span class="o">=</span> <span class="n">download_testdata</span><span class="p">(</span><span class="n">labels_url</span><span class="p">,</span> <span class="s2">&quot;synset.txt&quot;</span><span class="p">,</span> <span class="n">module</span><span class="o">=</span><span class="s2">&quot;data&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">labels_path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>

<span class="n">output_file</span> <span class="o">=</span> <span class="s2">&quot;predictions.npz&quot;</span>

<span class="c1"># Open the output and read the output tensor</span>
<span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">output_file</span><span class="p">):</span>
    <span class="k">with</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.load.html#numpy.load" title="View documentation for numpy.load"><span class="n">np</span><span class="o">.</span><span class="n">load</span></a><span class="p">(</span><span class="n">output_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">data</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <a href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.softmax.html#scipy.special.softmax" title="View documentation for scipy.special.softmax"><span class="n">softmax</span></a><span class="p">(</span><span class="n">data</span><span class="p">[</span><span class="s2">&quot;output_0&quot;</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html#numpy.squeeze" title="View documentation for numpy.squeeze"><span class="n">np</span><span class="o">.</span><span class="n">squeeze</span></a><span class="p">(</span><span class="n">scores</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <a href="https://numpy.org/doc/stable/reference/generated/numpy.argsort.html#numpy.argsort" title="View documentation for numpy.argsort"><span class="n">np</span><span class="o">.</span><span class="n">argsort</span></a><span class="p">(</span><span class="n">scores</span><span class="p">)[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>

        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">scores</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;class=&#39;</span><span class="si">%s</span><span class="s2">&#39; with probability=</span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">scores</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
</pre></div>
</div>
<p class="sphx-glr-script-out">Out:</p>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre>File /workspace/.tvm_test_data/data/synset.txt exists, skip.
</pre></div>
</div>
<p>When running the script, a list of predictions should be printed similar
the the example below.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>$ python post_processing.py
<span class="nv">class</span><span class="o">=</span>n02123045 tabby, tabby cat <span class="p">;</span> <span class="nv">probability</span><span class="o">=</span>446.000000
<span class="nv">class</span><span class="o">=</span>n02123159 tiger cat <span class="p">;</span> <span class="nv">probability</span><span class="o">=</span>675.000000
<span class="nv">class</span><span class="o">=</span>n02124075 Egyptian cat <span class="p">;</span> <span class="nv">probability</span><span class="o">=</span>836.000000
<span class="nv">class</span><span class="o">=</span>n02129604 tiger, Panthera tigris <span class="p">;</span> <span class="nv">probability</span><span class="o">=</span>917.000000
<span class="nv">class</span><span class="o">=</span>n04040759 radiator <span class="p">;</span> <span class="nv">probability</span><span class="o">=</span>213.000000
</pre></div>
</div>
</div>
<div class="section" id="tuning-the-model">
<h2>Tuning the model<a class="headerlink" href="#tuning-the-model" title="Permalink to this headline">¶</a></h2>
<p>In some cases, we might not get the expected performance when running
inferences using our compiled module. In cases like this, we can make use
of the auto-tuner, to find a better configuration for our model and
get a boost in performance.</p>
<p>Tuning in TVM refers to the process by which a model is optimized
to run faster on a given target. This differs from training or
fine-tuning in that it does not affect the accuracy of the model,
but only the runtime performance.</p>
<p>As part of the tuning process, TVM will try running many different
operator implementation variants to see which perform best. The
results of these runs are stored in a tuning records file, which is
ultimately the output of the <code class="docutils literal notranslate"><span class="pre">tune</span></code> subcommand.</p>
<p>In the simplest form, tuning requires you to provide three things:</p>
<ul class="simple">
<li><p>the target specification of the device you intend to run this model on;</p></li>
<li><p>the path to an output file in which the tuning records will be stored, and finally,</p></li>
<li><p>a path to the model to be tuned.</p></li>
</ul>
<p>The example below demonstrates how that works in practice:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>tvmc tune <span class="se">\</span>
  --target <span class="s2">&quot;llvm&quot;</span> <span class="se">\</span>
  --output autotuner_records.json <span class="se">\</span>
  resnet50-v2-7.onnx
</pre></div>
</div>
<p>Tuning sessions can take a long time, so <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">tune</span></code> offers many options to
customize your tuning process, in terms of number of repetitions (<code class="docutils literal notranslate"><span class="pre">--repeat</span></code> and
<code class="docutils literal notranslate"><span class="pre">--number</span></code>, for example), the tuning algorithm to be use, and so on.
Check <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">tune</span> <span class="pre">--help</span></code> for more information.</p>
<p>As an output of the tuning process above, we obtained the tuning records stored
in <code class="docutils literal notranslate"><span class="pre">autotuner_records.json</span></code>. This file can be used in two ways:</p>
<ul class="simple">
<li><p>as an input to further tuning (via <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">tune</span> <span class="pre">--tuning-records</span></code>), or</p></li>
<li><p>as an input to the compiler</p></li>
</ul>
<p>The compiler will use the results to generate high performance code for the model
on your specified target. To do that we can use <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">compile</span> <span class="pre">--tuning-records</span></code>.
Check <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">compile</span> <span class="pre">--help</span></code> for more information.</p>
</div>
<div class="section" id="final-remarks">
<h2>Final Remarks<a class="headerlink" href="#final-remarks" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial, we presented TVMC, a command line driver for TVM.
We demonstrated how to compile, run and tune a model, as well
as discussed the need for pre and post processing of inputs and outputs.</p>
<p>Here we presented a simple example using ResNet 50 V2 locally. However, TVMC
supports many more features including cross-compilation, remote execution and
profiling/benchmarking.</p>
<p>To see what other options are available, please have a look at <code class="docutils literal notranslate"><span class="pre">tvmc</span> <span class="pre">--help</span></code>.</p>
<div class="sphx-glr-footer class sphx-glr-footer-example docutils container" id="sphx-glr-download-tutorials-get-started-tvmc-command-line-driver-py">
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/18fb1ab3ed0a0c9f304520f2beaf4fd6/tvmc_command_line_driver.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">tvmc_command_line_driver.py</span></code></a></p>
</div>
<div class="sphx-glr-download docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/dfa0880631b34bb8814952afdc9031d8/tvmc_command_line_driver.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">tvmc_command_line_driver.ipynb</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</div>
</div>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="relay_quick_start.html" class="btn btn-neutral float-right" title="Quick Start Tutorial for Compiling Deep Learning Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../index.html" class="btn btn-neutral float-left" title="Get Started Tutorials" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static//img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <ul class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <li class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2020 Apache Software Foundation | All right reserved</h5>
        </li>
      </ul>

    </div>

    <ul>
      <li class="footernote">Apache TVM is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF. Copyright © 2020 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</li>
    </ul>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>