





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Marvell Machine Learning Integration &mdash; tvm 0.17.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../../_static/tvm-logo-square.png"/>
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Deploy Deep Learning Models" href="../deploy_models/index.html" />
    <link rel="prev" title="Relay BNNS Integration" href="bnns.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/vta>VTA</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/blog>Blog</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmconf.org>Conference</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html">
          

          
            
            <img src="../../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <input type="checkbox" class="version-toggle-box" hidden id="version-toggle">
              <label for="version-toggle" class="version-toggle-label">
                  <div tabindex="0" class="version version-selector version-selector-show">
                    0.17.dev0 <span class="chevron versions-hidden"><svg fill="none" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="m8 4 8 8-8 8" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"/></svg></span><span class="chevron versions-shown"><svg fill="none" height="24" viewBox="0 0 24 24" width="24" xmlns="http://www.w3.org/2000/svg"><path d="m4 8 8 8 8-8" stroke="#000" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"/></svg></span>
                  </div>
                </label>
                <div class="version-details wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
                  <p class="caption" role="heading"><span class="caption-text">Versions</span></p>
                  <ol style="text-align: left">
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="/">0.17.dev0 (main)</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.8.0/">v0.8.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.9.0/">v0.9.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.10.0/">v0.10.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.11.0/">v0.11.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.12.0/">v0.12.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.13.0/">v0.13.0</a></div></li>
                    
                    
                    
                    
                      <li><div class="version"><a style="font-size: 0.8em; padding: 4px" href="v0.14.0/">v0.14.0</a></div></li>
                    
                  </ol>
                </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contributor Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">User Guide</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../tutorial/index.html">User Tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">How To Guides</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../compile_models/index.html">Compile Deep Learning Models</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html">Deploy Models and Integrate TVM</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="index.html#build-the-tvm-runtime-library">Build the TVM runtime library</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#cross-compile-the-tvm-runtime-for-other-architectures">Cross compile the TVM runtime for other architectures</a></li>
<li class="toctree-l3"><a class="reference internal" href="index.html#optimize-and-tune-models-for-target-devices">Optimize and tune models for target devices</a></li>
<li class="toctree-l3 current"><a class="reference internal" href="index.html#deploy-optimized-model-on-target-devices">Deploy optimized model on target devices</a><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="cpp_deploy.html">Deploy TVM Module using C++ API</a></li>
<li class="toctree-l4"><a class="reference internal" href="android.html">Deploy to Android</a></li>
<li class="toctree-l4"><a class="reference internal" href="adreno.html">Deploy to Adreno™ GPU</a></li>
<li class="toctree-l4"><a class="reference internal" href="integrate.html">Integrate TVM into Your Project</a></li>
<li class="toctree-l4"><a class="reference internal" href="hls.html">HLS Backend Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="arm_compute_lib.html">Relay Arm<sup>®</sup> Compute Library Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="tensorrt.html">Relay TensorRT Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="vitis_ai.html">Vitis AI Integration</a></li>
<li class="toctree-l4"><a class="reference internal" href="bnns.html">Relay BNNS Integration</a></li>
<li class="toctree-l4 current"><a class="current reference internal" href="#">Marvell Machine Learning Integration</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="index.html#additional-deployment-how-tos">Additional Deployment How-Tos</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../work_with_relay/index.html">Work With Relay</a></li>
<li class="toctree-l2"><a class="reference internal" href="../work_with_schedules/index.html">Work With Tensor Expression and Schedules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../optimize_operators/index.html">Optimize Tensor Operators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tune_with_autotvm/index.html">Auto-Tune with Templates and AutoTVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tune_with_autoscheduler/index.html">Use AutoScheduler for Template-Free Scheduling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../work_with_microtvm/index.html">Work With microTVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../extend_tvm/index.html">Extend TVM</a></li>
<li class="toctree-l2"><a class="reference internal" href="../profile/index.html">Profile Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../errors.html">Handle TVM Errors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../faq.html">Frequently Asked Questions</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Developer Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../dev/tutorial/index.html">Developer Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dev/how_to/how_to.html">Developer How-To Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Architecture  Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../arch/index.html">Design and Architecture</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Topic Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../topic/microtvm/index.html">microTVM: TVM on bare-metal</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../topic/vta/index.html">VTA: Versatile Tensor Accelerator</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Reference Guide</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/links.html">Other APIs</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="../index.html">How To Guides</a> <span class="br-arrow">></span></li>
        
          <li><a href="index.html">Deploy Models and Integrate TVM</a> <span class="br-arrow">></span></li>
        
      <li>Marvell Machine Learning Integration</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/tvm/edit/main/docs/how_to/deploy/mrvl.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="marvell-machine-learning-integration">
<h1>Marvell Machine Learning Integration<a class="headerlink" href="#marvell-machine-learning-integration" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>1. Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>Marvell(R) supports a family of high performance Data Processing
Units (DPUs) with integrated compute, high speed I/O and workload
accelerators. These workload accelerators includes Marvell’s
Machine Learning Inference Processor (MLIP), a highly optimized,
integrated inference engine.</p>
<p>TVM supports Marvell’s MLIP using the “mrvl” library. This partitions and
compiles supported operations for accelerated execution on MLIP, or LLVM
for general compute.</p>
<p>For runtime, the library supports native execution on MLIP hardware
as well as Marvell’s ML simulator (mrvl-mlsim).</p>
<p>The library supports Marvell’s Octeon family of processors with ML accelarators.</p>
<p>This guide demonstrates building TVM with codegen and
runtime enabled. It also provides example code to compile and run
models using ‘mrvl’ runtime.</p>
</div>
<div class="section" id="building-tvm-with-mrvl-support">
<h2>2. Building TVM with mrvl support<a class="headerlink" href="#building-tvm-with-mrvl-support" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="clone-tvm-repo">
<h2>2.1 Clone TVM repo<a class="headerlink" href="#clone-tvm-repo" title="Permalink to this headline">¶</a></h2>
<p>Refer to the following TVM documentation for cloning TVM
<a class="reference external" href="https://tvm.apache.org/docs/install/from_source.html">https://tvm.apache.org/docs/install/from_source.html</a></p>
</div>
<div class="section" id="build-and-start-the-tvm-mrvl-docker-container">
<h2>2.2 Build and start the TVM - mrvl docker container<a class="headerlink" href="#build-and-start-the-tvm-mrvl-docker-container" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>./docker/build.sh<span class="w"> </span>demo_mrvl<span class="w"> </span>bash<span class="w">  </span><span class="c1"># Build the docker container</span>
./docker/bash.sh<span class="w"> </span>tvm.demo_mrvl<span class="w">    </span><span class="c1"># Load the docker image</span>
</pre></div>
</div>
</div>
<div class="section" id="compiling-a-model-using-tvmc-command-line">
<h2>3. Compiling a model using TVMC command line<a class="headerlink" href="#compiling-a-model-using-tvmc-command-line" title="Permalink to this headline">¶</a></h2>
<p>Models can be compiled and run for mrvl target using TVMC
which is optimized for performance.</p>
<p>Refer to the following TVMC documentation, for tvmc generic options.
<a class="reference external" href="https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html">https://tvm.apache.org/docs/tutorial/tvmc_command_line_driver.html</a></p>
<p>Additional mrvl-specific options may be added as attributes if
necessary. The advanced usage is described in this document below.</p>
</div>
<div class="section" id="tvmc-compilation-flow-for-a-model">
<h2>3.1 TVMC Compilation Flow for a model<a class="headerlink" href="#tvmc-compilation-flow-for-a-model" title="Permalink to this headline">¶</a></h2>
<p>Refer to the following TVM documentation, for compilation flow
<a class="reference external" href="https://tvm.apache.org/docs/arch/index.html#example-compilation-flow">https://tvm.apache.org/docs/arch/index.html#example-compilation-flow</a></p>
</div>
<div class="section" id="tvmc-command-line-option-s-syntax-for-mrvl-target">
<h2>3.2. TVMC - Command line option(s): Syntax for mrvl target<a class="headerlink" href="#tvmc-command-line-option-s-syntax-for-mrvl-target" title="Permalink to this headline">¶</a></h2>
<p>Compiling an ONNX model using the tvmc for mrvl target.</p>
<p><strong>Syntax:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">tvm</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">tvmc</span> <span class="nb">compile</span> <span class="o">--</span><span class="n">target</span><span class="o">=</span><span class="s2">&quot;mrvl, llvm&quot;</span>
    <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">llvm</span><span class="o">-&lt;</span><span class="n">options</span><span class="o">&gt;</span>
    <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">mrvl</span><span class="o">-&lt;</span><span class="n">options</span><span class="o">&gt;</span>
    <span class="o">--&lt;</span><span class="n">tvm</span><span class="o">-</span><span class="n">generic</span><span class="o">-</span><span class="n">options</span><span class="o">&gt;</span>
    <span class="n">model_file</span><span class="o">.</span><span class="n">onnx</span>
</pre></div>
</div>
<p>Following is an example TVMC Compile command for an ARMv9 core and
integrated MLIP cn10ka processor, using only 4 tiles in the block.</p>
<p><strong>Example:</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">tvm</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">tvmc</span> <span class="nb">compile</span> <span class="o">--</span><span class="n">target</span><span class="o">=</span><span class="s2">&quot;mrvl, llvm&quot;</span> \
    <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">llvm</span><span class="o">-</span><span class="n">mtriple</span><span class="o">=</span><span class="n">aarch64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span> <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">llvm</span><span class="o">-</span><span class="n">mcpu</span><span class="o">=</span><span class="n">neoverse</span><span class="o">-</span><span class="n">n2</span> \
    <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">mrvl</span><span class="o">-</span><span class="n">num_tiles</span><span class="o">=</span><span class="mi">4</span> \
    <span class="o">--</span><span class="n">cross</span><span class="o">-</span><span class="n">compiler</span> <span class="n">aarch64</span><span class="o">-</span><span class="n">linux</span><span class="o">-</span><span class="n">gnu</span><span class="o">-</span><span class="n">gcc</span> \
    <span class="o">--</span><span class="n">output</span> <span class="n">model</span><span class="o">.</span><span class="n">tar</span> \
    <span class="n">mnist</span><span class="o">-</span><span class="mf">12.</span><span class="n">onnx</span>
</pre></div>
</div>
<p>The runtime support for hardware acceleration is a WIP, it will be added in future PR.</p>
</div>
<div class="section" id="tvmc-compiler-mrvl-specific-command-line-options">
<h2>3.3. TVMC Compiler: mrvl specific Command Line Options<a class="headerlink" href="#tvmc-compiler-mrvl-specific-command-line-options" title="Permalink to this headline">¶</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">mrvl</span><span class="o">-</span><span class="n">mcpu</span>
<span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">mrvl</span><span class="o">-</span><span class="n">num_tiles</span>
<span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">mrvl</span><span class="o">-</span><span class="n">mattr</span>
</pre></div>
</div>
<p><strong>Description of mrvl options</strong></p>
<ul>
<li><dl class="simple">
<dt>mcpu:</dt><dd><p>The CPU class of Marvell(R) ML Inference Processor;
possible values = {cn10ka, cnf10kb}; defaults to cn10ka</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>num_tiles:</dt><dd><p>Maximum number of tiles that may be used, possible values = {1,2,4,8}, defaults to 8</p>
</dd>
</dl>
</li>
<li><dl>
<dt>mattr:</dt><dd><p>Attributes for mrvl; possible values = {quantize, wb_pin_ocm}</p>
<p>mattr specifies the data type, code generation options and optimizations.</p>
<p><em>List of supported attributes are:</em></p>
<p><strong>1. quantize</strong></p>
<p>Specify the data type. Possible values = {fp16, int8}.
Default is fp16, int8 is WIP and full support will be added in a future PR.</p>
<p><strong>2. wb_pin_ocm</strong></p>
<p>Optimize runtime by preloading a model’s weights and bias into
the on chip memory. Possible values = {0, 1}. Default is 0 (no preload)</p>
</dd>
</dl>
</li>
</ul>
</div>
<div class="section" id="compile-onnx-model-for-simulator-llvm-x86-64-target">
<h2>4. Compile ONNX model for Simulator + LLVM / x86_64 target<a class="headerlink" href="#compile-onnx-model-for-simulator-llvm-x86-64-target" title="Permalink to this headline">¶</a></h2>
<p>In the TVMC mrvl flow, the model is partitioned into Marvell and LLVM regions.
Building each partitioned Marvell subgraph generates serialized nodes.json and
const.json. Partitioned nodes.json is the representation of the model graph which is
suitable for the Marvell compiler (mrvl-tmlc). The compiler compiles the model graph to
generate the model binary with MLIP instructions.</p>
<p><strong>Model Compilation for Simulator + LLVM / x86_64 target</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">tvm</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">tvmc</span> <span class="nb">compile</span> <span class="o">--</span><span class="n">target</span><span class="o">=</span><span class="s2">&quot;mrvl, llvm&quot;</span> \
    <span class="o">--</span><span class="n">target</span><span class="o">-</span><span class="n">mrvl</span><span class="o">-</span><span class="n">num_tiles</span><span class="o">=</span><span class="mi">4</span> <span class="o">--</span><span class="n">output</span> <span class="n">model</span><span class="o">.</span><span class="n">tar</span> <span class="n">model</span><span class="o">.</span><span class="n">onnx</span>
</pre></div>
</div>
<p><strong>Run TVM models on x86_64 host using MLIP Simulator</strong></p>
<p>Generated model binary is simulated using Marvell’s MLIP Simulator(mrvl-mlsim).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python3</span> <span class="o">-</span><span class="n">m</span> <span class="n">tvm</span><span class="o">.</span><span class="n">driver</span><span class="o">.</span><span class="n">tvmc</span> <span class="n">run</span> <span class="o">--</span><span class="n">inputs</span> <span class="n">infer</span><span class="o">.</span><span class="n">npz</span> <span class="o">--</span><span class="n">outputs</span> <span class="n">predict</span><span class="o">.</span><span class="n">npz</span> <span class="n">model</span><span class="o">.</span><span class="n">tar</span> <span class="o">--</span><span class="n">number</span><span class="o">=</span><span class="mi">0</span>
</pre></div>
</div>
</div>
<div class="section" id="compiling-a-model-using-python-apis">
<h2>5. Compiling a model using Python APIs<a class="headerlink" href="#compiling-a-model-using-python-apis" title="Permalink to this headline">¶</a></h2>
<p>In addition to using TVMC, models can also be compiled and run using
TVM Python API. Below is an example to compile and run the MNIST model.</p>
<p><strong>Download MNIST model from the web</strong></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">cd</span><span class="w"> </span><span class="nv">$HOME</span>
wget<span class="w"> </span>https://github.com/onnx/models/raw/main/validated/vision/classification/mnist/model/mnist-12.onnx
</pre></div>
</div>
<p><strong>Import the TVM and other dependent modules</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tvm</span><span class="o">,</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tvm.relay</span> <span class="k">as</span> <span class="nn">relay</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">graph_executor</span>
<span class="kn">from</span> <span class="nn">tvm.relay.op.contrib.mrvl</span> <span class="kn">import</span> <span class="n">partition_for_mrvl</span>
<span class="kn">from</span> <span class="nn">tvm.relay.build_module</span> <span class="kn">import</span> <span class="n">build</span>
<span class="kn">from</span> <span class="nn">keras.datasets</span> <span class="kn">import</span> <span class="n">mnist</span>
</pre></div>
</div>
<p><strong>Load model onnx file</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">onnx_model</span> <span class="o">=</span> <span class="n">onnx</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;mnist-12.onnx&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Create a Relay graph from MNIST model</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">shape_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Input3&#39;</span> <span class="p">:</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">28</span><span class="p">,</span><span class="mi">28</span><span class="p">)}</span>
<span class="n">mod</span><span class="p">,</span> <span class="n">params</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">frontend</span><span class="o">.</span><span class="n">from_onnx</span><span class="p">(</span><span class="n">onnx_model</span><span class="p">,</span> <span class="n">shape_dict</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Define option dictionary and Partition the Model</strong></p>
<p>Annotate and partition the graph for mrvl. All operations which are supported
by the mrvl will be marked and offloaded to mrvl hardware accelerator. The rest of the
operations will go through the regular LLVM compilation and code generation for ARM.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tvm_target</span> <span class="o">=</span> <span class="s2">&quot;llvm&quot;</span>

<span class="n">option_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;num_tiles&#39;</span><span class="p">:</span> <span class="mi">4</span><span class="p">}</span>

<span class="n">mod</span> <span class="o">=</span> <span class="n">partition_for_mrvl</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="o">**</span><span class="n">option_dict</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Build the Relay Graph</strong></p>
<p>Build the Relay graph, using the new module returned by partition_for_mrvl.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;relay.ext.mrvl.options&quot;</span> <span class="p">:</span> <span class="n">option_dict</span><span class="p">}):</span>
    <span class="n">model_lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">tvm_target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Generate runtime graph of the model library</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">model_rt_graph</span> <span class="o">=</span> <span class="n">graph_executor</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">model_lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">dev</span><span class="p">))</span>
</pre></div>
</div>
<p><strong>Get test data and initialize model input</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">),</span> <span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">)</span> <span class="o">=</span> <span class="n">mnist</span><span class="o">.</span><span class="n">load_data</span><span class="p">()</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">test_X</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span> <span class="o">/</span> <span class="mi">255</span><span class="p">)</span>
<span class="n">inputs_dict</span> <span class="o">=</span> <span class="p">{}</span>
<span class="n">inputs_dict</span><span class="p">[</span><span class="s2">&quot;Input3&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">image</span>
<span class="n">model_rt_graph</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="o">**</span><span class="n">inputs_dict</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Run Inference and print the output</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model_rt_graph</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
<span class="n">output_tensor</span> <span class="o">=</span> <span class="n">model_rt_graph</span><span class="o">.</span><span class="n">get_output</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span> <span class="p">(</span><span class="n">output_tensor</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../deploy_models/index.html" class="btn btn-neutral float-right" title="Deploy Deep Learning Models" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="bnns.html" class="btn btn-neutral float-left" title="Relay BNNS Integration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2023 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2023 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    

    
   

</body>
</html>