{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\n# Installs TVM version 0.13.0 from PyPI. If you wish to build\n# from source, see https://tvm.apache.org/docs/install/from_source.html\npip install apache-tvm==0.13.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# 9. Bring microTVM to your own development environment\n**Author**:\n[Mohamad Katanbaf](https://github.com/mkatanbaf)\n\nThis tutorial describes the steps required to integrate a model compiled with microTVM into a custom development environment.\nWe use [STM32CubeIDE](https://www.st.com/en/development-tools/stm32cubeide.html), as the target IDE in this tutorial, but we do not rely on any specific feature of this IDE and integrating microTVM in other IDEs would be similar.\nWe also use the Visual Wake Word (VWW) model from MLPerf Tiny and the nucleo_l4r5zi board here, but the same steps can be used for any other model or target MCU.\nIf you want to use another target MCU with the vww model, we recommend a cortex-M4 or cortex-M7 device with ~512 KB and ~256 KB of Flash and RAM respectively.\n\nHere is a brief overview of the steps that we would take in this tutorial.\n\n1. We start by importing the model, compiling it using TVM and generating the [Model Library Format](https://tvm.apache.org/docs/arch/model_library_format.html) (MLF) tar-file that includes the generated code for the model as well as all the required TVM dependencies.\n2. We also add two sample images in binary format (one person and one not-person sample) to the .tar file for evaluating the model.\n3. Next we use the stmCubeMX to generate the initialization code for the project in stmCube IDE.\n4. After that, we include our MLF file and the required CMSIS libraries in the project and build it.\n5. Finally, we flash the device and evaluate the model performance on our sample images.\n\nLet's Begin.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install microTVM Python dependencies\n\nTVM does not include a package for Python serial communication, so\nwe must install one before using microTVM. We will also need TFLite\nto load models, and Pillow to prepare the sample images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%shell\npip install pyserial==3.5 tflite==2.1 Pillow==9.0 typing_extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Python dependencies\n\nIf you want to run this script locally, check out [TVM Online Documentation](https://tvm.apache.org/docs/install/index.html) for instructions to install TVM.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport numpy as np\nimport pathlib\nimport json\nfrom PIL import Image\nimport tarfile\n\nimport tvm\nfrom tvm import relay\nfrom tvm.relay.backend import Executor, Runtime\nfrom tvm.contrib.download import download_testdata\nfrom tvm.micro import export_model_library_format\nfrom tvm.relay.op.contrib import cmsisnn\nfrom tvm.micro.testing.utils import create_header_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the TFLite model\n\nTo begin with, download and import a Visual Wake Word TFLite model. This model takes in a 96x96x3 RGB image and determines whether a person is present in the image or not.\nThis model is originally from [MLPerf Tiny repository](https://github.com/mlcommons/tiny).\nTo test this model, we use two samples from [COCO 2014 Train images](https://cocodataset.org/).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "MODEL_URL = \"https://github.com/mlcommons/tiny/raw/bceb91c5ad2e2deb295547d81505721d3a87d578/benchmark/training/visual_wake_words/trained_models/vww_96_int8.tflite\"\nMODEL_NAME = \"vww_96_int8.tflite\"\nMODEL_PATH = download_testdata(MODEL_URL, MODEL_NAME, module=\"model\")\n\ntflite_model_buf = open(MODEL_PATH, \"rb\").read()\ntry:\n    import tflite\n\n    tflite_model = tflite.Model.GetRootAsModel(tflite_model_buf, 0)\nexcept AttributeError:\n    import tflite.Model\n\n    tflite_model = tflite.Model.Model.GetRootAsModel(tflite_model_buf, 0)\n\ninput_shape = (1, 96, 96, 3)\nINPUT_NAME = \"input_1_int8\"\nrelay_mod, params = relay.frontend.from_tflite(\n    tflite_model, shape_dict={INPUT_NAME: input_shape}, dtype_dict={INPUT_NAME: \"int8\"}\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the Model Library Format file\n\nFirst we define the target, runtime and executor. Then we compile the model for the target device and\nfinally we export the generated code and all the required dependencies in a single file.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We can use TVM native schedules or rely on the CMSIS-NN kernels using TVM Bring-Your-Own-Code (BYOC) capability.\nUSE_CMSIS_NN = True\n\n# USMP (Unified Static Memory Planning) performs memory planning of all tensors holistically to achieve best memory utilization\nDISABLE_USMP = False\n\n# Use the C runtime (crt)\nRUNTIME = Runtime(\"crt\")\n\n# We define the target by passing the board name to `tvm.target.target.micro`.\n# If your board is not included in the supported models, you can define the target such as:\n# TARGET = tvm.target.Target(\"c -keys=arm_cpu,cpu -mcpu=cortex-m4\")\nTARGET = tvm.target.target.micro(\"stm32l4r5zi\")\n\n# Use the AOT executor rather than graph or vm executors. Use unpacked API and C calling style.\nEXECUTOR = tvm.relay.backend.Executor(\n    \"aot\", {\"unpacked-api\": True, \"interface-api\": \"c\", \"workspace-byte-alignment\": 8}\n)\n\n# Now, we set the compilation configurations and compile the model for the target:\nconfig = {\"tir.disable_vectorize\": True}\nif USE_CMSIS_NN:\n    config[\"relay.ext.cmsisnn.options\"] = {\"mcpu\": TARGET.mcpu}\nif DISABLE_USMP:\n    config[\"tir.usmp.enable\"] = False\n\nwith tvm.transform.PassContext(opt_level=3, config=config):\n    if USE_CMSIS_NN:\n        # When we are using CMSIS-NN, TVM searches for patterns in the\n        # relay graph that it can offload to the CMSIS-NN kernels.\n        relay_mod = cmsisnn.partition_for_cmsisnn(relay_mod, params, mcpu=TARGET.mcpu)\n    lowered = tvm.relay.build(\n        relay_mod, target=TARGET, params=params, runtime=RUNTIME, executor=EXECUTOR\n    )\nparameter_size = len(tvm.runtime.save_param_dict(lowered.get_params()))\nprint(f\"Model parameter size: {parameter_size}\")\n\n# We need to pick a directory where our file will be saved.\n# If running on Google Colab, we'll save everything in ``/root/tutorial`` (aka ``~/tutorial``)\n# but you'll probably want to store it elsewhere if running locally.\n\nBUILD_DIR = pathlib.Path(\"/root/tutorial\")\n\nBUILD_DIR.mkdir(exist_ok=True)\n\n# Now, we export the model into a tar file:\nTAR_PATH = pathlib.Path(BUILD_DIR) / \"model.tar\"\nexport_model_library_format(lowered, TAR_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Add sample images to the MLF files\nFinally, we downlaod two sample images (one person and one not-person), convert them to binary format and store them in two header files.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "with tarfile.open(TAR_PATH, mode=\"a\") as tar_file:\n    SAMPLES_DIR = \"samples\"\n    SAMPLE_PERSON_URL = (\n        \"https://github.com/tlc-pack/web-data/raw/main/testdata/microTVM/data/vww_sample_person.jpg\"\n    )\n    SAMPLE_NOT_PERSON_URL = \"https://github.com/tlc-pack/web-data/raw/main/testdata/microTVM/data/vww_sample_not_person.jpg\"\n\n    SAMPLE_PERSON_PATH = download_testdata(SAMPLE_PERSON_URL, \"person.jpg\", module=SAMPLES_DIR)\n    img = Image.open(SAMPLE_PERSON_PATH)\n    create_header_file(\"sample_person\", np.asarray(img), SAMPLES_DIR, tar_file)\n\n    SAMPLE_NOT_PERSON_PATH = download_testdata(\n        SAMPLE_NOT_PERSON_URL, \"not_person.jpg\", module=SAMPLES_DIR\n    )\n    img = Image.open(SAMPLE_NOT_PERSON_PATH)\n    create_header_file(\"sample_not_person\", np.asarray(img), SAMPLES_DIR, tar_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At this point you have all you need to take the compiled model to your IDE and evaluate it. Inside the MLF file (model.tar), you should find the following file hierearchy:\n\n```\n/root\n\u251c\u2500\u2500 codegen\n\u251c\u2500\u2500 parameters\n\u251c\u2500\u2500 runtime\n\u251c\u2500\u2500 samples\n\u251c\u2500\u2500 src\n\u251c\u2500\u2500 templates\n\u251c\u2500\u2500 metadata.json\n```\n* The codegen folder includes the C code TVM generated for your model.\n* The runtime folder includes all the TVM dependencies that the target needs to compile the generated C code.\n* The samples folder includes the two generated sample files for evaluating the model.\n* The src folder includes the relay module describing the model.\n* The templates folder includes two template files that you might need to edit based on your platform.\n* The metadata.json file includes information about the model, its layers and memory requirement.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate the project in your IDE\n\nThe next step is to create a project for our target device. We use STM32CubeIDE, you can download it [here](https://www.st.com/en/development-tools/stm32cubeide.html).\nWe are using version 1.11.0 in this tutorial. Once you install STM32CubeIDE follow these steps to create a project:\n\n#. select File -> New -> STM32Project. The target selection Window appears.\n\n#. Navigate to the \"Board Selector\" tab, type in the board name \"nucleo-l4r5zi\" in the \"Commercial Part Number\" text box. Select the board from the list of boards that appear on the right side of the screen and click \"Next\".\n\n#. Type in your project name (for example microtvm_vww_demo). We are using the default options. (Target Language: C, Binary Type: Executable, Project Type: STM32Cube). Click \"Finish\".\n\n#. A text box will appear asking if you want to \"Initialize all the peripherals with their default mode?\". click \"Yes\". This will generate the project and open the device configuration tool where you can use the GUI to setup the peripherals. By default the USB, USART3 and LPUART1 are enabled, as well as a few GPIOs.\n\n#. We will use LPUART1 to send data to the host pc. From the connectivity section, select the LPUART1 and set the \"Baud Rate\" to 115200 and the \"Word Length\" to 8. Save the changes and click \"Yes\" to regenerate the initialization code. This should regenerate the code and open your main.c file. You can also find main.c from the Project Explorer panel on the left, under microtvm_vww_demo -> Core -> Src.\n\n#. For sanity check, copy the code below and paste it in the \"Infinite loop (aka. While (1) ) section of the main function.\n\n   * Note: Make sure to write your code inside the sections marked by USER CODE BEGIN <...> and USER CODE END <...>. The code outside these sections get erased if you regenerate the initialization code.\n\n```c\nHAL_GPIO_TogglePin(LD2_GPIO_Port, LD2_Pin);\nHAL_UART_Transmit(&hlpuart1, \"Hello World.\\r\\n\", 14, 100);\nHAL_Delay(1000);\n```\n#. From the menu bar, select Project -> Build (or right click on project name and select Build). This should build the project and generate the .elf file. Select Run -> Run to download the binary on your MCU. If the \"Edit Configuration\" window opens, just click \"OK\".\n\n#. Open the terminal console on your host machine. On Mac you can simply use the \"screen <usb_device> 115200\" command, e.g. \"screen tty.usbmodemXXXX 115200\". An LED should blink on the board and the string \"Hello World.\" should print out on your terminal console every second. Press \"Control-a k\" to exit screen.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import the model to the generated project\n\nTo integrate the compiled model into the generated project, follow these steps:\n\n#. Extract the tar file and include it in the project\n\n   * Open the project Properties. (by right clicking on the project name and selecting \"Properties\" or by selecting Project -> Properties from the menu bar).\n   * Select C/C++ General -> Paths and Symbols. Select the Source Location tab.\n   * If you extracted the model inside the project folder, click \"Add Folder\" and select the \"model\" folder. (You might need to right click on the project name and select \"Refresh\" before it appears.)\n   * If you extracted the model file somewhere else, click on the \"Link Folder\" button, check the box for \"Link to folder in the file system\" in the window that appears, click \"Browse\" and select the model folder.\n\n#. If you used CMSIS-NN in compiling the model, you need to include the CMSIS-NN source files in your project too.\n\n   * Download or clone the files from the [CMSIS-NN repository](https://github.com/ARM-software/CMSIS-NN), and follow the above steps to include the CMSIS-NN folder in the project.\n\n#. Open the project properties. In C/C++ Build -> Settings: add the following folders to the list of Include Paths for MCU GCC Compiler (and MCU G++ Compiler if you have a C++ project) by clicking on the \"+\" button, selecting \"Workspace\" and navigating to each of the following folders:\n\n   * model/runtime/include\n   * model/codegen/host/include\n   * model/samples\n   * CMSIS-NN/Include\n\n#. Copy crt_config.h.template from model/templates to the Core/Inc folder, and rename it to crt_config.h.\n\n#. Copy platform.c.template from model/templates to the Core/Src folder, and rename it to platform.c.\n   * This file includes functions for managing the memory that you might need to edit based on your platform.\n   * define \"TVM_WORKSPACE_SIZE_BYTES\" in platform.c. if you are using USMP, a small value (for example 1024 Bytes) is enough.\n   * if you are not using usmp, checkout \"workspace_size_bytes\" field in metadata.json for an estimate of the required memory.\n\n#. Exclude the following folders from build (right click on the folder name, select Resource Configuration \u2192 Exclude from build). Check Debug and Release configurations.\n\n   * CMSIS_NN/Tests\n\n#. Download the CMSIS drivers from [CMSIS Version 5 repository](https://github.com/ARM-software/CMSIS_5).\n\n   * In your Project directory, delete the Drivers/CMSIS/Include folder (which is an older version of the CMSIS drivers) and copy the CMSIS/Core/Include from the one you downloaded in its place.\n\n#. Edit the main.c file:\n\n   * Include following header files:\n\n```c\n#include <stdio.h>\n#include <string.h>\n#include <stdarg.h>\n#include \"tvmgen_default.h\"\n#include \"sample_person.h\"\n#include \"sample_not_person.h\"\n```\n   * Copy the following code into the main function right before the infinite loop. It sets the input and output to the model.\n\n```c\nTVMPlatformInitialize();\nsigned char output[2];\nstruct tvmgen_default_inputs inputs = {\n.input_1_int8 = (void*)&sample_person,\n};\nstruct tvmgen_default_outputs outputs = {\n.Identity_int8 = (void*)&output,\n};\nchar msg[] = \"Evaluating VWW model using microTVM:\\r\\n\";\nHAL_UART_Transmit(&hlpuart1, msg, strlen(msg), 100);\nuint8_t sample = 0;\nuint32_t timer_val;\nchar buf[50];\nuint16_t buf_len;\n```\n   * Copy the following code inside the infinite loop to run inference on both images and print the result on the console:\n\n```c\nif (sample == 0)\n    inputs.input_1_int8 = (void*)&sample_person;\nelse\n    inputs.input_1_int8 = (void*)&sample_not_person;\n\ntimer_val = HAL_GetTick();\ntvmgen_default_run(&inputs, &outputs);\ntimer_val = HAL_GetTick() - timer_val;\nif (output[0] > output[1])\n    buf_len = sprintf(buf, \"Person not detected, inference time = %lu ms\\r\\n\", timer_val);\nelse\n    buf_len = sprintf(buf, \"Person detected, inference time = %lu ms\\r\\n\", timer_val);\nHAL_UART_Transmit(&hlpuart1, buf, buf_len, 100);\n\nsample++;\nif (sample == 2)\n    sample = 0;\n```\n   * Define the TVMLogf function in main, to receive TVM runtime errors on serial console.\n\n```c\nvoid TVMLogf(const char* msg, ...) {\n  char buffer[128];\n  int size;\n  va_list args;\n  va_start(args, msg);\n  size = TVMPlatformFormatMessage(buffer, 128, msg, args);\n  va_end(args);\n  HAL_UART_Transmit(&hlpuart1, buffer, size, 100);\n}\n```\n#. In project properties, C/C++ Build -> Settings, MCU GCC Compiler -> Optimization, set the Optimization level to \"Optimize more (-O2)\"\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluate the model\n\nNow, select Run -> Run from the menu bar to flash the MCU and run the project.\nYou should see the LED blinking and the inference result printing on the console.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}