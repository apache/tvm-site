

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Adding an Operator to Relay &mdash; tvm 0.7.dev1 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/tvm-logo-square.png"/>
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tvm_theme.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Adding a Compiler Pass to Relay" href="relay_add_pass.html" />
    <link rel="prev" title="Benchmark Performance Log Format" href="benchmark.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.7.dev1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../vta/index.html">VTA: Deep Learning Accelerator Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deploy/index.html">Deploy and Integration</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/index.html">Contribute to TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/links.html">Links to API References</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Design and Architecture</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#example-compilation-flow">Example Compilation Flow</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#logical-architecture-components">Logical Architecture Components</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-support">tvm/support</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-runtime">tvm/runtime</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-node">tvm/node</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-ir">tvm/ir</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-target">tvm/target</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-tir">tvm/tir</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-arith">tvm/arith</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-te">tvm/te</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#topi">topi</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-relay">tvm/relay</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#tvm-autotvm">tvm/autotvm</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#how-tos">How Tos</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Adding an Operator to Relay</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#registering-an-operator">Registering an Operator</a></li>
<li class="toctree-l4"><a class="reference internal" href="#creating-a-call-node">Creating a Call Node</a></li>
<li class="toctree-l4"><a class="reference internal" href="#including-a-python-api-hook">Including a Python API Hook</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gradient-operators">Gradient Operators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="relay_add_pass.html">Adding a Compiler Pass to Relay</a></li>
<li class="toctree-l3"><a class="reference internal" href="relay_bring_your_own_codegen.html">Bring Your Own Codegen To TVM</a></li>
<li class="toctree-l3"><a class="reference internal" href="codebase_walkthrough.html">TVM Codebase Walkthrough by Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="index.html#security">Security</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../frontend/tensorflow.html">TensorFlow Frontend</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">tvm</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="index.html">Design and Architecture</a> &raquo;</li>
        
      <li>Adding an Operator to Relay</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/dev/relay_add_op.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="adding-an-operator-to-relay">
<span id="relay-add-op"></span><h1>Adding an Operator to Relay<a class="headerlink" href="#adding-an-operator-to-relay" title="Permalink to this headline">¶</a></h1>
<p>In order to use TVM operators from within the Relay IR, the
operators need to be registered in Relay in order to ensure
that they will be integrated into Relay’s type system.</p>
<p>Registering an operator requires three steps:</p>
<ul class="simple">
<li><p>Using the <code class="docutils literal notranslate"><span class="pre">RELAY_REGISTER_OP</span></code> macro in C++ to register the operator’s arity and type information</p></li>
<li><p>Defining a C++ function to produce a call node for the operator and registering a Python API hook for the function</p></li>
<li><p>Wrapping the above Python API hook in a neater interface</p></li>
</ul>
<p>The file <code class="docutils literal notranslate"><span class="pre">src/relay/op/tensor/binary.cc</span></code> provides
examples of the first two steps, while
<code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/tensor.py</span></code> gives examples of the
last.</p>
<div class="section" id="registering-an-operator">
<h2>Registering an Operator<a class="headerlink" href="#registering-an-operator" title="Permalink to this headline">¶</a></h2>
<p>TVM already has an operator registry, but Relay cannot properly
incorporate TVM operators without additional type information.</p>
<p>To allow for flexibility in registering operators and greater
expressivity and granularity in expressing types in Relay, operators
are typed using relations between input and output types. These relations
are represented as functions that take in a list of input types and
output types (any of these types may be incomplete) and return a list
of input and output types that satisfies the relation. Essentially, a
relation for an operator can enforce all the necessary typing rules
(namely by inspecting the input types) in addition to computing the
output type.</p>
<p>For example, see <code class="docutils literal notranslate"><span class="pre">src/relay/op/type_relations.h</span></code> and their
implementations. E.g., <code class="docutils literal notranslate"><span class="pre">BroadcastRel</span></code> takes two input types and an
output type, checks that they are all tensor types with the same underlying
data type, and finally ensures that the shape of the output type is the
broadcast of the input types’ shapes.</p>
<p>It may be necessary to add another type relation to <code class="docutils literal notranslate"><span class="pre">type_relations.h</span></code>
if the existing ones do not capture the behavior of the desired operator.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">RELAY_REGISTER_OP</span></code> macro in C++ allows a developer
to specify the following information about an operator in Relay:</p>
<ul class="simple">
<li><p>Arity (number of arguments)</p></li>
<li><p>Names and descriptions for positional arguments</p></li>
<li><p>Support level (1 indicates an internal intrinsic; higher numbers indicate less integral or externally supported operators)</p></li>
<li><p>A type relation for the operator</p></li>
</ul>
<p>The below example is from <code class="docutils literal notranslate"><span class="pre">binary.cc</span></code> and uses a broadcasting
add for tensors:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span class="n">RELAY_REGISTER_OP</span><span class="p">(</span><span class="s">&quot;add&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">set_num_inputs</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&quot;lhs&quot;</span><span class="p">,</span> <span class="s">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s">&quot;The left hand side tensor.&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s">&quot;rhs&quot;</span><span class="p">,</span> <span class="s">&quot;Tensor&quot;</span><span class="p">,</span> <span class="s">&quot;The right hand side tensor.&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">set_support_level</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="p">.</span><span class="n">add_type_rel</span><span class="p">(</span><span class="s">&quot;Broadcast&quot;</span><span class="p">,</span> <span class="n">BroadcastRel</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="section" id="creating-a-call-node">
<h2>Creating a Call Node<a class="headerlink" href="#creating-a-call-node" title="Permalink to this headline">¶</a></h2>
<p>This step requires simply writing a function that takes
the arguments to the operator (as Relay expressions) and
returning a call node to the operator (i.e., the node that
should be placed into the Relay AST where the call to the
operator is intended).</p>
<p>At present call attributes and type arguments (the last two fields)
are not supported, so it suffices to use <code class="docutils literal notranslate"><span class="pre">Op::Get</span></code> to fetch
the operator’s information from the operator registry and pass in
the arguments to the call node, as below.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span class="n">TVM_REGISTER_GLOBAL</span><span class="p">(</span><span class="s">&quot;relay.op._make.add&quot;</span><span class="p">)</span>
    <span class="p">.</span><span class="n">set_body_typed</span><span class="o">&lt;</span><span class="n">Expr</span><span class="p">(</span><span class="n">Expr</span><span class="p">,</span> <span class="n">Expr</span><span class="p">)</span><span class="o">&gt;</span><span class="p">([](</span><span class="n">Expr</span> <span class="n">lhs</span><span class="p">,</span> <span class="n">Expr</span> <span class="n">rhs</span><span class="p">)</span> <span class="p">{</span>
        <span class="k">static</span> <span class="k">const</span> <span class="n">Op</span><span class="o">&amp;</span> <span class="n">op</span> <span class="o">=</span> <span class="n">Op</span><span class="o">::</span><span class="n">Get</span><span class="p">(</span><span class="s">&quot;add&quot;</span><span class="p">);</span>
      <span class="k">return</span> <span class="nf">Call</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="p">{</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">},</span> <span class="n">Attrs</span><span class="p">(),</span> <span class="p">{});</span>
    <span class="p">});</span>
</pre></div>
</div>
</div>
<div class="section" id="including-a-python-api-hook">
<h2>Including a Python API Hook<a class="headerlink" href="#including-a-python-api-hook" title="Permalink to this headline">¶</a></h2>
<p>It is generally the convention in Relay, that functions exported
through <code class="docutils literal notranslate"><span class="pre">TVM_REGISTER_GLOBAL</span></code> should be wrapped in a separate
Python function rather than called directly in Python. In the case
of the functions that produce calls to operators, it may be convenient
to bundle them, as in <code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/tensor.py</span></code>, where
elementwise operators on tensors are all provided. For example,
the following is how the add function from the previous section is
exposed in Python:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="k">def</span> <span class="nf">add</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Elementwise addition.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    lhs : relay.Expr</span>
<span class="sd">        The left hand side input data</span>
<span class="sd">    rhs : relay.Expr</span>
<span class="sd">        The right hand side input data</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    result : relay.Expr</span>
<span class="sd">        The computed result.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">_make</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">lhs</span><span class="p">,</span> <span class="n">rhs</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that these Python wrappers might also be good opportunities to
provide an easier interface to the operator. For example, the
<code class="docutils literal notranslate"><span class="pre">concat</span></code> operator is registered as taking only one operator,
namely a tuple with the tensors to be concatenated, but the Python
wrapper takes the tensors as arguments and combines them into a tuple
before producing the call node:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="k">def</span> <span class="nf">concat</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Concatenate the input tensors along the zero axis.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    args: list of Tensor</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tensor: The concatenated tensor.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tup</span> <span class="o">=</span> <span class="n">Tuple</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">args</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">_make</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span><span class="n">tup</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="gradient-operators">
<h2>Gradient Operators<a class="headerlink" href="#gradient-operators" title="Permalink to this headline">¶</a></h2>
<p>Gradient operators are important for writing differentiable programs in
Relay. While it is the case that Relay’s autodiff algorithm can differentiate
first-class language constructs, operators are opaque. Because Relay can’t
look into the implementation, an explicit differentiation rule must be
provided.</p>
<p>Both Python and C++ can be used to write gradient operators, but we focus our
examples on Python, as it is more commonly used.</p>
<div class="section" id="adding-a-gradient-in-python">
<h3>Adding a Gradient in Python<a class="headerlink" href="#adding-a-gradient-in-python" title="Permalink to this headline">¶</a></h3>
<p>A collection of Python gradient operators can be found in
<code class="docutils literal notranslate"><span class="pre">python/tvm/relay/op/_tensor_grad.py</span></code>. We will walk through two
representative examples: <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> and <code class="docutils literal notranslate"><span class="pre">multiply</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="nd">@register_gradient</span><span class="p">(</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">sigmoid_grad</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns [grad * sigmoid(x) * (1 - sigmoid(x))].&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">grad</span> <span class="o">*</span> <span class="n">orig</span> <span class="o">*</span> <span class="p">(</span><span class="n">ones_like</span><span class="p">(</span><span class="n">orig</span><span class="p">)</span> <span class="o">-</span> <span class="n">orig</span><span class="p">)]</span>
</pre></div>
</div>
<p>The inputs here are the original operator <code class="docutils literal notranslate"><span class="pre">orig</span></code> and a gradient <code class="docutils literal notranslate"><span class="pre">grad</span></code> to
accumulate into. What we return is a list, where the element at the i’th
index is the derivative of the operator with respect to the operator’s i’th
input. In general, the gradient will return a list with as many elements as
there are inputs to the base operator.</p>
<p>Before we further analyze this definition, first we should recall the
derivative of the sigmoid function: <span class="math notranslate nohighlight">\(\frac{\partial \sigma}{\partial x}
= \sigma(x)(1 - \sigma(x))\)</span>. The definition above looks similar to the
mathematical definition, but there is one important addition, which we
describe below.</p>
<p>The term <code class="docutils literal notranslate"><span class="pre">orig</span> <span class="pre">*</span> <span class="pre">(ones_like(orig)</span> <span class="pre">-</span> <span class="pre">orig)</span></code> directly matches the derivative,
because <code class="docutils literal notranslate"><span class="pre">orig</span></code> here is the sigmoid function, but we’re not just interested
in how to compute the gradient of this function. We’re interested in
composing this gradient with other gradients, so we can accumulate the
gradient across an entire program. This is where the <code class="docutils literal notranslate"><span class="pre">grad</span></code> term comes in.
In the expression <code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">*</span> <span class="pre">orig</span> <span class="pre">*</span> <span class="pre">(ones_like(orig)</span> <span class="pre">-</span> <span class="pre">orig)</span></code>, multiplying by
<code class="docutils literal notranslate"><span class="pre">grad</span></code> specifies how to compose the derivative with the gradient thus far.</p>
<p>Now, we consider <code class="docutils literal notranslate"><span class="pre">multiply</span></code>, a slightly more interesting example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="nd">@register_gradient</span><span class="p">(</span><span class="s2">&quot;multiply&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">multiply_grad</span><span class="p">(</span><span class="n">orig</span><span class="p">,</span> <span class="n">grad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns [grad * y, grad * x]&quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">orig</span><span class="o">.</span><span class="n">args</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">collapse_sum_like</span><span class="p">(</span><span class="n">grad</span> <span class="o">*</span> <span class="n">y</span><span class="p">,</span> <span class="n">x</span><span class="p">),</span>
            <span class="n">collapse_sum_like</span><span class="p">(</span><span class="n">grad</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)]</span>
</pre></div>
</div>
<p>In this example, there are two elements in the returned list, because
<code class="docutils literal notranslate"><span class="pre">multiply</span></code> is a binary operator. And to recall, if <span class="math notranslate nohighlight">\(f(x, y) = xy\)</span>, the
partial derivatives are <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial x} = y\)</span> and
<span class="math notranslate nohighlight">\(\frac{\partial f}{\partial y} = x\)</span>.</p>
<p>There is one required step for <code class="docutils literal notranslate"><span class="pre">multiply</span></code> that is not required for
<code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, because <code class="docutils literal notranslate"><span class="pre">multiply</span></code> has broadcasting semantics. Since the shape
of <code class="docutils literal notranslate"><span class="pre">grad</span></code> might not match the shape of the inputs, we use
<code class="docutils literal notranslate"><span class="pre">collapse_sum_like</span></code> to take the contents of the <code class="docutils literal notranslate"><span class="pre">grad</span> <span class="pre">*</span> <span class="pre">&lt;var&gt;</span></code> terms and
make the shape match the shape of the input we’re differentiating with
respect to.</p>
</div>
<div class="section" id="adding-a-gradient-in-c">
<h3>Adding a Gradient in C++<a class="headerlink" href="#adding-a-gradient-in-c" title="Permalink to this headline">¶</a></h3>
<p>Adding a gradient in C++ is similar to adding one in Python, but the
interface for registering is slightly different.</p>
<p>First, make sure <code class="docutils literal notranslate"><span class="pre">src/relay/pass/pattern_util.h</span></code> is included. It provides
helper functions for creating nodes in the Relay AST. Then, define the
gradient in a similar fashion as in the Python example:</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span class="n">tvm</span><span class="o">::</span><span class="n">Array</span><span class="o">&lt;</span><span class="n">Expr</span><span class="o">&gt;</span> <span class="n">MultiplyGrad</span><span class="p">(</span><span class="k">const</span> <span class="n">Expr</span><span class="o">&amp;</span> <span class="n">orig_call</span><span class="p">,</span> <span class="k">const</span> <span class="n">Expr</span><span class="o">&amp;</span> <span class="n">output_grad</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">const</span> <span class="n">Call</span><span class="o">&amp;</span> <span class="n">call</span> <span class="o">=</span> <span class="n">orig_call</span><span class="p">.</span><span class="n">Downcast</span><span class="o">&lt;</span><span class="n">Call</span><span class="o">&gt;</span><span class="p">();</span>
    <span class="k">return</span> <span class="p">{</span> <span class="n">CollapseSumLike</span><span class="p">(</span><span class="n">Multiply</span><span class="p">(</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">]),</span> <span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span>
             <span class="n">CollapseSumLike</span><span class="p">(</span><span class="n">Multiply</span><span class="p">(</span><span class="n">output_grad</span><span class="p">,</span> <span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">call</span><span class="p">.</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Notice that in C++ we can’t use the same operator overloading that we have in
Python, and we need to downcast, so the implementation is more verbose. Even
so, we can easily verify that this definition mirrors the earlier example in
Python.</p>
<p>Now, instead of using a Python decorator, we need to tack a <code class="docutils literal notranslate"><span class="pre">set_attr</span></code> call
for “FPrimalGradient” onto the end of the base operator’s registration, in
order to register the gradient.</p>
<div class="highlight-c notranslate"><div class="highlight"><pre><span class="n">RELAY_REGISTER_OP</span><span class="p">(</span><span class="s">&quot;multiply&quot;</span><span class="p">)</span>
    <span class="c1">// ...</span>
    <span class="c1">// Set other attributes</span>
    <span class="c1">// ...</span>
    <span class="p">.</span><span class="n">set_attr</span><span class="o">&lt;</span><span class="n">FPrimalGradient</span><span class="o">&gt;</span><span class="p">(</span><span class="s">&quot;FPrimalGradient&quot;</span><span class="p">,</span> <span class="n">MultiplyGrad</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>A TVM operator can be registered in Relay using a relation to express the appropriate type information.</p></li>
<li><p>Using an operator in Relay requires a function to produce a call node for the operator.</p></li>
<li><p>It is best to have a simple Python wrapper for producing the call node.</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="relay_add_pass.html" class="btn btn-neutral float-right" title="Adding a Compiler Pass to Relay" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="benchmark.html" class="btn btn-neutral float-left" title="Benchmark Performance Log Format" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Apache Software Foundation

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>