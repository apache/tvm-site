





<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Vitis-AI Integration &mdash; tvm 0.8.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/tlcpack_theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />

  
  
    <link rel="shortcut icon" href="../_static/tvm-logo-square.png"/>
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Developer How-To Guide" href="../dev/how_to.html" />
    <link rel="prev" title="Relay TensorRT Integration" href="tensorrt.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/vta>VTA</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/blog>Blog</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmconf.org>Conference</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/incubator-tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/security/>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/tvm-logo-small.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
              <div class="version">
                0.8.dev0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">How to</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../install/index.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribute/index.html">Contribute to TVM</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Deploy and Integration</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="cpp_deploy.html">Deploy TVM Module using C++ API</a></li>
<li class="toctree-l2"><a class="reference internal" href="android.html">Deploy to Android</a></li>
<li class="toctree-l2"><a class="reference internal" href="integrate.html">Integrate TVM into Your Project</a></li>
<li class="toctree-l2"><a class="reference internal" href="hls.html">HLS Backend Example</a></li>
<li class="toctree-l2"><a class="reference internal" href="arm_compute_lib.html">Relay Arm <sup>®</sup> Compute Library Integration</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorrt.html">Relay TensorRT Integration</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Vitis-AI Integration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#dpu-naming-information">DPU naming information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#build-instructions">Build instructions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#cloud-dpucadx8g">Cloud (DPUCADX8G)</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#getting-started">Getting started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#on-the-fly-quantization">On-the-fly quantization</a></li>
<li class="toctree-l4"><a class="reference internal" href="#config-settings">Config/Settings</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cloud-usage">Cloud usage</a></li>
<li class="toctree-l4"><a class="reference internal" href="#edge-usage">Edge usage</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../dev/how_to.html">Developer How-To Guide</a></li>
</ul>
<p class="caption"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Get Started Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#compile-deep-learning-models">Compile Deep Learning Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#tensor-expression-and-schedules">Tensor Expression and Schedules</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#optimize-tensor-operators">Optimize Tensor Operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#autotvm-template-based-auto-tuning">AutoTVM : Template-based Auto Tuning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#autoscheduler-template-free-auto-scheduling">AutoScheduler : Template-free Auto Scheduling</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#developer-tutorials">Developer Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#topi-tvm-operator-inventory">TOPI: TVM Operator Inventory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html#micro-tvm">Micro TVM</a></li>
</ul>
<p class="caption"><span class="caption-text">References</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../langref/index.html">Language Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/links.html">Links to Other API References</a></li>
</ul>
<p class="caption"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../dev/index.html">Design and Architecture</a></li>
</ul>
<p class="caption"><span class="caption-text">MISC</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../vta/index.html">VTA: Deep Learning Accelerator Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently Asked Questions</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of content
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="index.html">Deploy and Integration</a> <span class="br-arrow">></span></li>
        
      <li>Vitis-AI Integration</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/deploy/vitis_ai.rst.txt" rel="nofollow"> <img src="../_static//img/source.svg" alt="viewsource"/></a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="vitis-ai-integration">
<h1>Vitis-AI Integration<a class="headerlink" href="#vitis-ai-integration" title="Permalink to this headline">¶</a></h1>
<p><a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis-AI</a> is Xilinx’s
development stack for hardware-accelerated AI inference on Xilinx
platforms, including both edge devices and Alveo cards. It consists of
optimized IP, tools, libraries, models, and example designs. It is
designed with high efficiency and ease of use in mind, unleashing the
full potential of AI acceleration on Xilinx FPGA and ACAP.</p>
<p>The current Vitis-AI Byoc flow inside TVM enables acceleration of Neural
Network model inference on edge and cloud. The identifiers for the
supported edge and cloud Deep Learning Processor Units (DPU’s) are
DPUCZDX8G respectively DPUCADX8G. DPUCZDX8G and DPUCADX8G are hardware
accelerators for convolutional neural networks (CNN’s) on top of the
Xilinx <a class="reference external" href="https://www.xilinx.com/products/silicon-devices/soc/zynq-ultrascale-mpsoc.html">Zynq Ultrascale+
MPSoc</a>
respectively
<a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/alveo.html">Alveo</a>
(U200/U250) platforms. For more information about the DPU identifiers
see the section on <a class="reference external" href="#dpu-naming-information">DPU naming information</a>.</p>
<p>On this page you will find information on how to
<a class="reference external" href="#build-instructions">build</a> TVM with Vitis-AI and on how to <a class="reference external" href="#getting-started">get
started</a> with an example.</p>
<div class="section" id="dpu-naming-information">
<h2>DPU naming information<a class="headerlink" href="#dpu-naming-information" title="Permalink to this headline">¶</a></h2>
<table class="docutils align-default">
<colgroup>
<col style="width: 11%" />
<col style="width: 6%" />
<col style="width: 24%" />
<col style="width: 19%" />
<col style="width: 17%" />
<col style="width: 24%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>DPU</p></th>
<th class="head"><p>Application</p></th>
<th class="head"><p>HW Platform</p></th>
<th class="head"><p>Quantization Method</p></th>
<th class="head"><p>Quantization Bitwidth</p></th>
<th class="head"><p>Design Target</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Deep Learning Processing Unit</p></td>
<td><p>C: CNN R: RNN</p></td>
<td><p>AD: Alveo DDR AH: Alveo HBM VD: Versal DDR with AIE &amp; PL ZD: Zynq DDR</p></td>
<td><p>X: DECENT I: Integer threshold F: Float threshold R: RNN</p></td>
<td><p>4: 4-bit 8: 8-bit 16: 16-bit M: Mixed Precision</p></td>
<td><p>G: General purpose H: High throughput L: Low latency C: Cost optimized</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="build-instructions">
<h2>Build instructions<a class="headerlink" href="#build-instructions" title="Permalink to this headline">¶</a></h2>
<p>This section lists the instructions for building TVM with Vitis-AI for
both <a class="reference external" href="#cloud-dpucadx8g">cloud</a> and <a class="reference external" href="#edge-dpuczdx8g">edge</a>.</p>
<div class="section" id="cloud-dpucadx8g">
<h3>Cloud (DPUCADX8G)<a class="headerlink" href="#cloud-dpucadx8g" title="Permalink to this headline">¶</a></h3>
<p>For Vitis-AI acceleration in the cloud TVM has to be built on top of the
Xilinx Alveo platform.</p>
<div class="section" id="system-requirements">
<h4>System requirements<a class="headerlink" href="#system-requirements" title="Permalink to this headline">¶</a></h4>
<p>The following table lists system requirements for running docker
containers as well as Alveo cards.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 48%" />
<col style="width: 52%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Component</strong></p></th>
<th class="head"><p><strong>Requirement</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Motherboard</p></td>
<td><p>PCI Express 3.0-compliant with one dual-width x16 slot</p></td>
</tr>
<tr class="row-odd"><td><p>System Power Supply</p></td>
<td><p>225W</p></td>
</tr>
<tr class="row-even"><td><p>Operating System</p></td>
<td><p>Ubuntu 16.04, 18.04</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>CentOS 7.4, 7.5</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>RHEL 7.4, 7.5</p></td>
</tr>
<tr class="row-odd"><td><p>CPU</p></td>
<td><p>Intel i3/i5/i7/i9/Xeon 64-bit CPU</p></td>
</tr>
<tr class="row-even"><td><p>GPU (Optional to accelerate quantization)</p></td>
<td><p>NVIDIA GPU with a compute capability &gt; 3.0</p></td>
</tr>
<tr class="row-odd"><td><p>CUDA Driver (Optional to accelerate quantization)</p></td>
<td><p>nvidia-410</p></td>
</tr>
<tr class="row-even"><td><p>FPGA</p></td>
<td><p>Xilinx Alveo U200 or U250</p></td>
</tr>
<tr class="row-odd"><td><p>Docker Version</p></td>
<td><p>19.03.1</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="hardware-setup-and-docker-build">
<h4>Hardware setup and docker build<a class="headerlink" href="#hardware-setup-and-docker-build" title="Permalink to this headline">¶</a></h4>
<ol class="arabic">
<li><p>Clone the Vitis AI repository:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>git clone --recurse-submodules https://github.com/Xilinx/Vitis-AI
</pre></div>
</div>
</li>
<li><p>Install Docker, and add the user to the docker group. Link the user
to docker installation instructions from the following docker’s
website:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.docker.com/install/linux/docker-ce/ubuntu/">https://docs.docker.com/install/linux/docker-ce/ubuntu/</a></p></li>
<li><p><a class="reference external" href="https://docs.docker.com/install/linux/docker-ce/centos/">https://docs.docker.com/install/linux/docker-ce/centos/</a></p></li>
<li><p><a class="reference external" href="https://docs.docker.com/install/linux/linux-postinstall/">https://docs.docker.com/install/linux/linux-postinstall/</a></p></li>
</ul>
</li>
<li><p>Download the latest Vitis AI Docker with the following command. This container runs on CPU.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>docker pull xilinx/vitis-ai:latest
</pre></div>
</div>
<p>To accelerate the quantization, you can optionally use the Vitis-AI GPU docker image. Use the below commands to build the Vitis-AI GPU docker container:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> Vitis-AI/docker
./docker_build_gpu.sh
</pre></div>
</div>
</li>
<li><p>Set up Vitis AI to target Alveo cards. To target Alveo cards with
Vitis AI for machine learning workloads, you must install the
following software components:</p>
<ul class="simple">
<li><p>Xilinx Runtime (XRT)</p></li>
<li><p>Alveo Deployment Shells (DSAs)</p></li>
<li><p>Xilinx Resource Manager (XRM) (xbutler)</p></li>
<li><p>Xilinx Overlaybins (Accelerators to Dynamically Load - binary
programming files)</p></li>
</ul>
<p>While it is possible to install all of these software components
individually, a script has been provided to automatically install
them at once. To do so:</p>
<ul>
<li><p>Run the following commands:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> Vitis-AI/alveo/packages
sudo su
./install.sh
</pre></div>
</div>
</li>
<li><p>Power cycle the system.</p></li>
</ul>
</li>
<li><p>Clone tvm repo and pyxir repo</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>git clone --recursive https://github.com/apache/incubator-tvm.git
git clone --recursive https://github.com/Xilinx/pyxir.git
</pre></div>
</div>
</li>
<li><p>Build and start the tvm runtime Vitis-AI Docker Container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>./incubator-tvm/docker/build.sh demo_vitis_ai bash
./incubator-tvm/docker/bash.sh tvm.demo_vitis_ai

<span class="c1">#Setup inside container</span>
<span class="nb">source</span> /opt/xilinx/xrt/setup.sh
. $VAI_ROOT/conda/etc/profile.d/conda.sh
conda activate vitis-ai-tensorflow
</pre></div>
</div>
</li>
<li><p>Install PyXIR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> pyxir
python3 setup.py install --use_vai_rt_dpucadx8g --user
</pre></div>
</div>
</li>
<li><p>Build TVM inside the container with Vitis-AI</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> incubator-tvm
mkdir build
cp cmake/config.cmake build
<span class="nb">cd</span> build
<span class="nb">echo</span> set<span class="se">\(</span>USE_LLVM ON<span class="se">\)</span> &gt;&gt; config.cmake
<span class="nb">echo</span> set<span class="se">\(</span>USE_VITIS_AI ON<span class="se">\)</span> &gt;&gt; config.cmake
cmake ..
make -j<span class="k">$(</span>nproc<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Install TVM</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> incubator-tvm/python
pip3 install -e . --user
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="edge-dpuczdx8g">
<h4>Edge (DPUCZDX8G)<a class="headerlink" href="#edge-dpuczdx8g" title="Permalink to this headline">¶</a></h4>
<p>For edge deployment we make use of two systems referred to as host and
edge. The <a class="reference external" href="#host-requirements">host</a> system is responsible for
quantization and compilation of the neural network model in a first
offline step. Afterwards, the model will de deployed on the
<a class="reference external" href="#edge-requirements">edge</a> system.</p>
</div>
<div class="section" id="host-requirements">
<h4>Host requirements<a class="headerlink" href="#host-requirements" title="Permalink to this headline">¶</a></h4>
<p>The following table lists system requirements for running the TVM -
Vitis-AI docker container.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 54%" />
<col style="width: 46%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Component</strong></p></th>
<th class="head"><p><strong>Requirement</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Operating System</p></td>
<td><p>Ubuntu 16.04, 18.04</p></td>
</tr>
<tr class="row-odd"><td></td>
<td><p>CentOS 7.4, 7.5</p></td>
</tr>
<tr class="row-even"><td></td>
<td><p>RHEL 7.4, 7.5</p></td>
</tr>
<tr class="row-odd"><td><p>CPU</p></td>
<td><p>Intel i3/i5/i7/i9/Xeon 64-bit CPU</p></td>
</tr>
<tr class="row-even"><td><p>GPU (Optional to accelerate quantization)</p></td>
<td><p>NVIDIA GPU with a compute capability &gt; 3.0</p></td>
</tr>
<tr class="row-odd"><td><p>CUDA Driver (Optional to accelerate quantization)</p></td>
<td><p>nvidia-410</p></td>
</tr>
<tr class="row-even"><td><p>FPGA</p></td>
<td><p>Not necessary on host</p></td>
</tr>
<tr class="row-odd"><td><p>Docker Version</p></td>
<td><p>19.03.1</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="host-setup-and-docker-build">
<h4>Host setup and docker build<a class="headerlink" href="#host-setup-and-docker-build" title="Permalink to this headline">¶</a></h4>
<ol class="arabic">
<li><p>Clone tvm repo</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>git clone --recursive https://github.com/apache/incubator-tvm.git
</pre></div>
</div>
</li>
<li><p>Build and start the tvm runtime Vitis-AI Docker Container.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> incubator-tvm
./incubator-tvm/docker/build.sh demo_vitis_ai bash
./incubator-tvm/docker/bash.sh tvm.demo_vitis_ai

<span class="c1">#Setup inside container</span>
. $VAI_ROOT/conda/etc/profile.d/conda.sh
conda activate vitis-ai-tensorflow
</pre></div>
</div>
</li>
<li><p>Install PyXIR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>git clone --recursive https://github.com/Xilinx/pyxir.git
<span class="nb">cd</span> pyxir
python3 setup.py install --user
</pre></div>
</div>
</li>
<li><p>Build TVM inside the container with Vitis-AI.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> incubator-tvm
mkdir build
cp cmake/config.cmake build
<span class="nb">cd</span> build
<span class="nb">echo</span> set<span class="se">\(</span>USE_LLVM ON<span class="se">\)</span> &gt;&gt; config.cmake
<span class="nb">echo</span> set<span class="se">\(</span>USE_VITIS_AI ON<span class="se">\)</span> &gt;&gt; config.cmake
cmake ..
make -j<span class="k">$(</span>nproc<span class="k">)</span>
</pre></div>
</div>
</li>
<li><p>Install TVM</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> incubator-tvm/python
pip3 install -e . --user
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="edge-requirements">
<h4>Edge requirements<a class="headerlink" href="#edge-requirements" title="Permalink to this headline">¶</a></h4>
<p>The DPUCZDX8G can be deployed on the <a class="reference external" href="https://www.xilinx.com/products/silicon-devices/soc/zynq-ultrascale-mpsoc.html">Zynq Ultrascale+
MPSoc</a>
platform. The following development boards can be used out-of-the-box:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 18%" />
<col style="width: 19%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Target board</strong></p></th>
<th class="head"><p><strong>TVM identifier</strong></p></th>
<th class="head"><p><strong>Info</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Ultra96</p></td>
<td><p>DPUCZDX8G-ultra96</p></td>
<td><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/1-vad4rl.html">https://www.xilinx.com/products/boards-and-kits/1-vad4rl.html</a></p></td>
</tr>
<tr class="row-odd"><td><p>ZCU104</p></td>
<td><p>DPUCZDX8G-zcu104</p></td>
<td><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/zcu104.html">https://www.xilinx.com/products/boards-and-kits/zcu104.html</a></p></td>
</tr>
<tr class="row-even"><td><p>ZCU102</p></td>
<td><p>DPUCZDX8G-zcu102</p></td>
<td><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html">https://www.xilinx.com/products/boards-and-kits/ek-u1-zcu102-g.html</a></p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="edge-hardware-setup">
<h4>Edge hardware setup<a class="headerlink" href="#edge-hardware-setup" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This section provides instructions for setting up with the <a class="reference external" href="http://www.pynq.io/">Pynq</a> platform but
Petalinux based flows are also supported.</p>
</div>
<ol class="arabic">
<li><p>Download the Pynq v2.5 image for your target (use Z1 or Z2 for
Ultra96 target depending on board version) Link to image:
<a class="reference external" href="https://github.com/Xilinx/PYNQ/releases/tag/v2.5">https://github.com/Xilinx/PYNQ/releases/tag/v2.5</a></p></li>
<li><p>Follow Pynq instructions for setting up the board: <a class="reference external" href="https://pynq.readthedocs.io/en/latest/getting_started.html">pynq
setup</a></p></li>
<li><p>After connecting to the board, make sure to run as root. Execute
<code class="docutils literal notranslate"><span class="pre">su</span></code></p></li>
<li><p>Set up DPU on Pynq by following the steps here: <a class="reference external" href="https://github.com/Xilinx/DPU-PYNQ">DPU Pynq
setup</a></p></li>
<li><p>Run the following command to download the DPU bitstream:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>python3 -c <span class="s1">&#39;from pynq_dpu import DpuOverlay ; overlay = DpuOverlay(&quot;dpu.bit&quot;)&#39;</span>
</pre></div>
</div>
</li>
<li><p>Check whether the DPU kernel is alive:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>dexplorer -w
</pre></div>
</div>
</li>
</ol>
</div>
<div class="section" id="edge-tvm-setup">
<h4>Edge TVM setup<a class="headerlink" href="#edge-tvm-setup" title="Permalink to this headline">¶</a></h4>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When working on Petalinux instead of Pynq, the following steps might take more manual work (e.g building
hdf5 from source). Also, TVM has a scipy dependency which you then might have to build from source or
circumvent. We don’t depend on scipy in our flow.</p>
</div>
<p>Building TVM depends on the Xilinx
<a class="reference external" href="https://github.com/Xilinx/pyxir">PyXIR</a> package. PyXIR acts as an
interface between TVM and Vitis-AI tools.</p>
<ol class="arabic">
<li><p>First install the PyXIR h5py and pydot dependencies:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>apt-get install libhdf5-dev
pip3 install pydot h5py
</pre></div>
</div>
</li>
<li><p>Install PyXIR</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>git clone --recursive https://github.com/Xilinx/pyxir.git
<span class="nb">cd</span> pyxir
sudo python3 setup.py install --use_vai_rt_dpuczdx8g
</pre></div>
</div>
</li>
<li><p>Build TVM with Vitis-AI</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>git clone --recursive https://github.com/apache/incubator-tvm
<span class="nb">cd</span> incubator-tvm
mkdir build
cp cmake/config.cmake build
<span class="nb">cd</span> build
<span class="nb">echo</span> set<span class="se">\(</span>USE_VITIS_AI ON<span class="se">\)</span> &gt;&gt; config.cmake
cmake ..
make
</pre></div>
</div>
</li>
<li><p>Install TVM</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span class="nb">cd</span> incubator-tvm/python
pip3 install -e . --user
</pre></div>
</div>
</li>
<li><p>Check whether the setup was successful in the Python shell:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre>python3 -c <span class="s1">&#39;import pyxir; import tvm&#39;</span>
</pre></div>
</div>
</li>
</ol>
</div>
</div>
</div>
<div class="section" id="getting-started">
<h2>Getting started<a class="headerlink" href="#getting-started" title="Permalink to this headline">¶</a></h2>
<p>This section shows how to use TVM with Vitis-AI. For this it’s important
to understand that neural network models are quantized for Vitis-AI
execution in fixed point arithmetic. The approach we take here is to
quantize on-the-fly using the first N inputs as explained in the next
section.</p>
<div class="section" id="on-the-fly-quantization">
<h3>On-the-fly quantization<a class="headerlink" href="#on-the-fly-quantization" title="Permalink to this headline">¶</a></h3>
<p>Usually, to be able to accelerate inference of Neural Network models
with Vitis-AI DPU accelerators, those models need to quantized upfront.
In TVM - Vitis-AI flow, we make use of on-the-fly quantization to remove
this additional preprocessing step. In this flow, one doesn’t need to
quantize his/her model upfront but can make use of the typical inference
execution calls (module.run) to quantize the model on-the-fly using the
first N inputs that are provided (see more information below). This will
set up and calibrate the Vitis-AI DPU and from that point onwards
inference will be accelerated for all next inputs. Note that the edge
flow deviates slightly from the explained flow in that inference won’t
be accelerated after the first N inputs but the model will have been
quantized and compiled and can be moved to the edge device for
deployment. Please check out the <a class="reference external" href="#Edge%20usage">edge</a> usage
instructions below for more information.</p>
</div>
<div class="section" id="config-settings">
<h3>Config/Settings<a class="headerlink" href="#config-settings" title="Permalink to this headline">¶</a></h3>
<p>A couple of environment variables can be used to customize the Vitis-AI
Byoc flow.</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 7%" />
<col style="width: 10%" />
<col style="width: 83%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Environment Variable</strong></p></th>
<th class="head"><p><strong>Default if unset</strong></p></th>
<th class="head"><p><strong>Explanation</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>PX_QUANT_SIZE</p></td>
<td><p>128</p></td>
<td><p>The number of inputs that will be used for quantization (necessary for Vitis-AI acceleration)</p></td>
</tr>
<tr class="row-odd"><td><p>PX_BUILD_DIR</p></td>
<td><p>Use the on-the-fly quantization flow</p></td>
<td><p>Loads the quantization and compilation information from the provided build directory and immediately starts Vitis-AI hardware acceleration. This configuration can be used if the model has been executed before using on-the-fly quantization during which the quantization and comilation information was cached in a build directory.</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="cloud-usage">
<h3>Cloud usage<a class="headerlink" href="#cloud-usage" title="Permalink to this headline">¶</a></h3>
<p>This section shows how to accelerate a convolutional neural network
model in TVM with Vitis-AI on the cloud.</p>
<p>To be able to target the Vitis-AI cloud DPUCADX8G target we first have
to import the target in PyXIR. This PyXIR package is the interface being
used by TVM to integrate with the Vitis-AI stack. Additionaly, import
the typical TVM and Relay modules and the Vitis-AI contrib module inside
TVM.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">pyxir</span>
<span class="kn">import</span> <span class="nn">pyxir.contrib.target.DPUCADX8G</span>

<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">tvm.relay</span> <span class="kn">as</span> <span class="nn">relay</span>
<span class="kn">from</span> <span class="nn">tvm.contrib.target</span> <span class="kn">import</span> <span class="n">vitis_ai</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">util</span><span class="p">,</span> <span class="n">graph_runtime</span>
<span class="kn">from</span> <span class="nn">tvm.relay.build_module</span> <span class="kn">import</span> <span class="n">bind_params_by_name</span>
<span class="kn">from</span> <span class="nn">tvm.relay.op.contrib.vitis_ai</span> <span class="kn">import</span> <span class="n">annotation</span>
</pre></div>
</div>
<p>After importing a convolutional neural network model using the usual
Relay API’s, annotate the Relay expression for the given Vitis-AI DPU
target and partition the graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bind_params_by_name</span><span class="p">(</span><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">],</span> <span class="n">params</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">annotation</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">MergeCompilerRegions</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PartitionGraph</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can build the TVM runtime library for executing the model. The
TVM target is ‘llvm’ as the operations that can’t be handled by the DPU
are executed on the CPU. The Vitis-AI target is DPUCADX8G as we are
targeting the cloud DPU and this target is passed as a config to the TVM
build call.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">tvm_target</span> <span class="o">=</span> <span class="s1">&#39;llvm&#39;</span>
<span class="n">target</span><span class="o">=</span><span class="s1">&#39;DPUCADX8G&#39;</span>

<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span> <span class="p">{</span><span class="s1">&#39;relay.ext.vitis_ai.options.target&#39;</span><span class="p">:</span> <span class="n">target</span><span class="p">}):</span>
   <span class="n">lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">tvm_target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<p>As one more step before we can accelerate a model with Vitis-AI in TVM
we have to quantize and compile the model for execution on the DPU. We
make use of on-the-fly quantization for this. Using this method one
doesn’t need to quantize their model upfront and can make use of the
typical inference execution calls (module.run) to calibrate the model
on-the-fly using the first N inputs that are provided. After the first N
iterations, computations will be accelerated on the DPU. So now we will
feed N inputs to the TVM runtime module. Note that these first N inputs
will take a substantial amount of time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">module</span> <span class="o">=</span> <span class="n">graph_runtime</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>

<span class="c1"># First N (default = 128) inputs are used for quantization calibration and will</span>
<span class="c1"># be executed on the CPU</span>
<span class="c1"># This config can be changed by setting the &#39;PX_QUANT_SIZE&#39; (e.g. export PX_QUANT_SIZE=64)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
   <span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
   <span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>Afterwards, inference will be accelerated on the DPU.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>To save and load the built module, one can use the typical TVM API’s:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">lib_path</span> <span class="o">=</span> <span class="s2">&quot;deploy_lib.so&quot;</span>
<span class="n">lib</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>
</pre></div>
</div>
<p>Load the module from compiled files and run inference</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="c1"># load the module into memory</span>
<span class="n">loaded_lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="n">lib_path</span><span class="p">)</span>

<span class="n">module</span> <span class="o">=</span> <span class="n">graph_runtime</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>
<span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="edge-usage">
<h3>Edge usage<a class="headerlink" href="#edge-usage" title="Permalink to this headline">¶</a></h3>
<p>This section shows how to accelerate a convolutional neural network
model in TVM with Vitis-AI at the edge. The first couple of steps will
have to be run on the host machine and take care of quantization and
compilation for deployment at the edge.</p>
<div class="section" id="host-steps">
<h4>Host steps<a class="headerlink" href="#host-steps" title="Permalink to this headline">¶</a></h4>
<p>To be able to target the Vitis-AI cloud DPUCZDX8G target we first have
to import the target in PyXIR. This PyXIR package is the interface being
used by TVM to integrate with the Vitis-AI stack. Additionaly, import
the typical TVM and Relay modules and the Vitis-AI contrib module inside
TVM.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="kn">import</span> <span class="nn">pyxir</span>
<span class="kn">import</span> <span class="nn">pyxir.contrib.target.DPUCZDX8G</span>

<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">import</span> <span class="nn">tvm.relay</span> <span class="kn">as</span> <span class="nn">relay</span>
<span class="kn">from</span> <span class="nn">tvm.contrib.target</span> <span class="kn">import</span> <span class="n">vitis_ai</span>
<span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">util</span><span class="p">,</span> <span class="n">graph_runtime</span>
<span class="kn">from</span> <span class="nn">tvm.relay.build_module</span> <span class="kn">import</span> <span class="n">bind_params_by_name</span>
<span class="kn">from</span> <span class="nn">tvm.relay.op.contrib.vitis_ai</span> <span class="kn">import</span> <span class="n">annotation</span>
</pre></div>
</div>
<p>After importing a convolutional neural network model using the usual
Relay API’s, annotate the Relay expression for the given Vitis-AI DPU
target and partition the graph.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">bind_params_by_name</span><span class="p">(</span><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">],</span> <span class="n">params</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">annotation</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">MergeCompilerRegions</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PartitionGraph</span><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, we can build the TVM runtime library for executing the model. The
TVM target is ‘llvm’ as the operations that can’t be handled by the DPU
are executed on the CPU. At this point that means the CPU on the host machine.
The Vitis-AI target is DPUCZDX8G-zcu104 as we are targeting the edge DPU
on the ZCU104 board and this target is passed as a config to the TVM
build call. Note that different identifiers can be passed for different
targets, see <a class="reference external" href="#edge-requirements">edge targets info</a>. Additionally, we
provide the ‘export_runtime_module’ config that points to a file to which we
can export the Vitis-AI runtime module. We have to do this because we will
first be compiling and quantizing the model on the host machine before building
the model for edge deployment. As you will see later on, the exported runtime
module will be passed to the edge build so that the Vitis-AI runtime module
can be included.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">util</span>

<span class="n">temp</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">tempdir</span><span class="p">()</span>

<span class="n">tvm_target</span> <span class="o">=</span> <span class="s1">&#39;llvm&#39;</span>
<span class="n">target</span><span class="o">=</span><span class="s1">&#39;DPUCZDX8G-zcu104&#39;</span>
<span class="n">export_rt_mod_file</span> <span class="o">=</span> <span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;vitis_ai.rtmod&quot;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span> <span class="p">{</span><span class="s1">&#39;relay.ext.vitis_ai.options.target&#39;</span><span class="p">:</span> <span class="n">target</span><span class="p">,</span>
                                                     <span class="s1">&#39;relay.ext.vitis_ai.options.export_runtime_module&#39;</span><span class="p">:</span> <span class="n">export_rt_mod_file</span><span class="p">}):</span>
   <span class="n">lib</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">tvm_target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<p>We will quantize and compile the model for execution on the DPU using on-the-fly
quantization on the host machine. This makes use of TVM inference calls
(module.run) to quantize the model on the host with the first N inputs.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">module</span> <span class="o">=</span> <span class="n">graph_runtime</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>

<span class="c1"># First N (default = 128) inputs are used for quantization calibration and will</span>
<span class="c1"># be executed on the CPU</span>
<span class="c1"># This config can be changed by setting the &#39;PX_QUANT_SIZE&#39; (e.g. export PX_QUANT_SIZE=64)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">128</span><span class="p">):</span>
   <span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
   <span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
<p>Save the TVM lib module so that the Vitis-AI runtime module will also be exported
(to the ‘export_runtime_module’ path we previously passed as a config).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="kn">from</span> <span class="nn">tvm.contrib</span> <span class="kn">import</span> <span class="n">util</span>

<span class="n">temp</span> <span class="o">=</span> <span class="n">util</span><span class="o">.</span><span class="n">tempdir</span><span class="p">()</span>
<span class="n">lib</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="n">temp</span><span class="o">.</span><span class="n">relpath</span><span class="p">(</span><span class="s2">&quot;tvm_lib.so&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p>After quantizing and compiling the model for Vitis-AI acceleration using the
first N inputs we can build the model for execution on the ARM edge device.
Here we pass the previously exported Vitis-AI runtime module so it can be included
in the TVM build.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="c1"># Export lib for aarch64 target</span>
<span class="n">tvm_target</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">arm_cpu</span><span class="p">(</span><span class="s1">&#39;ultra96&#39;</span><span class="p">)</span>
<span class="n">lib_kwargs</span> <span class="o">=</span> <span class="p">{</span>
     <span class="s1">&#39;fcompile&#39;</span><span class="p">:</span> <span class="n">contrib</span><span class="o">.</span><span class="n">cc</span><span class="o">.</span><span class="n">create_shared</span><span class="p">,</span>
     <span class="s1">&#39;cc&#39;</span><span class="p">:</span> <span class="s2">&quot;/usr/aarch64-linux-gnu/bin/ld&quot;</span>
<span class="p">}</span>

<span class="k">with</span> <span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                               <span class="n">config</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;relay.ext.vitis_ai.options.load_runtime_module&#39;</span><span class="p">:</span> <span class="n">export_rt_mod_file</span><span class="p">}):</span>
     <span class="n">lib_arm</span> <span class="o">=</span> <span class="n">relay</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">tvm_target</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

<span class="n">lib_dpuv2</span><span class="o">.</span><span class="n">export_library</span><span class="p">(</span><span class="s1">&#39;tvm_dpu_arm.so&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">lib_kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, move the TVM build files (tvm_dpu_arm.json, tvm_dpu_arm.so,
tvm_dpu_arm.params) to the edge device. For information on setting
up the edge device check out the <a class="reference external" href="#edge-dpuczdx8g">edge setup</a>
section.</p>
</div>
<div class="section" id="edge-steps">
<h4>Edge steps<a class="headerlink" href="#edge-steps" title="Permalink to this headline">¶</a></h4>
<p>After setting up TVM with Vitis-AI on the edge device, you can now load
the TVM runtime module into memory and feed inputs for inference.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span class="n">ctx</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

<span class="c1"># load the module into memory</span>
<span class="n">lib</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">load_module</span><span class="p">(</span><span class="s2">&quot;tvm_dpu_arm.so&quot;</span><span class="p">)</span>

<span class="n">module</span> <span class="o">=</span> <span class="n">graph_runtime</span><span class="o">.</span><span class="n">GraphModule</span><span class="p">(</span><span class="n">lib</span><span class="p">[</span><span class="s2">&quot;default&quot;</span><span class="p">](</span><span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()))</span>
<span class="n">module</span><span class="o">.</span><span class="n">set_input</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
<span class="n">module</span><span class="o">.</span><span class="n">run</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../dev/how_to.html" class="btn btn-neutral float-right" title="Developer How-To Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="tensorrt.html" class="btn btn-neutral float-left" title="Relay TensorRT Integration" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../_static//img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <ul class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <li class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2020 Apache Software Foundation | All right reserved</h5>
        </li>
      </ul>

    </div>

    <ul>
      <li class="footernote">Apache TVM is an effort undergoing incubation at The Apache Software Foundation (ASF), sponsored by the Apache Incubator. Incubation is required of all newly accepted projects until a further review indicates that the infrastructure, communications, and decision making process have stabilized in a manner consistent with other successful ASF projects. While incubation status is not necessarily a reflection of the completeness or stability of the code, it does indicate that the project has yet to be fully endorsed by the ASF. Copyright © 2020 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</li>
    </ul>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>