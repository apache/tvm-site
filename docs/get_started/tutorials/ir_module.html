



<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>IRModule &mdash; tvm 0.20.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/downloads/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../_static/documentation_options.js?v=1b5e2a23"></script>
        <script src="../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="End-to-End Optimize Model" href="../../how_to/tutorials/e2e_opt_model.html" />
    <link rel="prev" title="Quick Start" href="quick_start.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/blog>Blog</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvmconf.org>Conference</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="sidetitle" alt="Documentation Home"> tvm
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick_start.html">Quick Start</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">IRModule</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#create-irmodule">Create IRModule</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#import-from-existing-models">Import from existing models</a></li>
<li class="toctree-l3"><a class="reference internal" href="#write-with-relax-nn-module">Write with Relax NN Module</a></li>
<li class="toctree-l3"><a class="reference internal" href="#create-via-tvmscript">Create via TVMScript</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#attributes-of-an-irmodule">Attributes of an IRModule</a></li>
<li class="toctree-l2"><a class="reference internal" href="#transformations-on-irmodules">Transformations on IRModules</a></li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-the-irmodule-universally">Deploy the IRModule Universally</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#deploy-on-cpu">Deploy on CPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploy-on-gpu">Deploy on GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="#deploy-on-other-backends">Deploy on Other Backends</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How To</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../how_to/tutorials/e2e_opt_model.html">End-to-End Optimize Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how_to/tutorials/customize_opt.html">Customize Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how_to/tutorials/optimize_llm.html">Optimize Large Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how_to/tutorials/cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../how_to/dev/index.html">Development Guides</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../arch/index.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/tensor_ir/index.html">TensorIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../deep_dive/relax/index.html">Relax</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/api/links.html">Other APIs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../contribute/index.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../reference/security.html">Security Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
      <li>IRModule</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/tvm/edit/main/docs/get_started/tutorials/ir_module.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial can be used interactively with Google Colab! You can also click
<a class="reference internal" href="#sphx-glr-download-get-started-tutorials-ir-module-py"><span class="std std-ref">here</span></a> to run the Jupyter notebook locally.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/apache/tvm-site/blob/asf-site/docs/_downloads/a6d7947451d373bc811080cffa18dc7c/ir_module.ipynb"><img alt="../../_static/downloads/colab_button.svg" class="align-center" src="../../_static/downloads/colab_button.svg" style="width: 300px;" />
</a>
</div>
<section class="sphx-glr-example-title" id="irmodule">
<span id="ir-module"></span><span id="sphx-glr-get-started-tutorials-ir-module-py"></span><h1>IRModule<a class="headerlink" href="#irmodule" title="Link to this heading"></a></h1>
<p>This tutorial presents the core abstraction of Apache TVM Unity, the IRModule.
The IRModule encompasses the <strong>entirety</strong> of the ML models, incorporating the
computational graph, tensor programs, and potential calls to external libraries.</p>
<nav class="contents local" id="table-of-contents">
<p class="topic-title">Table of Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#create-irmodule" id="id1">Create IRModule</a></p></li>
<li><p><a class="reference internal" href="#attributes-of-an-irmodule" id="id2">Attributes of an IRModule</a></p></li>
<li><p><a class="reference internal" href="#transformations-on-irmodules" id="id3">Transformations on IRModules</a></p></li>
<li><p><a class="reference internal" href="#deploy-the-irmodule-universally" id="id4">Deploy the IRModule Universally</a></p></li>
</ul>
</nav>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">tvm</span>
<span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">relax</span>
</pre></div>
</div>
<section id="create-irmodule">
<h2><a class="toc-backref" href="#id1" role="doc-backlink">Create IRModule</a><a class="headerlink" href="#create-irmodule" title="Link to this heading"></a></h2>
<p>IRModules can be initialized in various ways. We demonstrate a few of them
below.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.export</span> <span class="kn">import</span> <span class="n">export</span>
<span class="kn">from</span> <span class="nn">tvm.relax.frontend.torch</span> <span class="kn">import</span> <a href="../../reference/api/python/relax/frontend.html#tvm.relax.frontend.torch.from_exported_program" title="tvm.relax.frontend.torch.from_exported_program" class="sphx-glr-backref-module-tvm-relax-frontend-torch sphx-glr-backref-type-py-function"><span class="n">from_exported_program</span></a>
</pre></div>
</div>
<section id="import-from-existing-models">
<h3>Import from existing models<a class="headerlink" href="#import-from-existing-models" title="Link to this heading"></a></h3>
<p>The most common way to initialize an IRModule is to import from an existing
model. Apache TVM Unity accommodates imports from a range of frameworks,
such as PyTorch and ONNX. This tutorial solely demonstrates the import process
from PyTorch.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a dummy model</span>
<span class="k">class</span> <span class="nc">TorchModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TorchModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="c1"># Give an example argument to torch.export</span>
<a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">example_args</span></a> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">),)</span>

<span class="c1"># Convert the model to IRModule</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">exported_program</span> <span class="o">=</span> <span class="n">export</span><span class="p">(</span><span class="n">TorchModel</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">(),</span> <a href="https://docs.python.org/3/library/stdtypes.html#tuple" title="builtins.tuple" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">example_args</span></a><span class="p">)</span>
    <span class="n">mod_from_torch</span> <span class="o">=</span> <a href="../../reference/api/python/relax/frontend.html#tvm.relax.frontend.torch.from_exported_program" title="tvm.relax.frontend.torch.from_exported_program" class="sphx-glr-backref-module-tvm-relax-frontend-torch sphx-glr-backref-type-py-function"><span class="n">from_exported_program</span></a><span class="p">(</span>
        <span class="n">exported_program</span><span class="p">,</span> <span class="n">keep_params_as_input</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">unwrap_unit_return_tuple</span><span class="o">=</span><span class="kc">True</span>
    <span class="p">)</span>

<span class="n">mod_from_torch</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">params_from_torch</span></a> <span class="o">=</span> <a href="../../reference/api/python/relax/frontend.html#tvm.relax.frontend.detach_params" title="tvm.relax.frontend.detach_params" class="sphx-glr-backref-module-tvm-relax-frontend sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">frontend</span><span class="o">.</span><span class="n">detach_params</span></a><span class="p">(</span><span class="n">mod_from_torch</span><span class="p">)</span>
<span class="c1"># Print the IRModule</span>
<span class="n">mod_from_torch</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import relax as R

@I.ir_module
class Module:
    @R.function
    def main(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), p_fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), p_fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), p_fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;), p_fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        with R.dataflow():
            lv: R.Tensor((784, 256), dtype=&quot;float32&quot;) = R.permute_dims(p_fc1_weight, axes=None)
            lv1: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.matmul(x, lv, out_dtype=&quot;float32&quot;)
            lv2: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.add(lv1, p_fc1_bias)
            lv3: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.nn.relu(lv2)
            lv4: R.Tensor((256, 10), dtype=&quot;float32&quot;) = R.permute_dims(p_fc2_weight, axes=None)
            lv5: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.matmul(lv3, lv4, out_dtype=&quot;float32&quot;)
            lv6: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.add(lv5, p_fc2_bias)
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = lv6
            R.output(gv)
        return gv
</pre></div>
</div>
</section>
<section id="write-with-relax-nn-module">
<h3>Write with Relax NN Module<a class="headerlink" href="#write-with-relax-nn-module" title="Link to this heading"></a></h3>
<p>Apache TVM Unity also provides a set of PyTorch-liked APIs, to help users
write the IRModule directly.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm.relax.frontend</span> <span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span> <span class="nc">RelaxModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RelaxModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">mod_from_relax</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">params_from_relax</span></a> <span class="o">=</span> <span class="n">RelaxModel</span><span class="p">()</span><span class="o">.</span><span class="n">export_tvm</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&quot;float32&quot;</span><span class="p">)}}</span>
<span class="p">)</span>
<span class="n">mod_from_relax</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import relax as R

@I.ir_module
class Module:
    @R.function
    def forward(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;), fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        with R.dataflow():
            permute_dims: R.Tensor((784, 256), dtype=&quot;float32&quot;) = R.permute_dims(fc1_weight, axes=None)
            matmul: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.matmul(x, permute_dims, out_dtype=&quot;void&quot;)
            add: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.add(matmul, fc1_bias)
            relu: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.nn.relu(add)
            permute_dims1: R.Tensor((256, 10), dtype=&quot;float32&quot;) = R.permute_dims(fc2_weight, axes=None)
            matmul1: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.matmul(relu, permute_dims1, out_dtype=&quot;void&quot;)
            add1: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.add(matmul1, fc2_bias)
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = add1
            R.output(gv)
        return gv
</pre></div>
</div>
</section>
<section id="create-via-tvmscript">
<h3>Create via TVMScript<a class="headerlink" href="#create-via-tvmscript" title="Link to this heading"></a></h3>
<p>TVMScript is a Python-based DSL for IRModules. We are able to
directly output the IRModule in the TVMScript syntax, or alternatively,
parse the TVMScript to obtain an IRModule.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">ir</span> <span class="k">as</span> <span class="n">I</span>
<span class="kn">from</span> <span class="nn">tvm.script</span> <span class="kn">import</span> <span class="n">relax</span> <span class="k">as</span> <span class="n">R</span>


<span class="nd">@I</span><span class="o">.</span><span class="n">ir_module</span>
<span class="k">class</span> <span class="nc">TVMScriptModule</span><span class="p">:</span>
    <span class="nd">@R</span><span class="o">.</span><span class="n">function</span>
    <span class="k">def</span> <span class="nf">main</span><span class="p">(</span>
        <span class="n">x</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span>
        <span class="n">fc1_weight</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span>
        <span class="n">fc1_bias</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">256</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span>
        <span class="n">fc2_weight</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span>
        <span class="n">fc2_bias</span><span class="p">:</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">),</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">R</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">):</span>
        <span class="n">R</span><span class="o">.</span><span class="n">func_attr</span><span class="p">({</span><span class="s2">&quot;num_input&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">})</span>
        <span class="k">with</span> <span class="n">R</span><span class="o">.</span><span class="n">dataflow</span><span class="p">():</span>
            <span class="n">permute_dims</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">fc1_weight</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">matmul</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">permute_dims</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&quot;void&quot;</span><span class="p">)</span>
            <span class="n">add</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">matmul</span><span class="p">,</span> <span class="n">fc1_bias</span><span class="p">)</span>
            <span class="n">relu</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">add</span><span class="p">)</span>
            <span class="n">permute_dims1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">permute_dims</span><span class="p">(</span><span class="n">fc2_weight</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
            <span class="n">matmul1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">relu</span><span class="p">,</span> <span class="n">permute_dims1</span><span class="p">,</span> <span class="n">out_dtype</span><span class="o">=</span><span class="s2">&quot;void&quot;</span><span class="p">)</span>
            <span class="n">add1</span> <span class="o">=</span> <span class="n">R</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">matmul1</span><span class="p">,</span> <span class="n">fc2_bias</span><span class="p">)</span>
            <a href="../../reference/api/python/ir.html#tvm.ir.GlobalVar" title="tvm.ir.GlobalVar" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gv</span></a> <span class="o">=</span> <span class="n">add1</span>
            <span class="n">R</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><a href="../../reference/api/python/ir.html#tvm.ir.GlobalVar" title="tvm.ir.GlobalVar" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gv</span></a><span class="p">)</span>
        <span class="k">return</span> <a href="../../reference/api/python/ir.html#tvm.ir.GlobalVar" title="tvm.ir.GlobalVar" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gv</span></a>


<span class="n">mod_from_script</span> <span class="o">=</span> <span class="n">TVMScriptModule</span>
<span class="n">mod_from_script</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import relax as R

@I.ir_module
class Module:
    @R.function
    def main(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;), fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        with R.dataflow():
            permute_dims: R.Tensor((784, 256), dtype=&quot;float32&quot;) = R.permute_dims(fc1_weight, axes=None)
            matmul: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.matmul(x, permute_dims, out_dtype=&quot;void&quot;)
            add: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.add(matmul, fc1_bias)
            relu: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.nn.relu(add)
            permute_dims1: R.Tensor((256, 10), dtype=&quot;float32&quot;) = R.permute_dims(fc2_weight, axes=None)
            matmul1: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.matmul(relu, permute_dims1, out_dtype=&quot;void&quot;)
            add1: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.add(matmul1, fc2_bias)
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = add1
            R.output(gv)
        return gv
</pre></div>
</div>
</section>
</section>
<section id="attributes-of-an-irmodule">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">Attributes of an IRModule</a><a class="headerlink" href="#attributes-of-an-irmodule" title="Link to this heading"></a></h2>
<p>An IRModule is a collection of functions, indexed by GlobalVars.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <span class="n">mod_from_torch</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">get_global_vars</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[I.GlobalVar(&quot;main&quot;)]
</pre></div>
</div>
<p>We can access the functions in the IRModule by indexing with the GlobalVars
or their names</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="c1"># index by global var name</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">])</span>
<span class="c1"># index by global var, and checking they are the same function</span>
<span class="p">(</span><a href="../../reference/api/python/ir.html#tvm.ir.GlobalVar" title="tvm.ir.GlobalVar" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gv</span></a><span class="p">,)</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">get_global_vars</span><span class="p">()</span>
<span class="k">assert</span> <span class="n">mod</span><span class="p">[</span><a href="../../reference/api/python/ir.html#tvm.ir.GlobalVar" title="tvm.ir.GlobalVar" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gv</span></a><span class="p">]</span> <span class="o">==</span> <span class="n">mod</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">]</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import relax as R

@R.function
def main(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), p_fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), p_fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), p_fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;), p_fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
    R.func_attr({&quot;num_input&quot;: 1})
    with R.dataflow():
        lv: R.Tensor((784, 256), dtype=&quot;float32&quot;) = R.permute_dims(p_fc1_weight, axes=None)
        lv1: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.matmul(x, lv, out_dtype=&quot;float32&quot;)
        lv2: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.add(lv1, p_fc1_bias)
        lv3: R.Tensor((1, 256), dtype=&quot;float32&quot;) = R.nn.relu(lv2)
        lv4: R.Tensor((256, 10), dtype=&quot;float32&quot;) = R.permute_dims(p_fc2_weight, axes=None)
        lv5: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.matmul(lv3, lv4, out_dtype=&quot;float32&quot;)
        lv6: R.Tensor((1, 10), dtype=&quot;float32&quot;) = R.add(lv5, p_fc2_bias)
        gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = lv6
        R.output(gv)
    return gv
</pre></div>
</div>
</section>
<section id="transformations-on-irmodules">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">Transformations on IRModules</a><a class="headerlink" href="#transformations-on-irmodules" title="Link to this heading"></a></h2>
<p>Transformations are the import component of Apache TVM Unity. One transformation
takes in an IRModule and outputs another IRModule. We can apply a sequence of
transformations to an IRModule to obtain a new IRModule. That is the common way to
optimize a model.</p>
<p>In this getting started tutorial, we only demonstrate how to apply transformations
to an IRModule. For details of each transformation, please refer to the
<a class="reference internal" href="../../reference/api/python/relax/transform.html#api-relax-transformation"><span class="std std-ref">Transformation API Reference</span></a></p>
<p>We first apply <strong>LegalizeOps</strong> transformation to the IRModule. This transformation
will convert the Relax module into a mixed stage, with both Relax and TensorIR function
within the same module. Meanwhile, the Relax operators will be converted into <code class="docutils literal notranslate"><span class="pre">call_tir</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <span class="n">mod_from_torch</span>
<span class="n">mod</span> <span class="o">=</span> <a href="../../reference/api/python/relax/transform.html#tvm.relax.transform.LegalizeOps" title="tvm.relax.transform.LegalizeOps" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">LegalizeOps</span></a><span class="p">()(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def add(lv1: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;), p_fc1_bias: T.Buffer((T.int64(256),), &quot;float32&quot;), T_add: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(1), T.int64(256)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(lv1[v_ax0, v_ax1], p_fc1_bias[v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = lv1[v_ax0, v_ax1] + p_fc1_bias[v_ax1]

    @T.prim_func(private=True)
    def add1(lv5: T.Buffer((T.int64(1), T.int64(10)), &quot;float32&quot;), p_fc2_bias: T.Buffer((T.int64(10),), &quot;float32&quot;), T_add: T.Buffer((T.int64(1), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(lv5[v_ax0, v_ax1], p_fc2_bias[v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = lv5[v_ax0, v_ax1] + p_fc2_bias[v_ax1]

    @T.prim_func(private=True)
    def matmul(x: T.Buffer((T.int64(1), T.int64(784)), &quot;float32&quot;), lv: T.Buffer((T.int64(784), T.int64(256)), &quot;float32&quot;), matmul: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for i0, i1, k in T.grid(T.int64(1), T.int64(256), T.int64(784)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(x[v_i0, v_k], lv[v_k, v_i1])
                T.writes(matmul[v_i0, v_i1])
                with T.init():
                    matmul[v_i0, v_i1] = T.float32(0.0)
                matmul[v_i0, v_i1] = matmul[v_i0, v_i1] + x[v_i0, v_k] * lv[v_k, v_i1]

    @T.prim_func(private=True)
    def matmul1(lv3: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;), lv4: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;), matmul: T.Buffer((T.int64(1), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for i0, i1, k in T.grid(T.int64(1), T.int64(10), T.int64(256)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(lv3[v_i0, v_k], lv4[v_k, v_i1])
                T.writes(matmul[v_i0, v_i1])
                with T.init():
                    matmul[v_i0, v_i1] = T.float32(0.0)
                matmul[v_i0, v_i1] = matmul[v_i0, v_i1] + lv3[v_i0, v_k] * lv4[v_k, v_i1]

    @T.prim_func(private=True)
    def relu(lv2: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;), compute: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for i0, i1 in T.grid(T.int64(1), T.int64(256)):
            with T.block(&quot;compute&quot;):
                v_i0, v_i1 = T.axis.remap(&quot;SS&quot;, [i0, i1])
                T.reads(lv2[v_i0, v_i1])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.max(lv2[v_i0, v_i1], T.float32(0.0))

    @T.prim_func(private=True)
    def transpose(p_fc1_weight: T.Buffer((T.int64(256), T.int64(784)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(784), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(784), T.int64(256)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(p_fc1_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = p_fc1_weight[v_ax1, v_ax0]

    @T.prim_func(private=True)
    def transpose1(p_fc2_weight: T.Buffer((T.int64(10), T.int64(256)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(256), T.int64(10)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(p_fc2_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = p_fc2_weight[v_ax1, v_ax0]

    @R.function
    def main(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), p_fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), p_fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), p_fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;), p_fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.transpose, (p_fc1_weight,), out_sinfo=R.Tensor((784, 256), dtype=&quot;float32&quot;))
            lv1 = R.call_tir(cls.matmul, (x, lv), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            lv2 = R.call_tir(cls.add, (lv1, p_fc1_bias), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            lv3 = R.call_tir(cls.relu, (lv2,), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            lv4 = R.call_tir(cls.transpose1, (p_fc2_weight,), out_sinfo=R.Tensor((256, 10), dtype=&quot;float32&quot;))
            lv5 = R.call_tir(cls.matmul1, (lv3, lv4), out_sinfo=R.Tensor((1, 10), dtype=&quot;float32&quot;))
            lv6 = R.call_tir(cls.add1, (lv5, p_fc2_bias), out_sinfo=R.Tensor((1, 10), dtype=&quot;float32&quot;))
            gv: R.Tensor((1, 10), dtype=&quot;float32&quot;) = lv6
            R.output(gv)
        return gv
</pre></div>
</div>
<p>After the transformation, there are much more functions inside the module. Let’s print
the global vars again.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">mod</span><span class="o">.</span><span class="n">get_global_vars</span><span class="p">())</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[I.GlobalVar(&quot;add&quot;), I.GlobalVar(&quot;add1&quot;), I.GlobalVar(&quot;main&quot;), I.GlobalVar(&quot;matmul&quot;), I.GlobalVar(&quot;matmul1&quot;), I.GlobalVar(&quot;relu&quot;), I.GlobalVar(&quot;transpose&quot;), I.GlobalVar(&quot;transpose1&quot;)]
</pre></div>
</div>
<p>Next, Apache TVM Unity provides a set of default transformation pipelines for users,
to simplify the transformation process. We can then apply the default pipeline to the module.
The default <strong>zero</strong> pipeline contains very fundamental transformations, including:</p>
<ul class="simple">
<li><p><strong>LegalizeOps</strong>: This transform converts the Relax operators into <cite>call_tir</cite> functions
with the corresponding TensorIR Functions. After this transform, the IRModule will
contain both Relax functions and TensorIR functions.</p></li>
<li><p><strong>AnnotateTIROpPattern</strong>: This transform annotates the pattern of the TensorIR functions,
preparing them for subsequent operator fusion.</p></li>
<li><p><strong>FoldConstant</strong>: This pass performs constant folding, optimizing operations
involving constants.</p></li>
<li><p><strong>FuseOps and FuseTIR</strong>: These two passes work together to fuse operators based on the
patterns annotated in the previous step (AnnotateTIROpPattern). These passes transform
both Relax functions and TensorIR functions.</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Here, we have applied <strong>LegalizeOps</strong> twice in the flow. The second time is useless but
harmless.</p>
<p>Every passes can be duplicated in the flow, since we ensure the passes can handle all legal
IRModule inputs. This design can help users to construct their own pipeline.</p>
</div>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.get_pipeline" title="tvm.relax.get_pipeline" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">get_pipeline</span></a><span class="p">(</span><span class="s2">&quot;zero&quot;</span><span class="p">)(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def fused_matmul1_add1(lv3: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;), lv4: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;), p_fc2_bias: T.Buffer((T.int64(10),), &quot;float32&quot;), T_add_intermediate: T.Buffer((T.int64(1), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(10)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(10), T.int64(256)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(lv3[v_i0, v_k], lv4[v_k, v_i1])
                T.writes(matmul_intermediate[v_i0, v_i1])
                with T.init():
                    matmul_intermediate[v_i0, v_i1] = T.float32(0.0)
                matmul_intermediate[v_i0, v_i1] = matmul_intermediate[v_i0, v_i1] + lv3[v_i0, v_k] * lv4[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(10)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(matmul_intermediate[v_ax0, v_ax1], p_fc2_bias[v_ax1])
                T.writes(T_add_intermediate[v_ax0, v_ax1])
                T_add_intermediate[v_ax0, v_ax1] = matmul_intermediate[v_ax0, v_ax1] + p_fc2_bias[v_ax1]

    @T.prim_func(private=True)
    def fused_matmul_add_relu(x: T.Buffer((T.int64(1), T.int64(784)), &quot;float32&quot;), lv: T.Buffer((T.int64(784), T.int64(256)), &quot;float32&quot;), p_fc1_bias: T.Buffer((T.int64(256),), &quot;float32&quot;), compute_intermediate: T.Buffer((T.int64(1), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        matmul_intermediate = T.alloc_buffer((T.int64(1), T.int64(256)))
        T_add_intermediate = T.alloc_buffer((T.int64(1), T.int64(256)))
        for i0, i1, k in T.grid(T.int64(1), T.int64(256), T.int64(784)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(x[v_i0, v_k], lv[v_k, v_i1])
                T.writes(matmul_intermediate[v_i0, v_i1])
                with T.init():
                    matmul_intermediate[v_i0, v_i1] = T.float32(0.0)
                matmul_intermediate[v_i0, v_i1] = matmul_intermediate[v_i0, v_i1] + x[v_i0, v_k] * lv[v_k, v_i1]
        for ax0, ax1 in T.grid(T.int64(1), T.int64(256)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(matmul_intermediate[v_ax0, v_ax1], p_fc1_bias[v_ax1])
                T.writes(T_add_intermediate[v_ax0, v_ax1])
                T_add_intermediate[v_ax0, v_ax1] = matmul_intermediate[v_ax0, v_ax1] + p_fc1_bias[v_ax1]
        for i0, i1 in T.grid(T.int64(1), T.int64(256)):
            with T.block(&quot;compute&quot;):
                v_i0, v_i1 = T.axis.remap(&quot;SS&quot;, [i0, i1])
                T.reads(T_add_intermediate[v_i0, v_i1])
                T.writes(compute_intermediate[v_i0, v_i1])
                compute_intermediate[v_i0, v_i1] = T.max(T_add_intermediate[v_i0, v_i1], T.float32(0.0))

    @T.prim_func(private=True)
    def transpose(p_fc1_weight: T.Buffer((T.int64(256), T.int64(784)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(784), T.int64(256)), &quot;float32&quot;)):
        T.func_attr({&quot;op_pattern&quot;: 2, &quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(784), T.int64(256)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(p_fc1_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = p_fc1_weight[v_ax1, v_ax0]

    @T.prim_func(private=True)
    def transpose1(p_fc2_weight: T.Buffer((T.int64(10), T.int64(256)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(256), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;op_pattern&quot;: 2, &quot;tir.noalias&quot;: T.bool(True)})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(256), T.int64(10)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(p_fc2_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = p_fc2_weight[v_ax1, v_ax0]

    @R.function
    def main(x: R.Tensor((1, 784), dtype=&quot;float32&quot;), p_fc1_weight: R.Tensor((256, 784), dtype=&quot;float32&quot;), p_fc1_bias: R.Tensor((256,), dtype=&quot;float32&quot;), p_fc2_weight: R.Tensor((10, 256), dtype=&quot;float32&quot;), p_fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((1, 10), dtype=&quot;float32&quot;):
        R.func_attr({&quot;num_input&quot;: 1})
        cls = Module
        with R.dataflow():
            lv = R.call_tir(cls.transpose, (p_fc1_weight,), out_sinfo=R.Tensor((784, 256), dtype=&quot;float32&quot;))
            lv_1 = R.call_tir(cls.fused_matmul_add_relu, (x, lv, p_fc1_bias), out_sinfo=R.Tensor((1, 256), dtype=&quot;float32&quot;))
            lv4 = R.call_tir(cls.transpose1, (p_fc2_weight,), out_sinfo=R.Tensor((256, 10), dtype=&quot;float32&quot;))
            gv = R.call_tir(cls.fused_matmul1_add1, (lv_1, lv4, p_fc2_bias), out_sinfo=R.Tensor((1, 10), dtype=&quot;float32&quot;))
            R.output(gv)
        return gv
</pre></div>
</div>
</section>
<section id="deploy-the-irmodule-universally">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">Deploy the IRModule Universally</a><a class="headerlink" href="#deploy-the-irmodule-universally" title="Link to this heading"></a></h2>
<p>After the optimization, we can compile the model into a TVM runtime module.
Notably, Apache TVM Unity provides the ability of universal deployment, which means
we can deploy the same IRModule on different backends, including CPU, GPU, and other emerging
backends.</p>
<section id="deploy-on-cpu">
<h3>Deploy on CPU<a class="headerlink" href="#deploy-on-cpu" title="Link to this heading"></a></h3>
<p>We can deploy the IRModule on CPU by specifying the target as <code class="docutils literal notranslate"><span class="pre">llvm</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="../../reference/api/python/relax/relax.html#tvm.relax.Executable" title="tvm.relax.Executable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">exec</span></a> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.build" title="tvm.relax.build" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">build</span></a><span class="p">(</span><span class="n">mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;llvm&quot;</span><span class="p">)</span>
<span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
<span class="n">vm</span> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.VirtualMachine" title="tvm.relax.VirtualMachine" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span></a><span class="p">(</span><a href="../../reference/api/python/relax/relax.html#tvm.relax.Executable" title="tvm.relax.Executable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">exec</span></a><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>

<span class="n">raw_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
<span class="n">cpu_out</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">](</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">params_from_torch</span></a><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cpu_out</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[ 0.17299159  0.07954538 -0.028754   -0.10544708  0.10028081 -0.10305855
  -0.03538061 -0.22130662 -0.03098993 -0.03340686]]
</pre></div>
</div>
</section>
<section id="deploy-on-gpu">
<h3>Deploy on GPU<a class="headerlink" href="#deploy-on-gpu" title="Link to this heading"></a></h3>
<p>Besides, CPU backend, we can also deploy the IRModule on GPU. GPU requires
programs containing extra information, such as the thread bindings and shared memory
allocations. We need a further transformation to generate the GPU programs.</p>
<p>We use <code class="docutils literal notranslate"><span class="pre">DLight</span></code> to generate the GPU programs. In this tutorial, we won’t go into
the details of <code class="docutils literal notranslate"><span class="pre">DLight</span></code>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tvm</span> <span class="kn">import</span> <span class="n">dlight</span> <span class="k">as</span> <span class="n">dl</span>

<span class="k">with</span> <a href="../../reference/api/python/target.html#tvm.target.Target" title="tvm.target.Target" class="sphx-glr-backref-module-tvm-target sphx-glr-backref-type-py-class"><span class="n">tvm</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">Target</span></a><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
    <span class="n">gpu_mod</span> <span class="o">=</span> <a href="../../reference/api/python/dlight.html#tvm.dlight.ApplyDefaultSchedule" title="tvm.dlight.ApplyDefaultSchedule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">dl</span><span class="o">.</span><span class="n">ApplyDefaultSchedule</span></a><span class="p">(</span>
        <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Matmul</span></a><span class="p">(),</span>
        <a href="../../reference/api/python/dlight.html#tvm.dlight.ScheduleRule" title="tvm.dlight.ScheduleRule" class="sphx-glr-backref-module-tvm-dlight sphx-glr-backref-type-py-class"><span class="n">dl</span><span class="o">.</span><span class="n">gpu</span><span class="o">.</span><span class="n">Fallback</span></a><span class="p">(),</span>
    <span class="p">)(</span><span class="n">mod</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we can compile the IRModule on GPU, the similar way as we did on CPU.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><a href="../../reference/api/python/relax/relax.html#tvm.relax.Executable" title="tvm.relax.Executable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">exec</span></a> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.build" title="tvm.relax.build" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">build</span></a><span class="p">(</span><span class="n">gpu_mod</span><span class="p">,</span> <span class="n">target</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">dev</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">vm</span> <span class="o">=</span> <a href="../../reference/api/python/relax/relax.html#tvm.relax.VirtualMachine" title="tvm.relax.VirtualMachine" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">relax</span><span class="o">.</span><span class="n">VirtualMachine</span></a><span class="p">(</span><a href="../../reference/api/python/relax/relax.html#tvm.relax.Executable" title="tvm.relax.Executable" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">exec</span></a><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
<span class="c1"># Need to allocate data and params on GPU device</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">raw_data</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span>
<a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gpu_params</span></a> <span class="o">=</span> <span class="p">[</span><span class="n">tvm</span><span class="o">.</span><span class="n">nd</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">dev</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="builtins.dict" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">params_from_torch</span></a><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">]]</span>
<span class="n">gpu_out</span> <span class="o">=</span> <span class="n">vm</span><span class="p">[</span><span class="s2">&quot;main&quot;</span><span class="p">](</span><span class="n">data</span><span class="p">,</span> <span class="o">*</span><a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">gpu_params</span></a><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gpu_out</span><span class="p">)</span>

<span class="c1"># Check the correctness of the results</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">cpu_out</span><span class="p">,</span> <span class="n">gpu_out</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span>[[ 0.17299157  0.07954545 -0.02875405 -0.10544711  0.10028083 -0.10305853
  -0.03538062 -0.22130659 -0.03098993 -0.03340688]]
</pre></div>
</div>
</section>
<section id="deploy-on-other-backends">
<h3>Deploy on Other Backends<a class="headerlink" href="#deploy-on-other-backends" title="Link to this heading"></a></h3>
<p>Apache TVM Unity also supports other backends, such as different kinds of GPUs
(Metal, ROCm, Vulkan and OpenCL), different kinds of CPUs (x86, ARM), and other
emerging backends (e.g., WebAssembly). The deployment process is similar to the
GPU backend.</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-get-started-tutorials-ir-module-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/a6d7947451d373bc811080cffa18dc7c/ir_module.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">ir_module.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/0b64717d4cc6027368b96fad40119738/ir_module.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">ir_module.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../_downloads/11c11e53c7dace51a8be968ee169ed0d/ir_module.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">ir_module.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../how_to/tutorials/e2e_opt_model.html" class="btn btn-neutral float-right" title="End-to-End Optimize Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="quick_start.html" class="btn btn-neutral float-left" title="Quick Start" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2024 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2024 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="../../_static/downloads/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="../../_static/downloads/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
    <!-- Theme Analytics -->
    <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75982049-2', 'auto');
    ga('send', 'pageview');
    </script>

    
   

</body>
</html>