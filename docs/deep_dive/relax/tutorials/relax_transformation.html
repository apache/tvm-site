



<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Transformation &mdash; tvm 0.22.dev0 documentation</title>
  

  
  <link rel="stylesheet" href="../../../_static/downloads/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/sg_gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/tlcpack_theme.css" type="text/css" />

  
  

  
  
  
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js?v=5d32c60e"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="../../../_static/documentation_options.js?v=feaa0556"></script>
        <script src="../../../_static/doctools.js?v=9a2dae69"></script>
        <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    
    <script type="text/javascript" src="../../../_static/js/tlcpack_theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Python API" href="../../../reference/api/python/index.html" />
    <link rel="prev" title="Relax Creation" href="relax_creation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    
<header class="header">
    <div class="innercontainer">
      <div class="headerInner d-flex justify-content-between align-items-center">
          <div class="headerLogo">
               <a href="https://tvm.apache.org/"><img src=https://tvm.apache.org/assets/images/logo.svg alt="logo"></a>
          </div>

          <div id="headMenu" class="headerNav">
            <button type="button" id="closeHeadMenu" class="navCloseBtn"><img src="../../../_static/img/close-icon.svg" alt="Close"></button>
             <ul class="nav">
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/community>Community</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/download>Download</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://tvm.apache.org/docs>Docs</a>
                </li>
                <li class="nav-item">
                   <a class="nav-link" href=https://github.com/apache/tvm/>Github</a>
                </li>
             </ul>
               <div class="responsivetlcdropdown">
                 <button type="button" class="btn-link">
                   ASF
                 </button>
                 <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                 </ul>
               </div>
          </div>
            <div class="responsiveMenuIcon">
              <button type="button" id="menuBtn" class="btn-menu"><img src="../../../_static/img/menu-icon.svg" alt="Menu Icon"></button>
            </div>

            <div class="tlcDropdown">
              <div class="dropdown">
                <button type="button" class="btn-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                  ASF
                </button>
                <div class="dropdown-menu dropdown-menu-right">
                  <ul>
                     <li>
                       <a href=https://apache.org/>Apache Homepage</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/licenses/>License</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/sponsorship.html>Sponsorship</a>
                     </li>
                     <li>
                       <a href=https://tvm.apache.org/docs/reference/security.html>Security</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/foundation/thanks.html>Thanks</a>
                     </li>
                     <li>
                       <a href=https://www.apache.org/events/current-event>Events</a>
                     </li>
                  </ul>
                </div>
              </div>
          </div>
       </div>
    </div>
 </header>
 
    <nav data-toggle="wy-nav-shift" class="wy-nav-side fixed">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="sidetitle" alt="Documentation Home"> tvm
          

          
          </a>

          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../install/index.html">Installing TVM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/tutorials/quick_start.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../get_started/tutorials/ir_module.html">IRModule</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">How To</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../how_to/tutorials/e2e_opt_model.html">End-to-End Optimize Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../how_to/tutorials/customize_opt.html">Customize Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../how_to/tutorials/optimize_llm.html">Optimize Large Language Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../how_to/tutorials/cross_compilation_and_rpc.html">Cross Compilation and RPC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../how_to/dev/index.html">Development Guides</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Deep Dive</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../arch/index.html">Design and Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tensor_ir/index.html">TensorIR</a></li>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Relax</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/api/python/index.html">Python API</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/api/links.html">Other APIs</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">About</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../contribute/index.html">Contributor Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/publications.html">Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../reference/security.html">Security Guide</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Index</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../genindex.html">Index</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      
      <nav class="wy-nav-top" aria-label="top navigation" data-toggle="wy-nav-top">
        
            <div class="togglemenu">

            </div>
            <div class="nav-content">
              <!-- tvm -->
              Table of Contents
            </div>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        

          




















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html">Docs</a> <span class="br-arrow">></span></li>
        
          <li><a href="../index.html">Relax</a> <span class="br-arrow">></span></li>
        
      <li>Transformation</li>
    
    
      
      
        
      
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            
              <a href="https://github.com/apache/tvm/edit/main/docs/deep_dive/relax/tutorials/relax_transformation.rst" class="fa fa-github"> Edit on GitHub</a>
            
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="sphx-glr-download-link-note admonition note">
<p class="admonition-title">Note</p>
<p>This tutorial can be used interactively with Google Colab! You can also click
<a class="reference internal" href="#sphx-glr-download-deep-dive-relax-tutorials-relax-transformation-py"><span class="std std-ref">here</span></a> to run the Jupyter notebook locally.</p>
<a class="reference external image-reference" href="https://colab.research.google.com/github/apache/tvm-site/blob/asf-site/docs/_downloads/31d077cfa7c55c0edcd16ad5d4faf483/relax_transformation.ipynb"><img alt="../../../_static/downloads/colab_button.svg" class="align-center" src="../../../_static/downloads/colab_button.svg" style="width: 300px;" />
</a>
</div>
<section class="sphx-glr-example-title" id="transformation">
<span id="relax-transform"></span><span id="sphx-glr-deep-dive-relax-tutorials-relax-transformation-py"></span><h1>Transformation<a class="headerlink" href="#transformation" title="Link to this heading"></a></h1>
<p>In this section, we will dive into the transformation of Relax programs.
Transformations is one of the key ingredients of the compilation flows
for optimizing and integrating with hardware backends.</p>
<p>Let’s first create a simple Relax program as what we have done in
the <a class="reference internal" href="relax_creation.html#relax-creation"><span class="std std-ref">previous section</span></a>.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">tvm</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm</span><span class="w"> </span><span class="kn">import</span> <a href="../../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">IRModule</span></a><span class="p">,</span> <span class="n">relax</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax.frontend</span><span class="w"> </span><span class="kn">import</span> <span class="n">nn</span>


<span class="k">class</span><span class="w"> </span><span class="nc">NNModule</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="n">origin_mod</span><span class="p">,</span> <a href="https://docs.python.org/3/library/stdtypes.html#list" title="builtins.list" class="sphx-glr-backref-module-builtins sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">params</span></a> <span class="o">=</span> <span class="n">NNModule</span><span class="p">()</span><span class="o">.</span><span class="n">export_tvm</span><span class="p">(</span>
    <span class="p">{</span><span class="s2">&quot;forward&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">spec</span><span class="o">.</span><span class="n">Tensor</span><span class="p">((</span><span class="s2">&quot;n&quot;</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="s2">&quot;float32&quot;</span><span class="p">)}}</span>
<span class="p">)</span>
<span class="n">origin_mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @R.function
    def forward(x: R.Tensor((&quot;n&quot;, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((128, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((128,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 128), dtype=&quot;float32&quot;), fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((&quot;n&quot;, 10), dtype=&quot;float32&quot;):
        n = T.int64()
        R.func_attr({&quot;num_input&quot;: 1})
        with R.dataflow():
            permute_dims: R.Tensor((784, 128), dtype=&quot;float32&quot;) = R.permute_dims(fc1_weight, axes=None)
            matmul: R.Tensor((n, 128), dtype=&quot;float32&quot;) = R.matmul(x, permute_dims, out_dtype=&quot;void&quot;)
            add: R.Tensor((n, 128), dtype=&quot;float32&quot;) = R.add(matmul, fc1_bias)
            relu: R.Tensor((n, 128), dtype=&quot;float32&quot;) = R.nn.relu(add)
            permute_dims1: R.Tensor((128, 10), dtype=&quot;float32&quot;) = R.permute_dims(fc2_weight, axes=None)
            matmul1: R.Tensor((n, 10), dtype=&quot;float32&quot;) = R.matmul(relu, permute_dims1, out_dtype=&quot;void&quot;)
            add1: R.Tensor((n, 10), dtype=&quot;float32&quot;) = R.add(matmul1, fc2_bias)
            gv: R.Tensor((n, 10), dtype=&quot;float32&quot;) = add1
            R.output(gv)
        return gv
</pre></div>
</div>
<section id="apply-transformations">
<h2>Apply transformations<a class="headerlink" href="#apply-transformations" title="Link to this heading"></a></h2>
<p>Passes are the main way to apply transformations to the program.
We can apply passes to the program. As first step, let’s apply
a built-in pass <code class="docutils literal notranslate"><span class="pre">LegalizeOps</span></code> to lower the high-level operators
into low-level operators.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <a href="../../../reference/api/python/relax/transform.html#tvm.relax.transform.LegalizeOps" title="tvm.relax.transform.LegalizeOps" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">LegalizeOps</span></a><span class="p">()(</span><span class="n">origin_mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def add(var_matmul: T.handle, fc1_bias: T.Buffer((T.int64(128),), &quot;float32&quot;), var_T_add: T.handle):
        T.func_attr({&quot;tir.noalias&quot;: True})
        n = T.int64()
        matmul = T.match_buffer(var_matmul, (n, T.int64(128)))
        T_add = T.match_buffer(var_T_add, (n, T.int64(128)))
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(n, T.int64(128)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(matmul[v_ax0, v_ax1], fc1_bias[v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = matmul[v_ax0, v_ax1] + fc1_bias[v_ax1]

    @T.prim_func(private=True)
    def add1(var_matmul1: T.handle, fc2_bias: T.Buffer((T.int64(10),), &quot;float32&quot;), var_T_add: T.handle):
        T.func_attr({&quot;tir.noalias&quot;: True})
        n = T.int64()
        matmul1 = T.match_buffer(var_matmul1, (n, T.int64(10)))
        T_add = T.match_buffer(var_T_add, (n, T.int64(10)))
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(n, T.int64(10)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(matmul1[v_ax0, v_ax1], fc2_bias[v_ax1])
                T.writes(T_add[v_ax0, v_ax1])
                T_add[v_ax0, v_ax1] = matmul1[v_ax0, v_ax1] + fc2_bias[v_ax1]

    @T.prim_func(private=True)
    def matmul(var_x: T.handle, permute_dims: T.Buffer((T.int64(784), T.int64(128)), &quot;float32&quot;), var_matmul: T.handle):
        T.func_attr({&quot;tir.noalias&quot;: True})
        n = T.int64()
        x = T.match_buffer(var_x, (n, T.int64(784)))
        matmul = T.match_buffer(var_matmul, (n, T.int64(128)))
        # with T.block(&quot;root&quot;):
        for i0, i1, k in T.grid(n, T.int64(128), T.int64(784)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(x[v_i0, v_k], permute_dims[v_k, v_i1])
                T.writes(matmul[v_i0, v_i1])
                with T.init():
                    matmul[v_i0, v_i1] = T.float32(0.0)
                matmul[v_i0, v_i1] = matmul[v_i0, v_i1] + x[v_i0, v_k] * permute_dims[v_k, v_i1]

    @T.prim_func(private=True)
    def matmul1(var_relu: T.handle, permute_dims1: T.Buffer((T.int64(128), T.int64(10)), &quot;float32&quot;), var_matmul: T.handle):
        T.func_attr({&quot;tir.noalias&quot;: True})
        n = T.int64()
        relu = T.match_buffer(var_relu, (n, T.int64(128)))
        matmul = T.match_buffer(var_matmul, (n, T.int64(10)))
        # with T.block(&quot;root&quot;):
        for i0, i1, k in T.grid(n, T.int64(10), T.int64(128)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(relu[v_i0, v_k], permute_dims1[v_k, v_i1])
                T.writes(matmul[v_i0, v_i1])
                with T.init():
                    matmul[v_i0, v_i1] = T.float32(0.0)
                matmul[v_i0, v_i1] = matmul[v_i0, v_i1] + relu[v_i0, v_k] * permute_dims1[v_k, v_i1]

    @T.prim_func(private=True)
    def relu(var_add: T.handle, var_compute: T.handle):
        T.func_attr({&quot;tir.noalias&quot;: True})
        n = T.int64()
        add = T.match_buffer(var_add, (n, T.int64(128)))
        compute = T.match_buffer(var_compute, (n, T.int64(128)))
        # with T.block(&quot;root&quot;):
        for i0, i1 in T.grid(n, T.int64(128)):
            with T.block(&quot;compute&quot;):
                v_i0, v_i1 = T.axis.remap(&quot;SS&quot;, [i0, i1])
                T.reads(add[v_i0, v_i1])
                T.writes(compute[v_i0, v_i1])
                compute[v_i0, v_i1] = T.max(add[v_i0, v_i1], T.float32(0.0))

    @T.prim_func(private=True)
    def transpose(fc1_weight: T.Buffer((T.int64(128), T.int64(784)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(784), T.int64(128)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: True})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(784), T.int64(128)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(fc1_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = fc1_weight[v_ax1, v_ax0]

    @T.prim_func(private=True)
    def transpose1(fc2_weight: T.Buffer((T.int64(10), T.int64(128)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(128), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;tir.noalias&quot;: True})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(128), T.int64(10)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(fc2_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = fc2_weight[v_ax1, v_ax0]

    @R.function
    def forward(x: R.Tensor((&quot;n&quot;, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((128, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((128,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 128), dtype=&quot;float32&quot;), fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((&quot;n&quot;, 10), dtype=&quot;float32&quot;):
        n = T.int64()
        R.func_attr({&quot;num_input&quot;: 1})
        cls = Module
        with R.dataflow():
            permute_dims = R.call_tir(cls.transpose, (fc1_weight,), out_sinfo=R.Tensor((784, 128), dtype=&quot;float32&quot;))
            matmul = R.call_tir(cls.matmul, (x, permute_dims), out_sinfo=R.Tensor((n, 128), dtype=&quot;float32&quot;))
            add = R.call_tir(cls.add, (matmul, fc1_bias), out_sinfo=R.Tensor((n, 128), dtype=&quot;float32&quot;))
            relu = R.call_tir(cls.relu, (add,), out_sinfo=R.Tensor((n, 128), dtype=&quot;float32&quot;))
            permute_dims1 = R.call_tir(cls.transpose1, (fc2_weight,), out_sinfo=R.Tensor((128, 10), dtype=&quot;float32&quot;))
            matmul1 = R.call_tir(cls.matmul1, (relu, permute_dims1), out_sinfo=R.Tensor((n, 10), dtype=&quot;float32&quot;))
            add1 = R.call_tir(cls.add1, (matmul1, fc2_bias), out_sinfo=R.Tensor((n, 10), dtype=&quot;float32&quot;))
            gv: R.Tensor((n, 10), dtype=&quot;float32&quot;) = add1
            R.output(gv)
        return gv
</pre></div>
</div>
<p>As we can see from the output, the high-level operators (aka <code class="docutils literal notranslate"><span class="pre">relax.op</span></code>) in the program
are replaced by their corresponding low-level operators (aka <code class="docutils literal notranslate"><span class="pre">relax.call_tir</span></code>).</p>
<p>Then let’s trying to apply the operator fusion, which is a wide-used optimization technique
in ML compilers. Note that in relax, fusion optimizations are done with the collaboration of
a set of passes. We can apply them in a sequence.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="n">mod</span> <span class="o">=</span> <span class="n">tvm</span><span class="o">.</span><span class="n">ir</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
    <span class="p">[</span>
        <a href="../../../reference/api/python/relax/transform.html#tvm.relax.transform.AnnotateTIROpPattern" title="tvm.relax.transform.AnnotateTIROpPattern" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">AnnotateTIROpPattern</span></a><span class="p">(),</span>
        <a href="../../../reference/api/python/relax/transform.html#tvm.relax.transform.FuseOps" title="tvm.relax.transform.FuseOps" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FuseOps</span></a><span class="p">(),</span>
        <a href="../../../reference/api/python/relax/transform.html#tvm.relax.transform.FuseTIR" title="tvm.relax.transform.FuseTIR" class="sphx-glr-backref-module-tvm-relax-transform sphx-glr-backref-type-py-function"><span class="n">tvm</span><span class="o">.</span><span class="n">relax</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">FuseTIR</span></a><span class="p">(),</span>
    <span class="p">]</span>
<span class="p">)(</span><span class="n">mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @T.prim_func(private=True)
    def fused_matmul1_add1(p_relu: T.handle, permute_dims1: T.Buffer((T.int64(128), T.int64(10)), &quot;float32&quot;), fc2_bias: T.Buffer((T.int64(10),), &quot;float32&quot;), p_output0: T.handle):
        T.func_attr({&quot;tir.noalias&quot;: True})
        n = T.int64()
        relu = T.match_buffer(p_relu, (n, T.int64(128)))
        T_add_intermediate = T.match_buffer(p_output0, (n, T.int64(10)))
        # with T.block(&quot;root&quot;):
        matmul_intermediate = T.alloc_buffer((n, T.int64(10)))
        for i0, i1, k in T.grid(n, T.int64(10), T.int64(128)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(relu[v_i0, v_k], permute_dims1[v_k, v_i1])
                T.writes(matmul_intermediate[v_i0, v_i1])
                with T.init():
                    matmul_intermediate[v_i0, v_i1] = T.float32(0.0)
                matmul_intermediate[v_i0, v_i1] = matmul_intermediate[v_i0, v_i1] + relu[v_i0, v_k] * permute_dims1[v_k, v_i1]
        for ax0, ax1 in T.grid(n, T.int64(10)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(matmul_intermediate[v_ax0, v_ax1], fc2_bias[v_ax1])
                T.writes(T_add_intermediate[v_ax0, v_ax1])
                T_add_intermediate[v_ax0, v_ax1] = matmul_intermediate[v_ax0, v_ax1] + fc2_bias[v_ax1]

    @T.prim_func(private=True)
    def fused_matmul_add_relu(p_x: T.handle, permute_dims: T.Buffer((T.int64(784), T.int64(128)), &quot;float32&quot;), fc1_bias: T.Buffer((T.int64(128),), &quot;float32&quot;), p_output0: T.handle):
        T.func_attr({&quot;tir.noalias&quot;: True})
        n = T.int64()
        x = T.match_buffer(p_x, (n, T.int64(784)))
        compute_intermediate = T.match_buffer(p_output0, (n, T.int64(128)))
        # with T.block(&quot;root&quot;):
        matmul_intermediate = T.alloc_buffer((n, T.int64(128)))
        T_add_intermediate = T.alloc_buffer((n, T.int64(128)))
        for i0, i1, k in T.grid(n, T.int64(128), T.int64(784)):
            with T.block(&quot;matmul&quot;):
                v_i0, v_i1, v_k = T.axis.remap(&quot;SSR&quot;, [i0, i1, k])
                T.reads(x[v_i0, v_k], permute_dims[v_k, v_i1])
                T.writes(matmul_intermediate[v_i0, v_i1])
                with T.init():
                    matmul_intermediate[v_i0, v_i1] = T.float32(0.0)
                matmul_intermediate[v_i0, v_i1] = matmul_intermediate[v_i0, v_i1] + x[v_i0, v_k] * permute_dims[v_k, v_i1]
        for ax0, ax1 in T.grid(n, T.int64(128)):
            with T.block(&quot;T_add&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(matmul_intermediate[v_ax0, v_ax1], fc1_bias[v_ax1])
                T.writes(T_add_intermediate[v_ax0, v_ax1])
                T_add_intermediate[v_ax0, v_ax1] = matmul_intermediate[v_ax0, v_ax1] + fc1_bias[v_ax1]
        for i0, i1 in T.grid(n, T.int64(128)):
            with T.block(&quot;compute&quot;):
                v_i0, v_i1 = T.axis.remap(&quot;SS&quot;, [i0, i1])
                T.reads(T_add_intermediate[v_i0, v_i1])
                T.writes(compute_intermediate[v_i0, v_i1])
                compute_intermediate[v_i0, v_i1] = T.max(T_add_intermediate[v_i0, v_i1], T.float32(0.0))

    @T.prim_func(private=True)
    def transpose(fc1_weight: T.Buffer((T.int64(128), T.int64(784)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(784), T.int64(128)), &quot;float32&quot;)):
        T.func_attr({&quot;op_pattern&quot;: 2, &quot;tir.noalias&quot;: True})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(784), T.int64(128)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(fc1_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = fc1_weight[v_ax1, v_ax0]

    @T.prim_func(private=True)
    def transpose1(fc2_weight: T.Buffer((T.int64(10), T.int64(128)), &quot;float32&quot;), T_transpose: T.Buffer((T.int64(128), T.int64(10)), &quot;float32&quot;)):
        T.func_attr({&quot;op_pattern&quot;: 2, &quot;tir.noalias&quot;: True})
        # with T.block(&quot;root&quot;):
        for ax0, ax1 in T.grid(T.int64(128), T.int64(10)):
            with T.block(&quot;T_transpose&quot;):
                v_ax0, v_ax1 = T.axis.remap(&quot;SS&quot;, [ax0, ax1])
                T.reads(fc2_weight[v_ax1, v_ax0])
                T.writes(T_transpose[v_ax0, v_ax1])
                T_transpose[v_ax0, v_ax1] = fc2_weight[v_ax1, v_ax0]

    @R.function
    def forward(x: R.Tensor((&quot;n&quot;, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((128, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((128,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 128), dtype=&quot;float32&quot;), fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((&quot;n&quot;, 10), dtype=&quot;float32&quot;):
        n = T.int64()
        R.func_attr({&quot;num_input&quot;: 1})
        cls = Module
        with R.dataflow():
            permute_dims = R.call_tir(cls.transpose, (fc1_weight,), out_sinfo=R.Tensor((784, 128), dtype=&quot;float32&quot;))
            lv = R.call_tir(cls.fused_matmul_add_relu, (x, permute_dims, fc1_bias), out_sinfo=R.Tensor((n, 128), dtype=&quot;float32&quot;))
            permute_dims1 = R.call_tir(cls.transpose1, (fc2_weight,), out_sinfo=R.Tensor((128, 10), dtype=&quot;float32&quot;))
            gv = R.call_tir(cls.fused_matmul1_add1, (lv, permute_dims1, fc2_bias), out_sinfo=R.Tensor((n, 10), dtype=&quot;float32&quot;))
            R.output(gv)
        return gv
</pre></div>
</div>
<p>As result, we can see that the <code class="docutils literal notranslate"><span class="pre">matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">add</span></code> and <code class="docutils literal notranslate"><span class="pre">relu</span></code> operators are fused
into one kernel (aka one <code class="docutils literal notranslate"><span class="pre">call_tir</span></code>).</p>
<p>For all built-in passes, please refer to <code class="xref py py-class docutils literal notranslate"><span class="pre">relax.transform</span></code>.</p>
</section>
<section id="custom-passes">
<h2>Custom Passes<a class="headerlink" href="#custom-passes" title="Link to this heading"></a></h2>
<p>We can also define our own passes. Let’s taking an example of rewrite the <code class="docutils literal notranslate"><span class="pre">relu</span></code>
operator to <code class="docutils literal notranslate"><span class="pre">gelu</span></code> operator.</p>
<p>First, we need to write a Relax IR Mutator to do the rewriting.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">tvm.relax.expr_functor</span><span class="w"> </span><span class="kn">import</span> <a href="../../../reference/api/python/relax/relax.html#tvm.relax.PyExprMutator" title="tvm.relax.PyExprMutator" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class"><span class="n">PyExprMutator</span></a><span class="p">,</span> <span class="n">mutator</span>


<span class="nd">@mutator</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ReluRewriter</span><span class="p">(</span><a href="../../../reference/api/python/relax/relax.html#tvm.relax.PyExprMutator" title="tvm.relax.PyExprMutator" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class"><span class="n">PyExprMutator</span></a><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">visit_call_</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">call</span><span class="p">:</span> <a href="../../../reference/api/python/relax/relax.html#tvm.relax.Call" title="tvm.relax.Call" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">Call</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a href="../../../reference/api/python/ir.html#tvm.ir.RelaxExpr" title="tvm.ir.RelaxExpr" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">Expr</span></a><span class="p">:</span>
        <span class="c1"># visit the relax.Call expr, and only handle the case when op is relax.nn.relu</span>
        <span class="k">if</span> <span class="n">call</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;relax.nn.relu&quot;</span><span class="p">:</span>
            <span class="k">return</span> <a href="../../../reference/api/python/relax/op.html#tvm.relax.op.nn.gelu" title="tvm.relax.op.nn.gelu" class="sphx-glr-backref-module-tvm-relax-op-nn sphx-glr-backref-type-py-function"><span class="n">relax</span><span class="o">.</span><span class="n">op</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span></a><span class="p">(</span><span class="n">call</span><span class="o">.</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">visit_call_</span><span class="p">(</span><span class="n">call</span><span class="p">)</span>
</pre></div>
</div>
<p>Then we can write a pass to apply the mutator to the whole module.</p>
<div class="highlight-Python notranslate"><div class="highlight"><pre><span></span><span class="nd">@tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">module_pass</span><span class="p">(</span><span class="n">opt_level</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;ReluToGelu&quot;</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">ReluToGelu</span><span class="p">:</span>  <span class="c1"># pylint: disable=too-few-public-methods</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">transform_module</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod</span><span class="p">:</span> <a href="../../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">IRModule</span></a><span class="p">,</span> <span class="n">_ctx</span><span class="p">:</span> <a href="../../../reference/api/python/transform.html#tvm.transform.PassContext" title="tvm.transform.PassContext" class="sphx-glr-backref-module-tvm-transform sphx-glr-backref-type-py-class sphx-glr-backref-instance"><span class="n">tvm</span><span class="o">.</span><span class="n">transform</span><span class="o">.</span><span class="n">PassContext</span></a><span class="p">)</span> <span class="o">-&gt;</span> <a href="../../../reference/api/python/ir.html#tvm.ir.Node" title="tvm.ir.Node" class="sphx-glr-backref-module-tvm-ir sphx-glr-backref-type-py-class"><span class="n">IRModule</span></a><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;IRModule-level transformation&quot;&quot;&quot;</span>
        <span class="n">rewriter</span> <span class="o">=</span> <span class="n">ReluRewriter</span><span class="p">(</span><span class="n">mod</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">g_var</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="n">mod</span><span class="o">.</span><span class="n">functions_items</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <a href="../../../reference/api/python/relax/relax.html#tvm.relax.Function" title="tvm.relax.Function" class="sphx-glr-backref-module-tvm-relax sphx-glr-backref-type-py-class"><span class="n">relax</span><span class="o">.</span><span class="n">Function</span></a><span class="p">):</span>
                <span class="n">func</span> <span class="o">=</span> <span class="n">rewriter</span><span class="o">.</span><span class="n">visit_expr</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
                <span class="n">rewriter</span><span class="o">.</span><span class="n">builder_</span><span class="o">.</span><span class="n">update_func</span><span class="p">(</span><span class="n">g_var</span><span class="p">,</span> <span class="n">func</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">rewriter</span><span class="o">.</span><span class="n">builder_</span><span class="o">.</span><span class="n">get</span><span class="p">()</span>


<span class="n">mod</span> <span class="o">=</span> <span class="n">ReluToGelu</span><span class="p">()(</span><span class="n">origin_mod</span><span class="p">)</span>
<span class="n">mod</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="sphx-glr-script-out highlight-none notranslate"><div class="highlight"><pre><span></span># from tvm.script import ir as I
# from tvm.script import tir as T
# from tvm.script import relax as R

@I.ir_module
class Module:
    @R.function
    def forward(x: R.Tensor((&quot;n&quot;, 784), dtype=&quot;float32&quot;), fc1_weight: R.Tensor((128, 784), dtype=&quot;float32&quot;), fc1_bias: R.Tensor((128,), dtype=&quot;float32&quot;), fc2_weight: R.Tensor((10, 128), dtype=&quot;float32&quot;), fc2_bias: R.Tensor((10,), dtype=&quot;float32&quot;)) -&gt; R.Tensor((&quot;n&quot;, 10), dtype=&quot;float32&quot;):
        n = T.int64()
        R.func_attr({&quot;num_input&quot;: 1})
        with R.dataflow():
            permute_dims: R.Tensor((784, 128), dtype=&quot;float32&quot;) = R.permute_dims(fc1_weight, axes=None)
            matmul: R.Tensor((n, 128), dtype=&quot;float32&quot;) = R.matmul(x, permute_dims, out_dtype=&quot;void&quot;)
            add: R.Tensor((n, 128), dtype=&quot;float32&quot;) = R.add(matmul, fc1_bias)
            relu: R.Tensor((n, 128), dtype=&quot;float32&quot;) = R.nn.gelu(add)
            permute_dims1: R.Tensor((128, 10), dtype=&quot;float32&quot;) = R.permute_dims(fc2_weight, axes=None)
            matmul1: R.Tensor((n, 10), dtype=&quot;float32&quot;) = R.matmul(relu, permute_dims1, out_dtype=&quot;void&quot;)
            add1: R.Tensor((n, 10), dtype=&quot;float32&quot;) = R.add(matmul1, fc2_bias)
            gv: R.Tensor((n, 10), dtype=&quot;float32&quot;) = add1
            R.output(gv)
        return gv
</pre></div>
</div>
<p>The printed output shows that the <code class="docutils literal notranslate"><span class="pre">relax.nn.relu</span></code> operator is
rewritten to <code class="docutils literal notranslate"><span class="pre">relax.nn.gelu</span></code> operator.</p>
<p>For the details of the mutator, please refer to <code class="xref py py-class docutils literal notranslate"><span class="pre">relax.expr_functor.PyExprMutator</span></code>.</p>
</section>
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading"></a></h2>
<p>In this section, we have shown how to apply transformations to the Relax program.
We have also shown how to define and apply custom transformations.</p>
<div class="sphx-glr-footer sphx-glr-footer-example docutils container" id="sphx-glr-download-deep-dive-relax-tutorials-relax-transformation-py">
<div class="sphx-glr-download sphx-glr-download-jupyter docutils container">
<p><a class="reference download internal" download="" href="../../../_downloads/31d077cfa7c55c0edcd16ad5d4faf483/relax_transformation.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Jupyter</span> <span class="pre">notebook:</span> <span class="pre">relax_transformation.ipynb</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-python docutils container">
<p><a class="reference download internal" download="" href="../../../_downloads/4e684410fb30ce02332f55fde123c42e/relax_transformation.py"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Python</span> <span class="pre">source</span> <span class="pre">code:</span> <span class="pre">relax_transformation.py</span></code></a></p>
</div>
<div class="sphx-glr-download sphx-glr-download-zip docutils container">
<p><a class="reference download internal" download="" href="../../../_downloads/7d201684dfa095a5ea48d98e9a2ef7ad/relax_transformation.zip"><code class="xref download docutils literal notranslate"><span class="pre">Download</span> <span class="pre">zipped:</span> <span class="pre">relax_transformation.zip</span></code></a></p>
</div>
</div>
<p class="sphx-glr-signature"><a class="reference external" href="https://sphinx-gallery.github.io">Gallery generated by Sphinx-Gallery</a></p>
</section>
</section>


           </div>
           
          </div>
          

<footer>

    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../../../reference/api/python/index.html" class="btn btn-neutral float-right" title="Python API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="relax_creation.html" class="btn btn-neutral float-left" title="Relax Creation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>

<div id="button" class="backtop"><img src="../../../_static/img/right.svg" alt="backtop"/> </div>
<section class="footerSec">
    <div class="footerHeader">
      <div class="d-flex align-md-items-center justify-content-between flex-column flex-md-row">
        <div class="copywrite d-flex align-items-center">
          <h5 id="copy-right-info">© 2024 Apache Software Foundation | All rights reserved</h5>
        </div>
      </div>

    </div>

    <div>
      <div class="footernote">Copyright © 2024 The Apache Software Foundation. Apache TVM, Apache, the Apache feather, and the Apache TVM project logo are either trademarks or registered trademarks of the Apache Software Foundation.</div>
    </div>

</section>
</footer>
        </div>
      </div>

    </section>

  </div>
  

    <script src="../../../_static/downloads/popper.min.js" integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q" crossorigin="anonymous"></script>
    <script src="../../../_static/downloads/bootstrap.min.js" integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl" crossorigin="anonymous"></script>

  </body>
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>